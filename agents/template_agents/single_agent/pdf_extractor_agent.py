#!/usr/bin/env python3
"""
PDFå†…å®¹æå–Agent

ä¸“ä¸šçš„PDFå†…å®¹æå–ä¸“å®¶ï¼Œèƒ½å¤Ÿå¤„ç†PDFæ–‡ä»¶ï¼Œå°†å…¶è½¬æ¢ä¸ºå›¾ç‰‡ï¼Œè°ƒç”¨å¤šæ¨¡æ€Agentæå–æ–‡æœ¬å†…å®¹ï¼Œ
å¹¶ç”ŸæˆTXTæ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ã€‚

åŠŸèƒ½:
1. å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºå›¾ç‰‡ï¼Œå­˜å‚¨åœ¨.cacheç›®å½•
2. è°ƒç”¨å¤šæ¨¡æ€Agentæå–å›¾ç‰‡ä¸­çš„æ–‡æœ¬å†…å®¹
3. åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬å†…å®¹ç”ŸæˆTXTæ–‡ä»¶
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼Œå½“å¤„ç†ä¸­æ–­æ—¶èƒ½ä»æ–­ç‚¹ç»§ç»­

ä½œè€…: Nexus-AI
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2025-09-12
"""

import os
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import argparse

from nexus_utils.agent_factory import create_agent_from_prompt_template

# å¯¼å…¥å·¥å…·å‡½æ•°
from tools.generated_tools.pdf_content_extractor.pdf_processing_tools import (
    pdf_to_images,
    manage_processing_state,
    merge_text_content,
    initialize_pdf_extraction,
    cleanup_extraction_files
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("pdf_extractor_agent")

from strands.telemetry import StrandsTelemetry
from nexus_utils.config_loader import ConfigLoader
loader = ConfigLoader()
# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ.setdefault("BYPASS_TOOL_CONSENT", "true")
otel_endpoint = loader.get_with_env_override(
    "OTEL_EXPORTER_OTLP_ENDPOINT",
    "nexus_ai", "OTEL_EXPORTER_OTLP_ENDPOINT",
    default="http://localhost:4318"
)
os.environ.setdefault("OTEL_EXPORTER_OTLP_ENDPOINT", otel_endpoint)
strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter()

class PDFExtractorAgent:
    """PDFå†…å®¹æå–Agentï¼Œå¤„ç†PDFæ–‡ä»¶å¹¶æå–æ–‡æœ¬å†…å®¹"""
    
    def __init__(self, cache_dir: str = ".cache", env: str = "production", version: str = "latest", model_id: str = "default"):
        """
        åˆå§‹åŒ–PDFæå–Agent
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œç”¨äºå­˜å‚¨ä¸´æ—¶æ–‡ä»¶
        """
        self.cache_dir = cache_dir
        self.multimodal_agent = None
        self.state = {}
        self.env = env
        self.version = version
        self.model_id = model_id
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        Path(cache_dir).mkdir(parents=True, exist_ok=True)
        
        logger.info(f"PDFæå–Agentåˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {cache_dir}")
    
    def _initialize_multimodal_agent(self) -> None:
        """åˆå§‹åŒ–å¤šæ¨¡æ€Agent"""
        if self.multimodal_agent is None:
            logger.info("åˆå§‹åŒ–å¤šæ¨¡æ€Agent...")
            
            # åˆ›å»º agent çš„é€šç”¨å‚æ•°
            agent_params = {
                "env": self.env,
                "version": self.version, 
                "model_id": self.model_id,
                "enable_logging": True
            }
            
            # ä½¿ç”¨ agent_factory åˆ›å»ºå¤šæ¨¡æ€åˆ†æ agent
            self.multimodal_agent = create_agent_from_prompt_template(
                agent_name="system_agents_prompts/multimodal_analysis/multimodal_analyzer_agent", 
                **agent_params
            )
            
            logger.info("å¤šæ¨¡æ€Agentåˆå§‹åŒ–å®Œæˆ")
    
    def process_pdf(self, pdf_path: str, output_file: Optional[str] = None, 
                   force_restart: bool = False, dpi: int = 300, 
                   cleanup: bool = True) -> Dict[str, Any]:
        """
        å¤„ç†PDFæ–‡ä»¶ï¼Œæå–æ–‡æœ¬å†…å®¹
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            force_restart: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¼€å§‹å¤„ç†
            dpi: å›¾åƒåˆ†è¾¨ç‡
            cleanup: æ˜¯å¦åœ¨å¤„ç†å®Œæˆåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # éªŒè¯å’Œä¿®æ­£è¾“å‡ºæ–‡ä»¶è·¯å¾„
            if output_file and os.path.isdir(output_file):
                # å¦‚æœè¾“å‡ºè·¯å¾„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                pdf_filename = os.path.basename(pdf_path)
                base_name = os.path.splitext(pdf_filename)[0]
                output_file = os.path.join(output_file, f"{base_name}.md")
                logger.info(f"è¾“å‡ºè·¯å¾„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å: {output_file}")
            
            # åˆå§‹åŒ–å¤šæ¨¡æ€Agent
            self._initialize_multimodal_agent()
            
            # åˆå§‹åŒ–PDFæå–è¿‡ç¨‹
            init_result = json.loads(initialize_pdf_extraction(
                pdf_path=pdf_path,
                output_dir=self.cache_dir,
                output_file=output_file,
                force_restart=force_restart
            ))
            
            if not init_result["success"]:
                logger.error(f"åˆå§‹åŒ–å¤±è´¥: {init_result['message']}")
                return {"success": False, "message": init_result["message"]}
            
            logger.info(f"PDFæå–åˆå§‹åŒ–å®Œæˆ: {init_result['message']}")
            
            # è·å–PDFä¿¡æ¯å’ŒçŠ¶æ€
            pdf_info = init_result["pdf_info"]
            is_resumed = init_result["is_resumed"]
            output_file = init_result["output_file"]
            
            # è¯»å–å¤„ç†çŠ¶æ€
            state_result = json.loads(manage_processing_state(
                action="read",
                pdf_path=pdf_path,
                state_dir=self.cache_dir
            ))
            
            if not state_result["success"]:
                logger.error(f"è¯»å–çŠ¶æ€å¤±è´¥: {state_result['message']}")
                return {"success": False, "message": state_result["message"]}
            
            self.state = state_result["state_data"]
            total_pages = self.state["processing_status"]["total_pages"]
            processed_pages = self.state["processing_status"]["processed_pages"]
            
            # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
            if is_resumed:
                logger.info(f"ä»æ–­ç‚¹ç»§ç»­å¤„ç†ï¼Œå·²å¤„ç† {len(processed_pages)}/{total_pages} é¡µ")
            else:
                logger.info(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶ï¼Œå…± {total_pages} é¡µ")
            
            # å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡
            pages_to_process = [i for i in range(total_pages) if i not in processed_pages]
            
            if not pages_to_process:
                logger.info("æ‰€æœ‰é¡µé¢å·²å¤„ç†å®Œæˆï¼Œåˆå¹¶æ–‡æœ¬å†…å®¹...")
            else:
                logger.info(f"éœ€è¦å¤„ç† {len(pages_to_process)} é¡µ...")
                
                # è½¬æ¢PDFä¸ºå›¾ç‰‡
                convert_result = json.loads(pdf_to_images(
                    pdf_path=pdf_path,
                    output_dir=self.cache_dir,
                    dpi=dpi,
                    image_format="png"
                ))
                
                if not convert_result["success"]:
                    logger.error(f"PDFè½¬å›¾ç‰‡å¤±è´¥: {convert_result['message']}")
                    return {"success": False, "message": convert_result["message"]}
                
                logger.info(f"PDFè½¬å›¾ç‰‡å®Œæˆ: {convert_result['message']}")
                
                # å¤„ç†æ¯ä¸€é¡µ
                for page_num in pages_to_process:
                    try:
                        # è·å–å›¾ç‰‡è·¯å¾„
                        image_path = f"{self.cache_dir}/page_{page_num}.png"
                        
                        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
                        if not os.path.exists(image_path):
                            logger.warning(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}ï¼Œè·³è¿‡å¤„ç†")
                            continue
                        
                        # ä½¿ç”¨å¤šæ¨¡æ€Agentæå–æ–‡æœ¬
                        logger.info(f"å¤„ç†ç¬¬ {page_num+1} é¡µ...")
                        
                        # æ„å»ºè¾“å…¥
                        agent_input = f"æ–‡ä»¶åˆ†æè¯·æ±‚: {image_path},åˆ†æ/å¤„ç†è¦æ±‚: æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œä¿æŒåŸå§‹æ ¼å¼"
                        
                        # è°ƒç”¨å¤šæ¨¡æ€Agent
                        start_time = time.time()
                        result = self.multimodal_agent(agent_input)
                        processing_time = time.time() - start_time
                        
                        # æå–æ–‡æœ¬å†…å®¹
                        extracted_text = ""
                        if hasattr(result, 'content') and result.content:
                            extracted_text = result.content
                        elif isinstance(result, str):
                            extracted_text = result
                        elif hasattr(result, 'text'):
                            extracted_text = result.text
                        else:
                            # å¦‚æœresultæ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            extracted_text = str(result)
                        
                        # ä¿å­˜æå–çš„æ–‡æœ¬
                        text_file = f"{self.cache_dir}/text_{page_num}.md"
                        with open(text_file, "w", encoding="utf-8") as f:
                            f.write(extracted_text)
                        
                        # æ›´æ–°å¤„ç†çŠ¶æ€
                        self.state["processing_status"]["processed_pages"].append(page_num)
                        self.state["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S")
                        
                        # ä¿å­˜çŠ¶æ€
                        update_result = json.loads(manage_processing_state(
                            action="update",
                            pdf_path=pdf_path,
                            state_dir=self.cache_dir,
                            state_data=self.state
                        ))
                        
                        if not update_result["success"]:
                            logger.warning(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {update_result['message']}")
                        
                        logger.info(f"ç¬¬ {page_num+1} é¡µå¤„ç†å®Œæˆ (ç”¨æ—¶: {processing_time:.2f}ç§’)")
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†ç¬¬ {page_num+1} é¡µæ—¶å‡ºé”™: {str(e)}")
                        # è®°å½•å¤±è´¥çš„é¡µé¢
                        if page_num not in self.state["processing_status"]["failed_pages"]:
                            self.state["processing_status"]["failed_pages"].append(page_num)
                            
                        # ä¿å­˜çŠ¶æ€
                        manage_processing_state(
                            action="update",
                            pdf_path=pdf_path,
                            state_dir=self.cache_dir,
                            state_data=self.state
                        )
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
            text_files = []
            for page_num in range(total_pages):
                text_file = f"{self.cache_dir}/text_{page_num}.md"
                if os.path.exists(text_file):
                    text_files.append(text_file)
            
            if text_files:
                merge_result = json.loads(merge_text_content(
                    input_files=text_files,
                    output_file=output_file,
                    include_page_numbers=True,
                    handle_missing_files=True
                ))
                
                if not merge_result["success"]:
                    logger.error(f"åˆå¹¶æ–‡æœ¬å¤±è´¥: {merge_result['message']}")
                    return {"success": False, "message": merge_result["message"]}
                
                logger.info(f"æ–‡æœ¬åˆå¹¶å®Œæˆ: {merge_result['message']}")
                
                # æ›´æ–°å¤„ç†çŠ¶æ€ä¸ºå®Œæˆ
                self.state["processing_status"]["completed"] = True
                self.state["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S")
                
                manage_processing_state(
                    action="update",
                    pdf_path=pdf_path,
                    state_dir=self.cache_dir,
                    state_data=self.state
                )
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if cleanup:
                    cleanup_result = json.loads(cleanup_extraction_files(
                        pdf_path=pdf_path,
                        cache_dir=self.cache_dir,
                        keep_state_file=True,
                        keep_output_file=True
                    ))
                    
                    if cleanup_result["success"]:
                        logger.info(f"æ¸…ç†å®Œæˆ: {cleanup_result['message']}")
                    else:
                        logger.warning(f"æ¸…ç†å¤±è´¥: {cleanup_result['message']}")
                
                return {
                    "success": True,
                    "message": f"PDFæ–‡æœ¬æå–å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_file}",
                    "output_file": output_file,
                    "total_pages": total_pages,
                    "processed_pages": len(self.state["processing_status"]["processed_pages"]),
                    "failed_pages": self.state["processing_status"]["failed_pages"]
                }
            else:
                return {
                    "success": False,
                    "message": "æ²¡æœ‰å¯åˆå¹¶çš„æ–‡æœ¬æ–‡ä»¶",
                    "output_file": None
                }
                
        except Exception as e:
            logger.error(f"PDFå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {"success": False, "message": f"å¤„ç†é”™è¯¯: {str(e)}"}
    
    def get_processing_status(self, pdf_path: str) -> Dict[str, Any]:
        """
        è·å–PDFå¤„ç†çŠ¶æ€
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†çŠ¶æ€å­—å…¸
        """
        try:
            state_result = json.loads(manage_processing_state(
                action="read",
                pdf_path=pdf_path,
                state_dir=self.cache_dir
            ))
            
            if not state_result["success"]:
                return {"success": False, "message": state_result["message"]}
            
            state_data = state_result["state_data"]
            total_pages = state_data["processing_status"]["total_pages"]
            processed_pages = state_data["processing_status"]["processed_pages"]
            failed_pages = state_data["processing_status"]["failed_pages"]
            completed = state_data["processing_status"]["completed"]
            
            return {
                "success": True,
                "pdf_path": pdf_path,
                "total_pages": total_pages,
                "processed_pages": len(processed_pages),
                "failed_pages": len(failed_pages),
                "completion_percentage": round(len(processed_pages) / total_pages * 100, 2) if total_pages > 0 else 0,
                "completed": completed,
                "output_file": state_data.get("output_file", None),
                "last_updated": state_data.get("last_updated", None)
            }
            
        except Exception as e:
            return {"success": False, "message": f"è·å–çŠ¶æ€é”™è¯¯: {str(e)}"}


# åˆ›å»º BedrockAgentCoreApp å®ä¾‹
from bedrock_agentcore.runtime import BedrockAgentCoreApp
import json as _json
app = BedrockAgentCoreApp()


# ==================== AgentCore å…¥å£ç‚¹ï¼ˆå¿…é¡»åŒ…å«ï¼‰====================
from typing import Dict, Any as TypingAny

@app.entrypoint
def handler(payload: Dict[str, TypingAny]) -> str:
    """
    AgentCore æ ‡å‡†å…¥å£ç‚¹

    Args:
        payload: AgentCore ä¼ å…¥çš„è¯·æ±‚ä½“ï¼ŒåŒ…å«:
            - prompt: ç”¨æˆ·æ¶ˆæ¯
            - pdf_path: PDFæ–‡ä»¶è·¯å¾„
            - output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Returns:
        str: å“åº”æ–‡æœ¬
    """
    print(f"ğŸ“¥ Received payload: {_json.dumps(payload, ensure_ascii=False)}")

    prompt = payload.get("prompt") or payload.get("message") or payload.get("input", "")
    pdf_path = payload.get("pdf_path") or payload.get("file_path")
    output_path = payload.get("output_path")

    if not prompt and not pdf_path:
        return "Error: Missing 'prompt' or 'pdf_path' in request"

    print(f"ğŸ”„ Processing: prompt={prompt}, pdf_path={pdf_path}")

    try:
        extractor = PDFExtractorAgent()
        if pdf_path:
            result = extractor.process_pdf(pdf_path, output_file=output_path)
            if result.get("success"):
                response_text = f"PDFå¤„ç†å®Œæˆ: {result.get('message', '')}"
                if result.get("output_file"):
                    response_text += f"\nè¾“å‡ºæ–‡ä»¶: {result['output_file']}"
                return response_text
            else:
                return f"Error: {result.get('message', 'Unknown error')}"
        else:
            return "Error: Please provide pdf_path parameter"
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return f"Error: {str(e)}"


# ==================== æœ¬åœ°è¿è¡Œå…¥å£ ====================
def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡ŒPDFæå–"""
    parser = argparse.ArgumentParser(description='PDFå†…å®¹æå–å·¥å…·')
    parser.add_argument('pdf_path', help='PDFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-c', '--cache', default='.cache', help='ç¼“å­˜ç›®å½•è·¯å¾„')
    parser.add_argument('-d', '--dpi', type=int, default=300, help='å›¾åƒåˆ†è¾¨ç‡')
    parser.add_argument('-f', '--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¼€å§‹å¤„ç†')
    parser.add_argument('-s', '--status', action='store_true', help='åªæ˜¾ç¤ºå¤„ç†çŠ¶æ€')
    parser.add_argument('--no-cleanup', action='store_true', help='ä¸æ¸…ç†ä¸´æ—¶æ–‡ä»¶')
    parser.add_argument('-e', '--env', type=str, default='production', help='æŒ‡å®šAgentè¿è¡Œç¯å¢ƒ (é»˜è®¤: production)')
    parser.add_argument('-v', '--version', type=str, default='latest', help='æŒ‡å®šAgentç‰ˆæœ¬ (é»˜è®¤: latest)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºPDFæå–Agent
    extractor = PDFExtractorAgent(cache_dir=args.cache, env=args.env, version=args.version)
    
    if args.status:
        # åªæ˜¾ç¤ºå¤„ç†çŠ¶æ€
        status = extractor.get_processing_status(args.pdf_path)
        if status["success"]:
            print(f"PDFæ–‡ä»¶: {args.pdf_path}")
            print(f"æ€»é¡µæ•°: {status['total_pages']}")
            print(f"å·²å¤„ç†é¡µæ•°: {status['processed_pages']}")
            print(f"å¤±è´¥é¡µæ•°: {status['failed_pages']}")
            print(f"å®Œæˆç™¾åˆ†æ¯”: {status['completion_percentage']}%")
            print(f"å¤„ç†çŠ¶æ€: {'å·²å®Œæˆ' if status['completed'] else 'æœªå®Œæˆ'}")
            if status["output_file"]:
                print(f"è¾“å‡ºæ–‡ä»¶: {status['output_file']}")
            if status["last_updated"]:
                print(f"æœ€åæ›´æ–°æ—¶é—´: {status['last_updated']}")
        else:
            print(f"è·å–çŠ¶æ€å¤±è´¥: {status['message']}")
    else:
        # å¤„ç†PDFæ–‡ä»¶
        print(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {args.pdf_path}")
        result = extractor.process_pdf(
            pdf_path=args.pdf_path,
            output_file=args.output,
            force_restart=args.force,
            dpi=args.dpi,
            cleanup=not args.no_cleanup
        )
        
        if result["success"]:
            print(f"å¤„ç†æˆåŠŸ: {result['message']}")
            print(f"è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
            print(f"æ€»é¡µæ•°: {result['total_pages']}")
            print(f"å·²å¤„ç†é¡µæ•°: {result['processed_pages']}")
            if result["failed_pages"]:
                print(f"å¤±è´¥é¡µæ•°: {len(result['failed_pages'])}")
                print(f"å¤±è´¥é¡µé¢: {result['failed_pages']}")
        else:
            print(f"å¤„ç†å¤±è´¥: {result['message']}")


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦åœ¨ Docker å®¹å™¨ä¸­è¿è¡Œï¼ˆAgentCore éƒ¨ç½²ï¼‰
    is_docker = os.environ.get("DOCKER_CONTAINER") == "1"

    if is_docker:
        # AgentCore éƒ¨ç½²æ¨¡å¼ï¼šå¯åŠ¨ HTTP æœåŠ¡å™¨
        print("ğŸš€ å¯åŠ¨ AgentCore HTTP æœåŠ¡å™¨ï¼Œç«¯å£: 8080")
        app.run()
    else:
        main()