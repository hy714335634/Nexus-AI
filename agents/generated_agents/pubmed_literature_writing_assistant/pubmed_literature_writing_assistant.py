#!/usr/bin/env python3
"""
PubMed Literature Writing Assistant

ä¸“é—¨è´Ÿè´£ç§‘ç ”æ–‡çŒ®ç¼–å†™å·¥ä½œï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„ææ–™å’Œæ€è·¯è¿›è¡Œæ–‡çŒ®ç»¼è¿°çš„ç¼–å†™å·¥ä½œã€‚
æ”¯æŒå¤„ç†å¤§é‡PubMedæ–‡çŒ®å¹¶ç”Ÿæˆé«˜è´¨é‡æ–‡çŒ®ç»¼è¿°ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ å’Œå¤šè¯­è¨€è¾“å‡ºåŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹ç‚¹:
- è¯»å–ç ”ç©¶IDå¯¹åº”çš„æ–‡çŒ®å…ƒæ•°æ®
- åŸºäºå…ƒæ•°æ®ç”Ÿæˆåˆå§‹æ–‡çŒ®ç»¼è¿°
- é€ç¯‡å¤„ç†æ–‡çŒ®å†…å®¹å¹¶æ›´æ–°ç»¼è¿°
- æ”¯æŒæ–­ç‚¹ç»­ä¼ åŠŸèƒ½
- å¤šè¯­è¨€è¾“å‡ºæ”¯æŒ
- ä¼šè¯æ•°æ®ç¼“å­˜
"""

import os
import json
import logging
import re
from time import sleep
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from nexus_utils.agent_factory import create_agent_from_prompt_template
from strands.telemetry import StrandsTelemetry

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é…ç½®é¥æµ‹
os.environ["BYPASS_TOOL_CONSENT"] = "true"
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "http://localhost:4318"
strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter()


class PubmedLiteratureWritingAssistant:
    """PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“ç±»"""
    
    def __init__(self, env: str = "production", version: str = "latest", model_id: str = "default"):
        """
        åˆå§‹åŒ–PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“
        
        Args:
            env (str): ç¯å¢ƒé…ç½® (development, production, testing)
            version (str): æ™ºèƒ½ä½“ç‰ˆæœ¬
            model_id (str): ä½¿ç”¨çš„æ¨¡å‹ID
        """
        self.env = env
        self.version = version
        self.model_id = model_id
        
        # æ™ºèƒ½ä½“å‚æ•°
        self.agent_params = {
            "env": self.env,
            "version": self.version,
            "model_id": self.model_id,
            "enable_logging": True
        }
        
        # æ™ºèƒ½ä½“é…ç½®è·¯å¾„
        self.agent_config_path = "generated_agents_prompts/pubmed_literature_writing_assistant/pubmed_literature_writing_assistant"
    
    def _create_agent(self):
        """åˆ›å»ºæ–°çš„æ™ºèƒ½ä½“å®ä¾‹"""
        return create_agent_from_prompt_template(
            agent_name=self.agent_config_path,
            **self.agent_params
        )
    
    def _get_processing_status(self, research_id: str) -> Dict:
        """è·å–å¤„ç†çŠ¶æ€"""
        try:
            cache_dir = Path(".cache/pmc_literature")
            research_dir = cache_dir / research_id
            status_file = research_dir / "step4.status"
            
            if not status_file.exists():
                total_literature = 0
                try:
                    manifest_path = research_dir / "manifest.json"
                    if manifest_path.exists():
                        with open(manifest_path, 'r', encoding='utf-8') as f:
                            manifest_data = json.load(f)
                        
                        if isinstance(manifest_data, dict):
                            if "marked_literature" in manifest_data:
                                marked_lit = manifest_data["marked_literature"]
                                all_literature = []
                                if isinstance(marked_lit, dict) and "by_year" in marked_lit:
                                    for year_data in marked_lit["by_year"].values():
                                        if isinstance(year_data, dict) and "literature" in year_data:
                                            all_literature.extend(year_data["literature"])
                                elif isinstance(marked_lit, dict) and "literature" in marked_lit:
                                    all_literature = marked_lit["literature"]
                                elif isinstance(marked_lit, list):
                                    all_literature = marked_lit
                                total_literature = len(all_literature)
                            elif "literature" in manifest_data:
                                total_literature = len(manifest_data["literature"]) if isinstance(manifest_data["literature"], list) else 1
                        elif isinstance(manifest_data, list):
                            total_literature = len(manifest_data)
                except Exception as e:
                    logger.warning(f"è¯»å–manifest.jsonå¤±è´¥: {str(e)}")
                
                initial_status = {
                    "research_id": research_id,
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat(),
                    "processed_literature": [],
                    "current_version": None,
                    "version_file_path": None,
                    "total_literature": total_literature,
                    "completed": False
                }
                
                with open(status_file, 'w', encoding='utf-8') as f:
                    json.dump(initial_status, f, ensure_ascii=False, indent=2)
                
                return initial_status
            
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
            
            return status
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            return None
    
    def _load_marked_literature_ids(self, research_id: str) -> List[str]:
        """ä»manifest.jsonåŠ è½½è¢«æ ‡è®°çš„æ–‡çŒ®IDåˆ—è¡¨"""
        try:
            manifest_path = Path(".cache/pmc_literature") / research_id / "manifest.json"
            
            if not manifest_path.exists():
                logger.error(f"manifest.jsonä¸å­˜åœ¨: {manifest_path}")
                return []
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_data = json.load(f)
            
            pmc_ids = []
            
            if isinstance(manifest_data, dict):
                if "marked_literature" in manifest_data:
                    marked_lit = manifest_data["marked_literature"]
                    if isinstance(marked_lit, dict) and "by_year" in marked_lit:
                        for year_data in marked_lit["by_year"].values():
                            if isinstance(year_data, dict) and "literature" in year_data:
                                for lit in year_data["literature"]:
                                    lit_id = lit.get("pmcid") or lit.get("id") or lit.get("pmid")
                                    if lit_id:
                                        pmc_ids.append(lit_id)
                    elif isinstance(marked_lit, dict) and "literature" in marked_lit:
                        for lit in marked_lit["literature"]:
                            lit_id = lit.get("pmcid") or lit.get("id") or lit.get("pmid")
                            if lit_id:
                                pmc_ids.append(lit_id)
                    elif isinstance(marked_lit, list):
                        for lit in marked_lit:
                            lit_id = lit.get("pmcid") or lit.get("id") or lit.get("pmid")
                            if lit_id:
                                pmc_ids.append(lit_id)
            
            return pmc_ids
            
        except Exception as e:
            logger.error(f"åŠ è½½æ ‡è®°æ–‡çŒ®IDå¤±è´¥: {str(e)}")
            return []
    
    def _load_analysis_results(self, research_id: str, pmc_ids: List[str] = None) -> List[Dict]:
        """ä»analysis_resultsåŠ è½½å®Œæ•´çš„æ–‡çŒ®åˆ†æç»“æœ"""
        try:
            analysis_dir = Path(".cache/pmc_literature") / research_id / "analysis_results"
            
            if not analysis_dir.exists():
                logger.warning(f"analysis_resultsç›®å½•ä¸å­˜åœ¨: {analysis_dir}")
                return []
            
            if pmc_ids is None:
                # åŠ è½½æ‰€æœ‰analysis_resultæ–‡ä»¶
                analysis_files = list(analysis_dir.glob("*.json"))
            else:
                # åªåŠ è½½æŒ‡å®šçš„pmcid
                analysis_files = [analysis_dir / f"{pmcid}.json" for pmcid in pmc_ids]
            
            results = []
            
            for analysis_file in analysis_files:
                if not analysis_file.exists():
                    logger.warning(f"åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {analysis_file}")
                    continue
                
                try:
                    with open(analysis_file, 'r', encoding='utf-8') as f:
                        analysis_data = json.load(f)
                    results.append(analysis_data)
                except Exception as e:
                    logger.error(f"è¯»å–åˆ†æç»“æœå¤±è´¥ {analysis_file}: {str(e)}")
                    continue
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(results)} ä¸ªåˆ†æç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"åŠ è½½åˆ†æç»“æœå¤±è´¥: {str(e)}")
            return []
    
    def _load_literature_metadata(self, research_id: str) -> List[Dict]:
        """åŠ è½½æ–‡çŒ®å…ƒæ•°æ®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        try:
            manifest_path = Path(".cache/pmc_literature") / research_id / "manifest.json"
            
            if not manifest_path.exists():
                logger.error(f"manifest.jsonä¸å­˜åœ¨: {manifest_path}")
                return []
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_data = json.load(f)
            
            if isinstance(manifest_data, dict):
                if "marked_literature" in manifest_data:
                    marked_lit = manifest_data["marked_literature"]
                    all_literature = []
                    if isinstance(marked_lit, dict) and "by_year" in marked_lit:
                        for year_data in marked_lit["by_year"].values():
                            if isinstance(year_data, dict) and "literature" in year_data:
                                all_literature.extend(year_data["literature"])
                    elif isinstance(marked_lit, dict) and "literature" in marked_lit:
                        all_literature = marked_lit["literature"]
                    elif isinstance(marked_lit, list):
                        all_literature = marked_lit
                    return all_literature
                elif "literature" in manifest_data:
                    return manifest_data["literature"]
            elif isinstance(manifest_data, list):
                return manifest_data
            
            return []
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡çŒ®å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _get_pending_literature(self, research_id: str) -> Optional[Dict]:
        """è·å–å¾…å¤„ç†çš„ä¸€ç¯‡æ–‡çŒ®ï¼ˆæŒ‰å½±å“å› å­ä»é«˜åˆ°ä½æ’åºï¼Œåªè¿”å›analysis_resultsä¸­çš„å…ƒæ•°æ®ï¼Œä¸æä¾›å…¨æ–‡ï¼‰"""
        try:
            # è·å–æ‰€æœ‰è¢«æ ‡è®°çš„æ–‡çŒ®ID
            all_pmc_ids = self._load_marked_literature_ids(research_id)
            
            if not all_pmc_ids:
                return None
            
            status = self._get_processing_status(research_id)
            if not status:
                return None
            
            processed_ids = status.get("processed_literature", [])
            
            # è·å–æ‰€æœ‰æœªå¤„ç†æ–‡çŒ®çš„åˆ†æç»“æœ
            pending_literatures = []
            
            for lit_id in all_pmc_ids:
                if lit_id not in processed_ids:
                    # ä»analysis_resultsåŠ è½½å®Œæ•´çš„åˆ†æç»“æœ
                    analysis_results = self._load_analysis_results(research_id, [lit_id])
                    
                    if analysis_results and len(analysis_results) > 0:
                        analysis_data = analysis_results[0]
                        
                        # æå–å½±å“å› å­ï¼Œé»˜è®¤å€¼ä¸º0
                        impact_factor = 0
                        try:
                            impact_factor_raw = analysis_data.get("impact_factor", 0)
                            if isinstance(impact_factor_raw, str):
                                impact_factor = float(impact_factor_raw)
                            else:
                                impact_factor = float(impact_factor_raw) if impact_factor_raw else 0
                        except (ValueError, TypeError):
                            impact_factor = 0
                        
                        pending_literatures.append({
                            "pmcid": lit_id,
                            "metadata": analysis_data,
                            "impact_factor": impact_factor,
                            "has_fulltext": False
                        })
            
            if not pending_literatures:
                return None
            
            # æŒ‰å½±å“å› å­ä»é«˜åˆ°ä½æ’åº
            pending_literatures.sort(key=lambda x: x["impact_factor"], reverse=True)
            
            # è¿”å›å½±å“å› å­æœ€é«˜çš„æœªå¤„ç†æ–‡çŒ®
            top_literature = pending_literatures[0]
            logger.info(f"é€‰æ‹©å¾…å¤„ç†æ–‡çŒ®: {top_literature['pmcid']} (å½±å“å› å­: {top_literature['impact_factor']})")
            
            return {
                "pmcid": top_literature["pmcid"],
                "metadata": top_literature["metadata"],
                "has_fulltext": top_literature["has_fulltext"]
            }
            
        except Exception as e:
            logger.error(f"è·å–å¾…å¤„ç†æ–‡çŒ®å¤±è´¥: {str(e)}")
            return None
    
    def _get_latest_review(self, research_id: str) -> Optional[str]:
        """è·å–æœ€æ–°ç”Ÿæˆçš„ç»¼è¿°å†…å®¹"""
        try:
            # æ–¹æ³•1ï¼šä¼˜å…ˆä»statusæ–‡ä»¶è¯»å–å½“å‰ç‰ˆæœ¬è·¯å¾„
            status = self._get_processing_status(research_id)
            if status and status.get("version_file_path"):
                version_path = status["version_file_path"]
                # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
                if not os.path.isabs(version_path):
                    file_path = Path(version_path)
                else:
                    file_path = Path(version_path)
                
                if file_path.exists():
                    logger.info(f"ä»statusè¯»å–ç‰ˆæœ¬æ–‡ä»¶: {version_path}")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    return content
            
            # æ–¹æ³•2ï¼šè§£ææ–‡ä»¶åä¸­çš„ç‰ˆæœ¬å·ï¼ŒæŒ‰ç‰ˆæœ¬æ’åº
            reviews_dir = Path(".cache/pmc_literature") / research_id / "reviews"
            
            if not reviews_dir.exists():
                return None
            
            # è§£ææ‰€æœ‰æ–‡ä»¶ï¼Œæå–ç‰ˆæœ¬å·
            def extract_version(filename: str) -> tuple:
                """æå–ç‰ˆæœ¬å·å’Œä¼˜å…ˆçº§
                è¿”å›: (priority, version_number, timestamp_str)
                initial=0, æ•°å­—ç‰ˆæœ¬ç›´æ¥æ¯”è¾ƒ"""
                name = filename.name
                if name.startswith("review_initial_"):
                    timestamp = name.replace("review_initial_", "").replace(".md", "")
                    return (0, 0, timestamp)
                elif name.startswith("review_v"):
                    # review_v{version}_{timestamp}.md
                    parts = name.replace("review_v", "").replace(".md", "").split("_")
                    if parts:
                        try:
                            version_num = int(parts[0])
                            timestamp = "_".join(parts[1:]) if len(parts) > 1 else ""
                            return (1, version_num, timestamp)
                        except:
                            return (2, 0, name)  # æ— æ³•è§£æï¼Œæ”¾åˆ°æœ€å
                elif name.startswith("review_final_"):
                    timestamp = name.replace("review_final_", "").replace(".md", "")
                    return (3, 999999, timestamp)  # final æ”¾åˆ°æœ€åä½†ä¼˜å…ˆçº§æœ€é«˜
                return (4, -1, name)  # æœªçŸ¥æ ¼å¼
            
            review_files = list(reviews_dir.glob("review_*.md"))
            if not review_files:
                return None
            
            # æŒ‰ç‰ˆæœ¬å·æ’åº
            sorted_files = sorted(review_files, key=extract_version)
            
            latest_file = sorted_files[-1]  # è·å–ç‰ˆæœ¬å·æœ€å¤§çš„
            logger.info(f"è·å–æœ€æ–°ç‰ˆæœ¬æ–‡ä»¶: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return content
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°ç»¼è¿°å¤±è´¥: {str(e)}")
            return None
    
    def _save_review_version(self, research_id: str, content: str, version: Union[str, int]) -> Dict:
        """ä¿å­˜æ–‡çŒ®ç»¼è¿°ç‰ˆæœ¬"""
        try:
            reviews_dir = Path(".cache/pmc_literature") / research_id / "reviews"
            os.makedirs(reviews_dir, exist_ok=True)
            
            version_str = str(version).lower()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if version_str in ["initial", "final"]:
                filename = f"review_{version_str}_{timestamp}.md"
            else:
                filename = f"review_v{version_str}_{timestamp}.md"
            
            file_path = reviews_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return {
                "status": "success",
                "research_id": research_id,
                "version": version_str,
                "file_path": str(file_path),
                "timestamp": timestamp
            }
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»¼è¿°ç‰ˆæœ¬å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "message": f"ä¿å­˜å¤±è´¥: {str(e)}"
            }
    
    def _update_processing_status(self, research_id: str, 
                                 processed_literature_id: str, version: Union[str, int],
                                 version_file_path: str = None) -> Dict:
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        try:
            cache_dir = Path(".cache/pmc_literature")
            research_dir = cache_dir / research_id
            status_file = research_dir / "step4.status"
            
            if status_file.exists():
                with open(status_file, 'r', encoding='utf-8') as f:
                    status = json.load(f)
            else:
                status = self._get_processing_status(research_id)
            
            if processed_literature_id and processed_literature_id.lower() not in ["initial_marker"]:
                if processed_literature_id not in status["processed_literature"]:
                    status["processed_literature"].append(processed_literature_id)
            
            status["current_version"] = str(version)
            status["updated_at"] = datetime.now().isoformat()
            
            if version_file_path:
                status["version_file_path"] = version_file_path
            
            total = status.get("total_literature", 0)
            processed = len(status["processed_literature"])
            
            if total > 0 and processed >= total:
                status["completed"] = True
            
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
            
            return {
                "status": "success",
                "processed": processed,
                "total": total,
                "completed": status["completed"]
            }
            
        except Exception as e:
            logger.error(f"æ›´æ–°å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "message": str(e)
            }
    
    def _mark_all_metadata_processed(self, research_id: str) -> Dict:
        """æ ‡è®°æ‰€æœ‰æ–‡çŒ®å…ƒæ•°æ®å·²å¤„ç†"""
        return self._update_processing_status(research_id, "initial_marker", "initial")
    
    def _mark_selected_papers_processed(self, research_id: str, pmc_ids: List[str]) -> None:
        """æ ‡è®°é€‰ä¸­çš„æ–‡çŒ®ä¸ºå·²å¤„ç†"""
        try:
            cache_dir = Path(".cache/pmc_literature")
            research_dir = cache_dir / research_id
            status_file = research_dir / "step4.status"
            
            if status_file.exists():
                with open(status_file, 'r', encoding='utf-8') as f:
                    status = json.load(f)
            else:
                status = self._get_processing_status(research_id)
            
            # å°†é€‰ä¸­çš„æ–‡çŒ®IDæ·»åŠ åˆ°å·²å¤„ç†åˆ—è¡¨
            for pmc_id in pmc_ids:
                if pmc_id and pmc_id not in status.get("processed_literature", []):
                    status["processed_literature"].append(pmc_id)
            
            status["updated_at"] = datetime.now().isoformat()
            
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æˆåŠŸæ ‡è®° {len(pmc_ids)} ç¯‡æ–‡çŒ®ä¸ºå·²å¤„ç†")
            
        except Exception as e:
            logger.error(f"æ ‡è®°é€‰ä¸­æ–‡çŒ®ä¸ºå·²å¤„ç†å¤±è´¥: {str(e)}")
    
    def _retry_agent_call(self, agent_input: str, max_retries: int = 5, retry_delay: int = 150):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ Agent è°ƒç”¨
        
        Args:
            agent_input: è¾“å…¥ç»™ Agent çš„å†…å®¹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
            
        Returns:
            AgentResult å¯¹è±¡
        """
        for attempt in range(1, max_retries + 1):
            agent_response = None
            try:
                logger.info(f"è°ƒç”¨ Agentï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰")
                # æ¯æ¬¡è°ƒç”¨å‰åˆ›å»ºæ–°çš„agentå®ä¾‹
                agent = self._create_agent()
                agent_response = agent(agent_input)
                logger.info("="*100)
                print("="*100)
                print(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                print(f"agent_response: {agent_response}")
                logger.info(f"agent_response: {agent_response}")
                logger.info(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                logger.info(f"Execution time: {sum(agent_response.metrics.cycle_durations):.2f} seconds")
                logger.info(f"Tools used: {list(agent_response.metrics.tool_metrics.keys())}")
                print("="*100)
                logger.info("="*100)
                logger.info(f"âœ… Agent è°ƒç”¨æˆåŠŸï¼ˆå°è¯• {attempt}ï¼‰")
                return agent_response
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Agent è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {str(e)}")
                
                if attempt < max_retries:
                    logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    # åªæœ‰åœ¨agent_responseå­˜åœ¨æ—¶æ‰æ‰“å°metrics
                    if agent_response and hasattr(agent_response, 'metrics'):
                        logger.info("="*100)
                        print(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                        logger.info(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                        logger.info(f"Execution time: {sum(agent_response.metrics.cycle_durations):.2f} seconds")
                        logger.info(f"Tools used: {list(agent_response.metrics.tool_metrics.keys())}")
                    sleep(retry_delay)
                else:
                    logger.error(f"âŒ Agent è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                    raise
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"Agent è°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
    
    def _extract_key_fields(self, metadata: Dict) -> Dict:
        """æå–æ–‡çŒ®å…ƒæ•°æ®çš„å…³é”®å­—æ®µ
        
        Args:
            metadata: å®Œæ•´çš„æ–‡çŒ®å…ƒæ•°æ®å­—å…¸
            
        Returns:
            åªåŒ…å«å…³é”®å­—æ®µçš„å­—å…¸
        """
        key_fields = [
            "pmcid", "title", "abstract", "methods", "results", 
            "conclusions", "publication_date", "reasoning", "key_findings"
        ]
        
        extracted = {}
        for field in key_fields:
            if field in metadata:
                value = metadata[field]
                # key_findingså¦‚æœæ˜¯åˆ—è¡¨ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if field == "key_findings" and isinstance(value, list):
                    extracted[field] = value
                else:
                    extracted[field] = value
        
        return extracted
    
    def _format_metadata_for_agent(self, metadata_list: List[Dict]) -> str:
        """å°†å…ƒæ•°æ®åˆ—è¡¨æ ¼å¼åŒ–ä¸ºç´§å‡‘çš„æ–‡æœ¬æ ¼å¼ä»¥å‡å°‘token
        
        ä½¿ç”¨ç´§å‡‘æ ¼å¼ï¼šæ¯ç¯‡æ–‡çŒ®ç”¨åˆ†éš”ç¬¦åˆ†éš”ï¼Œå­—æ®µç”¨ç®€çŸ­æ ‡ç­¾
        æ ¼å¼è¯´æ˜ï¼š[åºå·]ID:xxx|T:æ ‡é¢˜|A:æ‘˜è¦|M:æ–¹æ³•|R:ç»“æœ|C:ç»“è®º|Date:æ—¥æœŸ|Reason:ç†ç”±|Findings:å‘ç°
        æ³¨æ„ï¼šä¸é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œä¿ç•™å®Œæ•´å†…å®¹
        
        Args:
            metadata_list: æ–‡çŒ®å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„ç´§å‡‘æ–‡æœ¬å­—ç¬¦ä¸²
        """
        if not metadata_list:
            return ""
        
        formatted_items = []
        for i, meta in enumerate(metadata_list, 1):
            # æå–å…³é”®å­—æ®µ
            key_meta = self._extract_key_fields(meta)
            
            # æ„å»ºç´§å‡‘æ ¼å¼
            parts = [f"[{i}]"]
            
            if "pmcid" in key_meta:
                parts.append(f"ID:{key_meta['pmcid']}")
            
            if "title" in key_meta:
                parts.append(f"T:{key_meta['title']}")
            
            if "abstract" in key_meta:
                parts.append(f"A:{key_meta['abstract']}")
            
            if "methods" in key_meta:
                parts.append(f"M:{key_meta['methods']}")
            
            if "results" in key_meta:
                parts.append(f"R:{key_meta['results']}")
            
            if "conclusions" in key_meta:
                parts.append(f"C:{key_meta['conclusions']}")
            
            if "publication_date" in key_meta:
                parts.append(f"Date:{key_meta['publication_date']}")
            
            if "reasoning" in key_meta:
                parts.append(f"Reason:{key_meta['reasoning']}")
            
            if "key_findings" in key_meta and isinstance(key_meta['key_findings'], list):
                findings = "|".join(key_meta['key_findings'])
                parts.append(f"Findings:{findings}")
            
            formatted_items.append("|".join(parts))
        
        return "\n".join(formatted_items)
    
    def _parse_agent_json_response(self, agent_response: Any) -> Optional[Dict]:
        """ä»agent_responseä¸­æå–å¹¶è§£æJSONç»“æœ"""
        try:
            # æå–æ–‡æœ¬å†…å®¹
            text_content = None
            if hasattr(agent_response, 'message'):
                message = agent_response.message
                if isinstance(message, str):
                    text_content = message
                elif isinstance(message, dict) and 'content' in message:
                    content_list = message['content']
                    if content_list and isinstance(content_list, list):
                        text_content = content_list[0].get('text', '') if isinstance(content_list[0], dict) else str(content_list[0])
            elif isinstance(agent_response, str):
                text_content = agent_response
            elif isinstance(agent_response, dict):
                return agent_response
            
            if not text_content:
                return None
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥è§£æJSON
            try:
                return json.loads(text_content.strip())
            except json.JSONDecodeError:
                pass
            
            # æ–¹æ³•2: æŸ¥æ‰¾```jsonä»£ç å—
            json_block_match = re.search(r'```json\s*([\s\S]*?)\s*```', text_content)
            if json_block_match:
                json_str = json_block_match.group(1).strip()
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    pass
            
            # æ–¹æ³•3: ä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            json_end = -1
            json_start = -1
            brace_count = 0
            
            for i in range(len(text_content) - 1, -1, -1):
                if text_content[i] == '}':
                    json_end = i + 1
                    brace_count = 1
                    for j in range(i - 1, -1, -1):
                        char = text_content[j]
                        if char == '}':
                            brace_count += 1
                        elif char == '{':
                            brace_count -= 1
                            if brace_count == 0:
                                json_start = j
                                json_str = text_content[json_start:json_end].strip()
                                try:
                                    return json.loads(json_str)
                                except json.JSONDecodeError:
                                    pass
                                break
                    if json_start >= 0:
                        break
            
        except (AttributeError, KeyError, IndexError, TypeError) as e:
            logger.error(f"è§£æAgentå“åº”å¤±è´¥: {str(e)}")
        return None
    
    def generate_literature_review(self, research_id: str, requirement: str = None, 
                                  language: str = "english", max_paper_to_init: int = 50) -> str:
        """ç”Ÿæˆæ–‡çŒ®ç»¼è¿°ï¼ˆä¸»æ§åˆ¶å¾ªç¯ï¼‰"""
        try:
            all_results = []
            
            while True:
                status = self._get_processing_status(research_id)
                if not status:
                    return "è·å–å¤„ç†çŠ¶æ€å¤±è´¥"
                
                processed_count = len(status.get("processed_literature", []))
                total_count = status.get("total_literature", 0)
                pending_count = total_count - processed_count
                
                logger.info(f"å½“å‰è¿›åº¦: {processed_count}/{total_count}, å¾…å¤„ç†: {pending_count}")
                
                if processed_count == 0 and not status.get("current_version"):
                    # æƒ…å†µAï¼šç”Ÿæˆåˆå§‹ç‰ˆæœ¬
                    logger.info("æ‰€æœ‰æ–‡çŒ®æœªå¤„ç†ï¼Œç”Ÿæˆåˆå§‹ç‰ˆæœ¬")
                    
                    # ä»manifest.jsonè·å–è¢«æ ‡è®°çš„æ–‡çŒ®IDåˆ—è¡¨
                    pmc_ids = self._load_marked_literature_ids(research_id)
                    if not pmc_ids:
                        return "æ— æ³•åŠ è½½è¢«æ ‡è®°çš„æ–‡çŒ®IDåˆ—è¡¨"
                    
                    # ä»analysis_resultsåŠ è½½å®Œæ•´çš„åˆ†æç»“æœ
                    all_analysis_results = self._load_analysis_results(research_id, pmc_ids)
                    if not all_analysis_results:
                        return "æ— æ³•åŠ è½½æ–‡çŒ®åˆ†æç»“æœ"
                    
                    # æŒ‰ç›¸å…³æ€§åˆ†æ•°å’Œå½±å“å› å­æ’åºå¹¶å–å‰max_paper_to_initç¯‡
                    total_papers = len(all_analysis_results)
                    if total_papers > max_paper_to_init:
                        # æå–ç›¸å…³æ€§åˆ†æ•°å’Œå½±å“å› å­ç”¨äºæ’åº
                        for item in all_analysis_results:
                            # æå–ç›¸å…³æ€§åˆ†æ•°
                            relevance_score = 0
                            try:
                                relevance_score_raw = item.get("relevance_score", 0)
                                if isinstance(relevance_score_raw, str):
                                    relevance_score = float(relevance_score_raw)
                                else:
                                    relevance_score = float(relevance_score_raw) if relevance_score_raw else 0
                            except (ValueError, TypeError):
                                relevance_score = 0
                            item["_relevance_score_for_sort"] = relevance_score
                            
                            # æå–å½±å“å› å­
                            impact_factor = 0
                            try:
                                impact_factor_raw = item.get("impact_factor", 0)
                                if isinstance(impact_factor_raw, str):
                                    impact_factor = float(impact_factor_raw)
                                else:
                                    impact_factor = float(impact_factor_raw) if impact_factor_raw else 0
                            except (ValueError, TypeError):
                                impact_factor = 0
                            item["_impact_factor_for_sort"] = impact_factor
                        
                        # å…ˆæŒ‰ç›¸å…³æ€§åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œå–å‰max_paper_to_initç¯‡
                        papers_by_relevance = all_analysis_results.copy()
                        papers_by_relevance.sort(key=lambda x: x.get("_relevance_score_for_sort", 0), reverse=True)
                        top_by_relevance = papers_by_relevance[:max_paper_to_init]
                        
                        # å†æŒ‰å½±å“å› å­ä»é«˜åˆ°ä½æ’åºï¼Œå–å‰max_paper_to_initç¯‡
                        papers_by_impact = all_analysis_results.copy()
                        papers_by_impact.sort(key=lambda x: x.get("_impact_factor_for_sort", 0), reverse=True)
                        top_by_impact = papers_by_impact[:max_paper_to_init]
                        
                        # åˆå¹¶ä¸¤ä¸ªç»“æœå¹¶å»é‡ï¼ˆä½¿ç”¨pmcidä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                        selected_pmc_ids_set = set()
                        selected_results = []
                        
                        # å…ˆæ·»åŠ æŒ‰ç›¸å…³æ€§æ’åºçš„ç»“æœ
                        for item in top_by_relevance:
                            pmcid = item.get("pmcid")
                            if pmcid and pmcid not in selected_pmc_ids_set:
                                selected_pmc_ids_set.add(pmcid)
                                selected_results.append(item)
                        
                        # å†æ·»åŠ æŒ‰å½±å“å› å­æ’åºçš„ç»“æœï¼ˆé¿å…é‡å¤ï¼‰
                        for item in top_by_impact:
                            pmcid = item.get("pmcid")
                            if pmcid and pmcid not in selected_pmc_ids_set:
                                selected_pmc_ids_set.add(pmcid)
                                selected_results.append(item)
                        
                        # è®°å½•ä½¿ç”¨çš„æ–‡çŒ®IDï¼Œä»¥ä¾¿åç»­æ ‡è®°ä¸ºå·²å¤„ç†
                        selected_pmc_ids = list(selected_pmc_ids_set)
                        
                        logger.info(f"ä» {total_papers} ä¸ªæ–‡çŒ®ä¸­ï¼šæŒ‰ç›¸å…³æ€§é€‰æ‹©äº† {len(top_by_relevance)} ç¯‡ï¼ŒæŒ‰å½±å“å› å­é€‰æ‹©äº† {len(top_by_impact)} ç¯‡ï¼Œåˆå¹¶å»é‡åå…± {len(selected_results)} ç¯‡ç”¨äºç”Ÿæˆåˆå§‹ç‰ˆæœ¬")
                        all_analysis_results = selected_results
                    else:
                        # å¦‚æœæ–‡çŒ®æ•°é‡ä¸è¶…è¿‡é™åˆ¶ï¼Œè®°å½•æ‰€æœ‰æ–‡çŒ®ID
                        selected_pmc_ids = [item.get("pmcid") for item in all_analysis_results if item.get("pmcid")]
                    
                    logger.info(f"åŠ è½½äº† {len(all_analysis_results)} ä¸ªåˆ†æç»“æœç”¨äºåˆå§‹ç‰ˆæœ¬")
                    
                    agent_input = f"""
====================é¡¹ç›®åŸºç¡€ä¿¡æ¯====================
ç ”ç©¶ID: {research_id}
è¾“å‡ºè¯­è¨€: {language}
æ–‡çŒ®æ•°é‡: {len(all_analysis_results)}
============================================================
ç”¨æˆ·ç ”ç©¶éœ€æ±‚:
{requirement if requirement else "æ— ç‰¹æ®Šéœ€æ±‚"}
============================================================
æ–‡çŒ®å®Œæ•´åˆ†æç»“æœï¼ˆæ ¼å¼è¯´æ˜ï¼šæ¯è¡Œä¸€ç¯‡æ–‡çŒ®ï¼Œæ ¼å¼ä¸º[åºå·]ID:xxx|T:æ ‡é¢˜|A:æ‘˜è¦|M:æ–¹æ³•|R:ç»“æœ|C:ç»“è®º|Date:æ—¥æœŸ|Reason:ç†ç”±|Findings:å‘ç°ï¼‰:
{self._format_metadata_for_agent(all_analysis_results)}
============================================================
### ä»»åŠ¡:è¯·åŸºäºè¿™äº›æ–‡çŒ®çš„å®Œæ•´åˆ†æç»“æœç”Ÿæˆåˆå§‹ç‰ˆæœ¬çš„æ–‡çŒ®ç»¼è¿°ï¼Œå¹¶åœ¨å®Œæˆåä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š

ä½ çš„ä»»åŠ¡ï¼š
1. ä»”ç»†é˜…è¯»ç”¨æˆ·éœ€æ±‚ï¼Œä»¥åŠæ‰€æœ‰æä¾›çš„æ–‡ç« åˆ†æç»“æœ
2. ç”Ÿæˆåˆå§‹ç‰ˆæœ¬çš„æ–‡çŒ®ç»¼è¿°ï¼Œè¦æ±‚æœ‰é€»è¾‘æ€§ï¼Œå†…å®¹è¯¦ç»†
3. ä½¿ç”¨`file_write`å·¥å…·å°†ç»¼è¿°å†…å®¹ä¿å­˜åˆ°æ–‡ä»¶
4. **å¿…é¡»**ä»¥JSONæ ¼å¼è¿”å›ç»“æœ,ä¸è¦è¿”å›å…¶ä»–å†…å®¹ï¼š
```json
{{
    "status": "success",
    "research_id": "{research_id}",
    "version": "initial",
    "file_path": "ä¿å­˜çš„æ–‡ä»¶è·¯å¾„",
    "message": "æˆåŠŸç”Ÿæˆåˆå§‹ç‰ˆæœ¬"
}}
```
============================================================
"""
                    
                    logger.info("è°ƒç”¨Agentç”Ÿæˆåˆå§‹ç‰ˆæœ¬")
                    agent_response = self._retry_agent_call(agent_input)
                    result = self._parse_agent_json_response(agent_response)
                    
                    if result and isinstance(result, dict) and result.get("status") == "success":
                        file_path = result.get("file_path", "")
                        
                        # æ ‡è®°åˆå§‹ç‰ˆæœ¬ä½¿ç”¨çš„æ–‡çŒ®ä¸ºå·²å¤„ç†
                        # self._mark_selected_papers_processed(research_id, selected_pmc_ids)
                        
                        if file_path:
                            logger.info(f"åˆå§‹ç‰ˆæœ¬ä¿å­˜æˆåŠŸ: {file_path}")
                            all_results.append(f"âœ… åˆå§‹ç‰ˆæœ¬ç”ŸæˆæˆåŠŸ\næ–‡ä»¶è·¯å¾„: {file_path}\nä½¿ç”¨äº† {len(selected_pmc_ids)} ç¯‡æ–‡çŒ®\n")
                            
                            self._update_processing_status(research_id, 
                                                          "initial_marker", "initial", 
                                                          file_path)
                        else:
                            all_results.append(f"âš ï¸ åˆå§‹ç‰ˆæœ¬ç”ŸæˆæˆåŠŸï¼Œä½†æœªè·å–åˆ°æ–‡ä»¶è·¯å¾„")
                            self._update_processing_status(research_id, 
                                                          "initial_marker", "initial", 
                                                          None)
                    else:
                        all_results.append(f"âŒ Agentç”Ÿæˆåˆå§‹ç‰ˆæœ¬å¤±è´¥")
                        break
                
                elif pending_count > 0:
                    # æƒ…å†µBï¼šç»§ç»­å¤„ç†æœªå¤„ç†æ–‡çŒ®
                    logger.info(f"ç»§ç»­å¤„ç†ï¼Œè¿˜æœ‰ {pending_count} ç¯‡æ–‡çŒ®å¾…å¤„ç†")
                    
                    pending_literature = self._get_pending_literature(research_id)
                    
                    if not pending_literature:
                        logger.info("æ²¡æœ‰å¾…å¤„ç†çš„æ–‡çŒ®")
                        break
                    
                    lit_id = pending_literature["pmcid"]
                    logger.info(f"å¤„ç†æ–‡çŒ®: {lit_id}")
                    
                    latest_review = self._get_latest_review(research_id)
                    
                    if not latest_review:
                        all_results.append("âŒ æ— æ³•è·å–æœ€æ–°ç»¼è¿°")
                        break
                    
                    current_version = status.get("current_version", "initial")
                    if current_version == "initial":
                        next_version = 1
                    else:
                        try:
                            next_version = int(current_version) + 1
                        except:
                            next_version = 1
                    
                    # è·å–æ–°æ–‡çŒ®çš„å®Œæ•´åˆ†æç»“æœå…ƒæ•°æ®
                    new_literature_metadata = pending_literature.get('metadata', {})
                    
                    agent_input = f"""
====================é¡¹ç›®åŸºç¡€ä¿¡æ¯====================
ç ”ç©¶ID: {research_id}
ç°æœ‰ç‰ˆæœ¬æ–‡çŒ®åœ°å€: {status.get('version_file_path', 'N/A')}
æ–°æ–‡çŒ®ID: {lit_id}
æ–°æ–‡çŒ®åˆ†æç»“æœè·¯å¾„: .cache/pmc_literature/{research_id}/analysis_results/{lit_id}.json
è¾“å‡ºè¯­è¨€: {language}
====================ç°æœ‰æ–‡çŒ®ç»¼è¿°å†…å®¹====================
{latest_review}
====================æ–°æ–‡çŒ®å®Œæ•´åˆ†æç»“æœ====================
æ–‡çŒ®ID: {lit_id}

**å®Œæ•´åˆ†æç»“æœï¼ˆæ ¼å¼è¯´æ˜ï¼š[åºå·]ID:xxx|T:æ ‡é¢˜|A:æ‘˜è¦|M:æ–¹æ³•|R:ç»“æœ|C:ç»“è®º|Date:æ—¥æœŸ|Reason:ç†ç”±|Findings:å‘ç°ï¼‰:**
{self._format_metadata_for_agent([new_literature_metadata])}

============================================================

### ä»»åŠ¡ï¼šè¯·åŸºäºç°æœ‰æ–‡çŒ®ç»¼è¿°ï¼Œåˆ¤æ–­æ•´åˆæ–°æ–‡çŒ®çš„å†…å®¹æ˜¯å¦å¿…è¦ï¼Œè‹¥å¿…è¦åˆ™æ•´åˆæ–°æ–‡çŒ®çš„å†…å®¹ï¼ˆåŸºäºä¸Šé¢æä¾›çš„å®Œæ•´åˆ†æç»“æœï¼‰ï¼Œå¹¶åœ¨å®Œæˆåä»¥JSONæ ¼å¼è¿”å›ç»“æœ
æ³¨æ„ï¼š
- æ–°æ–‡çŒ®æä¾›çš„æ˜¯å…ƒæ•°æ®åŠä¹‹å‰çš„åˆ†æä¿¡æ¯ï¼Œè¯·ä½¿ç”¨å·¥å…·extract_literature_contentè·å–è¯¦ç»†å†…å®¹ï¼Œå¹¶åˆç†çš„å°†ç»“æœå†…å®¹æ•´åˆåˆ°ç°æœ‰ç»¼è¿°ä¸­ï¼Œè¯¦ç»†è¡¥å……å’Œæ›´æ–°ç›¸å…³å†…å®¹
- åŸå§‹ç‰ˆæœ¬ç»¼è¿°åŸºäºæ‰€æœ‰æ–‡çŒ®å…ƒæ•°æ®åˆ†æå¾—åˆ°,æ›´æ–°ç»¼è¿°æ—¶ä¼šæä¾›å…¶ä¸­ä¸€ç¯‡æ–‡çŒ®å…¨æ–‡,ç»Ÿè®¡æ•°é‡æ—¶ä¸åº”ä½œä¸ºæ–°æ–‡çŒ®æ•°é‡

ä½ çš„ä»»åŠ¡ï¼š
1. ç»“åˆæ–‡çŒ®å…ƒæ•°æ®åŠä¹‹å‰çš„åˆ†æç»“æœï¼Œä»¥åŠå½“å‰ç‰ˆæœ¬æ–‡çŒ®å†…å®¹ï¼Œåˆ†æåˆ¤æ–­æ˜¯å¦éœ€è¦å¼•ç”¨æˆ–å·²è¢«å¼•ç”¨
2. é’ˆå¯¹ç–‘é—®æˆ–ä¸æ˜ç¡®çš„å†…å®¹ï¼Œä½¿ç”¨å·¥å…·è·å–æ›´åŠ è¯¦ç»†çš„å†…å®¹ã€è¡¨æ ¼ã€ç»“è®ºç­‰
3. å¦‚éœ€å¼•ç”¨æˆ–è¯¥æ–‡çŒ®å·²è¢«å¼•ç”¨ï¼Œè‡³å°‘ä½¿ç”¨ä¸€æ¬¡å·¥å…·è·å–è¯¦ç»†å†…å®¹ï¼Œå¹¶åˆç†çš„å°†ç»“æœå†…å®¹æ•´åˆåˆ°ç°æœ‰ç»¼è¿°ä¸­ï¼Œè¯¦ç»†è¡¥å……å’Œæ›´æ–°ç›¸å…³å†…å®¹
4. ä½¿ç”¨`file_write`å·¥å…·å°†æ›´æ–°åçš„ç»¼è¿°å†…å®¹ä¿å­˜åˆ°æ–‡ä»¶
5. ä¿å­˜å®Œæˆæ–‡ä»¶åï¼Œè¯·æ€»ç»“è¾“å‡ºä¸€ä¸‹ä¸»è¦æ›´æ–°å†…å®¹
6. **å¿…é¡»**ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦è¿”å›å…¶ä»–å†…å®¹ï¼š
```json
{{
    "status": "success",
    "research_id": "{research_id}",
    "processed_literature_id": "{lit_id}",
    "version": "{next_version}",
    "file_path": "ä¿å­˜çš„æ–‡ä»¶è·¯å¾„",
    "message": "æˆåŠŸæ›´æ–°ç»¼è¿°"
}}
```

## é‡è¦è¯´æ˜
- **å½“ä½ è§‰å¾—æä¾›çš„æ–‡çŒ®å…¨æ–‡æ²¡æœ‰å¼•ç”¨ä»·å€¼æ—¶ï¼Œå¯ä»¥ç›´æ¥è¿”å›JSONç»“æœï¼Œç»“æœä¸­messageå€¼ä¸ºï¼šæ–‡çŒ®å…¨æ–‡æ²¡æœ‰å¼•ç”¨ä»·å€¼ï¼Œprocessed_literature_idä¸ºè¯¥æ–‡çŒ®IDï¼Œå…¶ä»–å€¼ä¿æŒä¸å˜**
- **å¿…é¡»å…ˆä¿å­˜æ–‡ä»¶**ï¼šä½¿ç”¨å·¥å…·file_writeä¿å­˜ç»¼è¿°å†…å®¹åˆ°æ–‡ä»¶åï¼Œå†è¿”å›JSONç»“æœ
- **JSONè¿”å›æ ¼å¼**ï¼šæ‰€æœ‰ä»»åŠ¡å®Œæˆåå¿…é¡»è¿”å›æŒ‡å®šæ ¼å¼çš„JSON
- **file_pathå­—æ®µ**ï¼šåœ¨JSONè¿”å›ä¸­åŒ…å«ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
- **ç‰ˆæœ¬æ ‡è¯†**ï¼šinitialç”¨äºåˆå§‹ç‰ˆæœ¬ï¼Œæ•°å­—1ã€2ã€3...ç”¨äºæ›´æ–°ç‰ˆæœ¬ï¼Œfinalç”¨äºæœ€ç»ˆç‰ˆæœ¬
============================================================
"""
                    # print(f"agent_input: {agent_input}")
                    logger.info(f"è°ƒç”¨Agentå¤„ç†æ–‡çŒ® {lit_id}")
                    agent_response = self._retry_agent_call(agent_input)
                    result = self._parse_agent_json_response(agent_response)
                    
                    logger.info(f"result: {result}")
                    if result and isinstance(result, dict):
                        logger.info(f"âœ… æˆåŠŸè§£æAgent JSONç»“æœ: status={result.get('status')}")
                        logger.info(f"   æ–‡ä»¶è·¯å¾„: {result.get('file_path')}")
                        logger.info(f"   ç‰ˆæœ¬: {result.get('version')}")
                    else:
                        logger.error(f"âŒ æ— æ³•è§£æAgent JSONç»“æœ")
                        logger.error(f"åŸå§‹å“åº”å‰1000å­—ç¬¦: {str(agent_response)[:1000]}")
                    
                    if result and isinstance(result, dict) and result.get("status") == "success":
                        logger.info(f"âœ… JSONè§£ææˆåŠŸï¼Œå¼€å§‹æ›´æ–°çŠ¶æ€")
                        file_path = result.get("file_path", "")
                        processed_id = result.get("processed_literature_id", lit_id)
                        
                        if file_path:
                            logger.info(f"ç‰ˆæœ¬ {next_version} ä¿å­˜æˆåŠŸ: {file_path}")
                            all_results.append(f"âœ… å¤„ç†æ–‡çŒ® {lit_id} æˆåŠŸ\nç‰ˆæœ¬: {next_version}\næ–‡ä»¶è·¯å¾„: {file_path}\n")
                            
                            self._update_processing_status(research_id, 
                                                          processed_id, next_version,
                                                          file_path)
                        else:
                            all_results.append(f"âš ï¸ å¤„ç†æ–‡çŒ® {lit_id} æˆåŠŸï¼Œä½†æœªè·å–åˆ°æ–‡ä»¶è·¯å¾„")
                            self._update_processing_status(research_id, 
                                                          processed_id, next_version,
                                                          None)
                    else:
                        all_results.append(f"âŒ å¤„ç†æ–‡çŒ® {lit_id} å¤±è´¥")
                        break
                
                elif pending_count == 0:
                    # æ‰€æœ‰æ–‡çŒ®å·²å¤„ç†å®Œæˆ
                    logger.info("æ‰€æœ‰æ–‡çŒ®å·²å¤„ç†å®Œæˆ")
                    
                    # Agentå·²ç»ä¿å­˜äº†æ¯ä¸ªç‰ˆæœ¬çš„æ–‡ä»¶ï¼Œåªéœ€è¦æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                    status["completed"] = True
                    
                    # è·å–æœ€æ–°çš„æ–‡ä»¶è·¯å¾„
                    reviews_dir = Path(".cache/pmc_literature") / research_id / "reviews"
                    if reviews_dir.exists():
                        review_files = sorted(reviews_dir.glob("review_*.md"), key=lambda x: x.stat().st_mtime, reverse=True)
                        if review_files:
                            latest_file = review_files[0]
                            file_path = str(latest_file)
                            status["version_file_path"] = file_path
                            all_results.append(f"âœ… æ‰€æœ‰æ–‡çŒ®å¤„ç†å®Œæˆæœ€ç»ˆç‰ˆæœ¬: {file_path}")
                    
                    status_file = Path(".cache/pmc_literature") / research_id / "step4.status"
                    
                    with open(status_file, 'w', encoding='utf-8') as f:
                        json.dump(status, f, ensure_ascii=False, indent=2)
                    
                    break
                
                else:
                    logger.warning("æœªçŸ¥çŠ¶æ€ï¼Œåœæ­¢å¤„ç†")
                    break
                sleep(60)
            
            
            return "\n" + "="*80 + "\n" + "\n".join(all_results)
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æ–‡çŒ®ç»¼è¿°ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"


    def get_review_status(self, research_id: str) -> str:
        """è·å–æ–‡çŒ®ç»¼è¿°å¤„ç†çŠ¶æ€"""
        try:
            status = self._get_processing_status(research_id)
            
            if not status:
                return "æ— æ³•è·å–å¤„ç†çŠ¶æ€"
            
            processed_count = len(status.get("processed_literature", []))
            total_count = status.get("total_literature", 0)
            pending_count = total_count - processed_count
            
            result = f"""
å¤„ç†çŠ¶æ€ä¿¡æ¯:
- ç ”ç©¶ID: {research_id}
- æ€»æ–‡çŒ®æ•°: {total_count}
- å·²å¤„ç†: {processed_count}
- å¾…å¤„ç†: {pending_count}
- å½“å‰ç‰ˆæœ¬: {status.get('current_version', 'N/A')}
- ç‰ˆæœ¬æ–‡ä»¶: {status.get('version_file_path', 'N/A')}
- æ˜¯å¦å®Œæˆ: {status.get('completed', False)}
"""
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            return f"è·å–å¤„ç†çŠ¶æ€è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“')
    parser.add_argument('-r', '--research_id', type=str, required=True,
                       help='ç ”ç©¶IDï¼Œå¯¹åº”.cache/pmc_literatureä¸‹çš„ç›®å½•å')
    parser.add_argument('-q', '--requirement', type=str, default=None,
                       help='ç”¨æˆ·é¢å¤–ç ”ç©¶éœ€æ±‚')
    parser.add_argument('-l', '--language', type=str, default='english',
                       help='è¾“å‡ºè¯­è¨€')
    parser.add_argument('-n', '--max_paper_to_init', type=int, default=50,
                       help='åˆå§‹ç‰ˆæœ¬ä½¿ç”¨çš„æœ€å¤§æ–‡çŒ®æ•°é‡ï¼Œé»˜è®¤50')
    parser.add_argument('-m', '--mode', type=str,
                       choices=['generate', 'status'],
                       default='generate',
                       help='æ“ä½œæ¨¡å¼')
    args = parser.parse_args()
    
    agent = PubmedLiteratureWritingAssistant()
    print(f"âœ… PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
    
    if args.mode == 'generate':
        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆæ–‡çŒ®ç»¼è¿°: ç ”ç©¶ID={args.research_id}")
        
        result = agent.generate_literature_review(
            research_id=args.research_id,
            requirement=args.requirement,
            language=args.language,
            max_paper_to_init=args.max_paper_to_init
        )
        
    elif args.mode == 'status':
        print(f"ğŸ“Š æŸ¥è¯¢å¤„ç†çŠ¶æ€: ç ”ç©¶ID={args.research_id}")
        result = agent.get_review_status(
            research_id=args.research_id
        )
    
    print(f"ğŸ“‹ å¤„ç†ç»“æœ:\n{str(result)}")
