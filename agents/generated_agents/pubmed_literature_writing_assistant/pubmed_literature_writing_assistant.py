#!/usr/bin/env python3
"""
PubMed Literature Writing Assistant

ä¸“é—¨è´Ÿè´£ç§‘ç ”æ–‡çŒ®ç¼–å†™å·¥ä½œï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„ææ–™å’Œæ€è·¯è¿›è¡Œæ–‡çŒ®ç»¼è¿°çš„ç¼–å†™å·¥ä½œã€‚
æ”¯æŒå¤„ç†å¤§é‡PubMedæ–‡çŒ®å¹¶ç”Ÿæˆé«˜è´¨é‡æ–‡çŒ®ç»¼è¿°ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ å’Œå¤šè¯­è¨€è¾“å‡ºåŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹ç‚¹:
- è¯»å–ç ”ç©¶IDå¯¹åº”çš„æ–‡çŒ®å…ƒæ•°æ®
- åŸºäºå…ƒæ•°æ®ç”Ÿæˆåˆå§‹æ–‡çŒ®ç»¼è¿°
- é€ç¯‡å¤„ç†æ–‡çŒ®å†…å®¹å¹¶æ›´æ–°ç»¼è¿°
- æ”¯æŒæ–­ç‚¹ç»­ä¼ åŠŸèƒ½
- å¤šè¯­è¨€è¾“å‡ºæ”¯æŒ
- ä¼šè¯æ•°æ®ç¼“å­˜
"""

import os
import json
import logging
import re
from time import sleep
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from nexus_utils.agent_factory import create_agent_from_prompt_template
from strands.telemetry import StrandsTelemetry
from tools.system_tools.qcli_integration import call_q_cli

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é…ç½®é¥æµ‹
os.environ["BYPASS_TOOL_CONSENT"] = "true"
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "http://localhost:4318"
strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter()


class PubmedLiteratureWritingAssistant:
    """PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“ç±»"""
    
    def __init__(self, env: str = "production", version: str = "latest", model_id: str = "default"):
        """
        åˆå§‹åŒ–PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“
        
        Args:
            env (str): ç¯å¢ƒé…ç½® (development, production, testing)
            version (str): æ™ºèƒ½ä½“ç‰ˆæœ¬
            model_id (str): ä½¿ç”¨çš„æ¨¡å‹ID
        """
        self.env = env
        self.version = version
        self.model_id = model_id
        
        # æ™ºèƒ½ä½“å‚æ•°
        self.agent_params = {
            "env": self.env,
            "version": self.version,
            "model_id": self.model_id
        }
        
        # æ™ºèƒ½ä½“é…ç½®è·¯å¾„
        self.agent_config_path = "generated_agents_prompts/pubmed_literature_writing_assistant/pubmed_literature_writing_assistant"
    
    def _create_agent(self):
        """åˆ›å»ºæ–°çš„æ™ºèƒ½ä½“å®ä¾‹"""
        return create_agent_from_prompt_template(
            agent_name=self.agent_config_path,
            **self.agent_params
        )
    
    def _get_processing_status(self, research_id: str) -> Dict:
        """è·å–å¤„ç†çŠ¶æ€"""
        try:
            cache_dir = Path(".cache/pmc_literature")
            research_dir = cache_dir / research_id
            status_file = research_dir / "step4.status"
            
            if not status_file.exists():
                total_literature = 0
                try:
                    manifest_path = research_dir / "manifest.json"
                    if manifest_path.exists():
                        with open(manifest_path, 'r', encoding='utf-8') as f:
                            manifest_data = json.load(f)
                        
                        if isinstance(manifest_data, dict):
                            if "marked_literature" in manifest_data:
                                marked_lit = manifest_data["marked_literature"]
                                all_literature = []
                                if isinstance(marked_lit, dict) and "by_year" in marked_lit:
                                    for year_data in marked_lit["by_year"].values():
                                        if isinstance(year_data, dict) and "literature" in year_data:
                                            all_literature.extend(year_data["literature"])
                                elif isinstance(marked_lit, dict) and "literature" in marked_lit:
                                    all_literature = marked_lit["literature"]
                                elif isinstance(marked_lit, list):
                                    all_literature = marked_lit
                                total_literature = len(all_literature)
                            elif "literature" in manifest_data:
                                total_literature = len(manifest_data["literature"]) if isinstance(manifest_data["literature"], list) else 1
                        elif isinstance(manifest_data, list):
                            total_literature = len(manifest_data)
                except Exception as e:
                    logger.warning(f"è¯»å–manifest.jsonå¤±è´¥: {str(e)}")
                
                initial_status = {
                    "research_id": research_id,
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat(),
                    "processed_literature": [],
                    "current_version": None,
                    "version_file_path": None,
                    "total_literature": total_literature,
                    "completed": False
                }
                
                with open(status_file, 'w', encoding='utf-8') as f:
                    json.dump(initial_status, f, ensure_ascii=False, indent=2)
                
                return initial_status
            
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
            
            return status
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            return None
    
    def _load_literature_metadata(self, research_id: str) -> List[Dict]:
        """åŠ è½½æ–‡çŒ®å…ƒæ•°æ®"""
        try:
            manifest_path = Path(".cache/pmc_literature") / research_id / "manifest.json"
            
            if not manifest_path.exists():
                logger.error(f"manifest.jsonä¸å­˜åœ¨: {manifest_path}")
                return []
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_data = json.load(f)
            
            if isinstance(manifest_data, dict):
                if "marked_literature" in manifest_data:
                    marked_lit = manifest_data["marked_literature"]
                    all_literature = []
                    if isinstance(marked_lit, dict) and "by_year" in marked_lit:
                        for year_data in marked_lit["by_year"].values():
                            if isinstance(year_data, dict) and "literature" in year_data:
                                all_literature.extend(year_data["literature"])
                    elif isinstance(marked_lit, dict) and "literature" in marked_lit:
                        all_literature = marked_lit["literature"]
                    elif isinstance(marked_lit, list):
                        all_literature = marked_lit
                    return all_literature
                elif "literature" in manifest_data:
                    return manifest_data["literature"]
            elif isinstance(manifest_data, list):
                return manifest_data
            
            return []
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡çŒ®å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _get_pending_literature(self, research_id: str) -> Optional[Dict]:
        """è·å–å¾…å¤„ç†çš„ä¸€ç¯‡æ–‡çŒ®"""
        try:
            all_metadata = self._load_literature_metadata(research_id)
            
            if not all_metadata:
                return None
            
            status = self._get_processing_status(research_id)
            if not status:
                return None
            
            processed_ids = status.get("processed_literature", [])
            research_dir = Path(".cache/pmc_literature") / research_id
            paper_dir = research_dir / "paper"
            
            for meta in all_metadata:
                lit_id = meta.get("pmcid") or meta.get("id") or meta.get("pmid")
                
                if lit_id and lit_id not in processed_ids:
                    paper_path = paper_dir / f"{lit_id}.txt"
                    fulltext = ""
                    
                    if paper_path.exists():
                        try:
                            with open(paper_path, 'r', encoding='utf-8') as f:
                                fulltext = f.read()
                        except Exception:
                            pass
                    
                    return {
                        "pmcid": lit_id,
                        "metadata": meta,
                        "fulltext": fulltext,
                        "has_fulltext": len(fulltext) > 0
                    }
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–å¾…å¤„ç†æ–‡çŒ®å¤±è´¥: {str(e)}")
            return None
    
    def _get_latest_review(self, research_id: str) -> Optional[str]:
        """è·å–æœ€æ–°ç”Ÿæˆçš„ç»¼è¿°å†…å®¹"""
        try:
            # æ–¹æ³•1ï¼šä¼˜å…ˆä»statusæ–‡ä»¶è¯»å–å½“å‰ç‰ˆæœ¬è·¯å¾„
            status = self._get_processing_status(research_id)
            if status and status.get("version_file_path"):
                version_path = status["version_file_path"]
                # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
                if not os.path.isabs(version_path):
                    file_path = Path(version_path)
                else:
                    file_path = Path(version_path)
                
                if file_path.exists():
                    logger.info(f"ä»statusè¯»å–ç‰ˆæœ¬æ–‡ä»¶: {version_path}")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    return content
            
            # æ–¹æ³•2ï¼šè§£ææ–‡ä»¶åä¸­çš„ç‰ˆæœ¬å·ï¼ŒæŒ‰ç‰ˆæœ¬æ’åº
            reviews_dir = Path(".cache/pmc_literature") / research_id / "reviews"
            
            if not reviews_dir.exists():
                return None
            
            # è§£ææ‰€æœ‰æ–‡ä»¶ï¼Œæå–ç‰ˆæœ¬å·
            def extract_version(filename: str) -> tuple:
                """æå–ç‰ˆæœ¬å·å’Œä¼˜å…ˆçº§
                è¿”å›: (priority, version_number, timestamp_str)
                initial=0, æ•°å­—ç‰ˆæœ¬ç›´æ¥æ¯”è¾ƒ"""
                name = filename.name
                if name.startswith("review_initial_"):
                    timestamp = name.replace("review_initial_", "").replace(".md", "")
                    return (0, 0, timestamp)
                elif name.startswith("review_v"):
                    # review_v{version}_{timestamp}.md
                    parts = name.replace("review_v", "").replace(".md", "").split("_")
                    if parts:
                        try:
                            version_num = int(parts[0])
                            timestamp = "_".join(parts[1:]) if len(parts) > 1 else ""
                            return (1, version_num, timestamp)
                        except:
                            return (2, 0, name)  # æ— æ³•è§£æï¼Œæ”¾åˆ°æœ€å
                elif name.startswith("review_final_"):
                    timestamp = name.replace("review_final_", "").replace(".md", "")
                    return (3, 999999, timestamp)  # final æ”¾åˆ°æœ€åä½†ä¼˜å…ˆçº§æœ€é«˜
                return (4, -1, name)  # æœªçŸ¥æ ¼å¼
            
            review_files = list(reviews_dir.glob("review_*.md"))
            if not review_files:
                return None
            
            # æŒ‰ç‰ˆæœ¬å·æ’åº
            sorted_files = sorted(review_files, key=extract_version)
            
            latest_file = sorted_files[-1]  # è·å–ç‰ˆæœ¬å·æœ€å¤§çš„
            logger.info(f"è·å–æœ€æ–°ç‰ˆæœ¬æ–‡ä»¶: {latest_file.name}")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return content
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°ç»¼è¿°å¤±è´¥: {str(e)}")
            return None
    
    def _save_review_version(self, research_id: str, content: str, version: Union[str, int]) -> Dict:
        """ä¿å­˜æ–‡çŒ®ç»¼è¿°ç‰ˆæœ¬"""
        try:
            reviews_dir = Path(".cache/pmc_literature") / research_id / "reviews"
            os.makedirs(reviews_dir, exist_ok=True)
            
            version_str = str(version).lower()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if version_str in ["initial", "final"]:
                filename = f"review_{version_str}_{timestamp}.md"
            else:
                filename = f"review_v{version_str}_{timestamp}.md"
            
            file_path = reviews_dir / filename
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return {
                "status": "success",
                "research_id": research_id,
                "version": version_str,
                "file_path": str(file_path),
                "timestamp": timestamp
            }
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»¼è¿°ç‰ˆæœ¬å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "message": f"ä¿å­˜å¤±è´¥: {str(e)}"
            }
    
    def _update_processing_status(self, research_id: str, 
                                 processed_literature_id: str, version: Union[str, int],
                                 version_file_path: str = None) -> Dict:
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        try:
            cache_dir = Path(".cache/pmc_literature")
            research_dir = cache_dir / research_id
            status_file = research_dir / "step4.status"
            
            if status_file.exists():
                with open(status_file, 'r', encoding='utf-8') as f:
                    status = json.load(f)
            else:
                status = self._get_processing_status(research_id)
            
            if processed_literature_id and processed_literature_id.lower() not in ["initial_marker"]:
                if processed_literature_id not in status["processed_literature"]:
                    status["processed_literature"].append(processed_literature_id)
            
            status["current_version"] = str(version)
            status["updated_at"] = datetime.now().isoformat()
            
            if version_file_path:
                status["version_file_path"] = version_file_path
            
            total = status.get("total_literature", 0)
            processed = len(status["processed_literature"])
            
            if total > 0 and processed >= total:
                status["completed"] = True
            
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
            
            return {
                "status": "success",
                "processed": processed,
                "total": total,
                "completed": status["completed"]
            }
            
        except Exception as e:
            logger.error(f"æ›´æ–°å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "status": "error",
                "message": str(e)
            }
    
    def _mark_all_metadata_processed(self, research_id: str) -> Dict:
        """æ ‡è®°æ‰€æœ‰æ–‡çŒ®å…ƒæ•°æ®å·²å¤„ç†"""
        return self._update_processing_status(research_id, "initial_marker", "initial")
    
    def _retry_agent_call(self, agent_input: str, max_retries: int = 3, retry_delay: int = 60):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ Agent è°ƒç”¨
        
        Args:
            agent_input: è¾“å…¥ç»™ Agent çš„å†…å®¹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’
            
        Returns:
            AgentResult å¯¹è±¡
        """
        for attempt in range(1, max_retries + 1):
            agent_response = None
            try:
                logger.info(f"è°ƒç”¨ Agentï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰")
                # æ¯æ¬¡è°ƒç”¨å‰åˆ›å»ºæ–°çš„agentå®ä¾‹
                agent = self._create_agent()
                agent_response = agent(agent_input)
                
                # éªŒè¯å“åº”
                if hasattr(agent_response, 'message') and agent_response.message:
                    logger.info(f"âœ… Agent è°ƒç”¨æˆåŠŸï¼ˆå°è¯• {attempt}ï¼‰")
                    return agent_response
                elif hasattr(agent_response, 'content') and agent_response.content:
                    logger.info(f"âœ… Agent è°ƒç”¨æˆåŠŸï¼ˆå°è¯• {attempt}ï¼‰")
                    return agent_response
                else:
                    raise ValueError("Agent å“åº”æ— æ•ˆ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Agent è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {str(e)}")
                
                if attempt < max_retries:
                    logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    # åªæœ‰åœ¨agent_responseå­˜åœ¨æ—¶æ‰æ‰“å°metrics
                    if agent_response and hasattr(agent_response, 'metrics'):
                        print("="*100)
                        print(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                        print(f"Execution time: {sum(agent_response.metrics.cycle_durations):.2f} seconds")
                        print(f"Tools used: {list(agent_response.metrics.tool_metrics.keys())}")
                    sleep(retry_delay)
                else:
                    logger.error(f"âŒ Agent è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                    raise
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"Agent è°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
    
    def _extract_agent_text(self, agent_response: Any) -> str:
        """ä» Agent å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å­—å…¸æ ¼å¼ï¼ˆStrandsæ¡†æ¶çš„æ ‡å‡†æ¶ˆæ¯æ ¼å¼ï¼‰
        if isinstance(agent_response, dict):
            if 'content' in agent_response:
                # æ ¼å¼ï¼š{'role': 'assistant', 'content': [{'text': '...'}]}
                content_list = agent_response['content']
                if isinstance(content_list, list):
                    texts = []
                    for item in content_list:
                        if isinstance(item, dict) and 'text' in item:
                            texts.append(item['text'])
                        elif isinstance(item, str):
                            texts.append(item)
                    return '\n'.join(texts)
                elif isinstance(content_list, str):
                    return content_list
            elif 'message' in agent_response:
                return str(agent_response['message'])
            else:
                return str(agent_response)
        elif hasattr(agent_response, 'message'):
            message = agent_response.message
            # å¦‚æœmessageæ˜¯å­—å…¸æ ¼å¼ï¼ˆStrandsæ¡†æ¶çš„æ ‡å‡†æ¶ˆæ¯æ ¼å¼ï¼‰
            if isinstance(message, dict):
                if 'content' in message:
                    content_list = message['content']
                    if isinstance(content_list, list):
                        texts = []
                        for item in content_list:
                            if isinstance(item, dict) and 'text' in item:
                                texts.append(item['text'])
                            elif isinstance(item, str):
                                texts.append(item)
                        return '\n'.join(texts)
                    elif isinstance(content_list, str):
                        return content_list
            return str(message)
        elif hasattr(agent_response, 'content'):
            content = agent_response.content
            if isinstance(content, str):
                return content
            elif isinstance(content, list):
                texts = []
                for item in content:
                    if isinstance(item, dict) and 'text' in item:
                        texts.append(item['text'])
                return '\n'.join(texts)
            elif isinstance(content, dict):
                # content æœ¬èº«æ˜¯ä¸€ä¸ªå­—å…¸ {'role': 'assistant', 'content': [...]}
                content_list = content.get('content', [])
                if isinstance(content_list, list):
                    texts = []
                    for item in content_list:
                        if isinstance(item, dict) and 'text' in item:
                            texts.append(item['text'])
                        elif isinstance(item, str):
                            texts.append(item)
                    return '\n'.join(texts)
                elif isinstance(content_list, str):
                    return content_list
            else:
                return str(content)
        else:
            # å°è¯•ä»å­—ç¬¦ä¸²è¡¨ç¤ºä¸­æå–å†…å®¹
            str_repr = str(agent_response)
            # å¦‚æœçœ‹èµ·æ¥æ˜¯å­—å…¸çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œå°è¯•æå–å…¶ä¸­çš„æ–‡æœ¬
            if str_repr.startswith("{'") and 'content' in str_repr:
                # å°è¯•è§£æè¿™ä¸ªå­—å…¸çš„å­—ç¬¦ä¸²è¡¨ç¤º
                try:
                    import ast
                    parsed = ast.literal_eval(str_repr)
                    if isinstance(parsed, dict) and 'content' in parsed:
                        content_list = parsed['content']
                        if isinstance(content_list, list):
                            texts = []
                            for item in content_list:
                                if isinstance(item, dict) and 'text' in item:
                                    texts.append(item['text'])
                            if texts:
                                return '\n'.join(texts)
                except:
                    pass
            return str_repr
    
    def _parse_agent_json_result(self, agent_response: str) -> Optional[Dict]:
        """è§£æAgentè¿”å›çš„JSONç»“æœ"""
        try:
            # æ‰“å°Agentå“åº”ç”¨äºè°ƒè¯•
            logger.info(f"Agentå“åº”å‰500å­—ç¬¦: {agent_response[:500]}")
            
            # æ–¹æ³•1: æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ˆæ›´å¥å£®çš„æ–¹æ³•ï¼‰
            json_block_match = re.search(r'```json\s*([\s\S]*?)\s*```', agent_response)
            if json_block_match:
                json_str = json_block_match.group(1).strip()
                logger.info("ä»```jsonä»£ç å—ä¸­æå–JSON")
                try:
                    result = json.loads(json_str)
                    logger.info(f"æˆåŠŸè§£æJSON: status={result.get('status')}")
                    return result
                except Exception as e:
                    logger.error(f"è§£æä»£ç å—ä¸­çš„JSONå¤±è´¥: {str(e)}")
            
            # æ–¹æ³•2: ä»åå¾€å‰æŸ¥æ‰¾ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            # Agenté€šå¸¸ä¼šåœ¨æœ€åè¿”å›JSONç»“æœï¼Œè¿™æ ·å¯ä»¥é¿å…åŒ¹é…åˆ°ä¸­é—´æ€è€ƒè¿‡ç¨‹çš„JSONç‰‡æ®µ
            logger.info("å°è¯•ä»åå¾€å‰æŸ¥æ‰¾JSONå¯¹è±¡")
            json_end = -1
            json_start = -1
            brace_count = 0
            
            # ä»åå¾€å‰æ‰¾æœ€åä¸€ä¸ª}
            for i in range(len(agent_response) - 1, -1, -1):
                if agent_response[i] == '}':
                    json_end = i + 1
                    brace_count = 1
                    # ç°åœ¨ä»i-1å¼€å§‹å¾€å‰æ‰¾åŒ¹é…çš„{
                    for j in range(i - 1, -1, -1):
                        char = agent_response[j]
                        if char == '}':
                            brace_count += 1
                        elif char == '{':
                            brace_count -= 1
                            if brace_count == 0:
                                json_start = j
                                # æ‰¾åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                                json_str = agent_response[json_start:json_end]
                                json_str = json_str.strip()
                                logger.info(f"ä»åå¾€å‰æ‰¾åˆ°JSONå¯¹è±¡ï¼ˆå‰200å­—ç¬¦ï¼‰: {json_str[:200]}...")
                                try:
                                    result = json.loads(json_str)
                                    logger.info(f"æˆåŠŸè§£æJSON: status={result.get('status')}")
                                    # éªŒè¯æ˜¯å¦åŒ…å«é¢„æœŸçš„å­—æ®µ
                                    if 'status' in result:
                                        return result
                                except Exception as e:
                                    logger.warning(f"è§£æJSONå¯¹è±¡å¤±è´¥: {str(e)}")
                                break
                    # å¦‚æœæ‰¾åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„JSONï¼Œé€€å‡ºå¤–å±‚å¾ªç¯
                    if json_start >= 0:
                        break
            
            # æ–¹æ³•3: å‘å‰å…¼å®¹ï¼Œä»å‰å¾€åæŸ¥æ‰¾ç¬¬ä¸€ä¸ªJSONå¯¹è±¡
            json_start = agent_response.find('{')
            if json_start >= 0:
                brace_count = 0
                json_end = -1
                for i in range(json_start, len(agent_response)):
                    char = agent_response[i]
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            json_end = i + 1
                            break
                
                if json_end > json_start:
                    json_str = agent_response[json_start:json_end]
                    logger.info(f"ä»å“åº”ä¸­æ‰¾åˆ°JSONå¯¹è±¡ï¼ˆå‰200å­—ç¬¦ï¼‰: {json_str[:200]}...")
                    try:
                        result = json.loads(json_str)
                        logger.info(f"æˆåŠŸè§£æJSON: status={result.get('status')}")
                        return result
                    except Exception as e:
                        logger.error(f"è§£æJSONå¯¹è±¡å¤±è´¥: {str(e)}")
                        logger.error(f"å®Œæ•´JSONå†…å®¹: {json_str}")
            
            # æ–¹æ³•3: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            try:
                result = json.loads(agent_response)
                logger.info("ç›´æ¥è§£æå“åº”æˆåŠŸ")
                return result
            except Exception as e:
                logger.warning(f"ç›´æ¥è§£æå¤±è´¥: {str(e)}")
            
            logger.error("æ— æ³•ä»Agentå“åº”ä¸­è§£æJSON")
            logger.error(f"å®Œæ•´å“åº”å†…å®¹: {agent_response}")
            return None
            
        except Exception as e:
            logger.error(f"è§£æAgent JSONç»“æœæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logger.error(f"å“åº”å†…å®¹: {agent_response}")
            return None
    
    def generate_literature_review(self, research_id: str, requirement: str = None, 
                                  language: str = "english") -> str:
        """ç”Ÿæˆæ–‡çŒ®ç»¼è¿°ï¼ˆä¸»æ§åˆ¶å¾ªç¯ï¼‰"""
        try:
            all_results = []
            
            while True:
                status = self._get_processing_status(research_id)
                if not status:
                    return "è·å–å¤„ç†çŠ¶æ€å¤±è´¥"
                
                processed_count = len(status.get("processed_literature", []))
                total_count = status.get("total_literature", 0)
                pending_count = total_count - processed_count
                
                logger.info(f"å½“å‰è¿›åº¦: {processed_count}/{total_count}, å¾…å¤„ç†: {pending_count}")
                
                if processed_count == 0 and not status.get("current_version"):
                    # æƒ…å†µAï¼šç”Ÿæˆåˆå§‹ç‰ˆæœ¬
                    logger.info("æ‰€æœ‰æ–‡çŒ®æœªå¤„ç†ï¼Œç”Ÿæˆåˆå§‹ç‰ˆæœ¬")
                    
                    metadata = self._load_literature_metadata(research_id)
                    if not metadata:
                        return "æ— æ³•åŠ è½½æ–‡çŒ®å…ƒæ•°æ®"
                    
                    agent_input = f"""
è¯·æ ¹æ®ä»¥ä¸‹æ–‡çŒ®å…ƒæ•°æ®ç”Ÿæˆåˆå§‹ç‰ˆæœ¬çš„æ–‡çŒ®ç»¼è¿°ï¼š

ç ”ç©¶ID: {research_id}
è¾“å‡ºè¯­è¨€: {language}
æ–‡çŒ®æ•°é‡: {len(metadata)}

ç”¨æˆ·ç ”ç©¶éœ€æ±‚:
{requirement if requirement else "æ— ç‰¹æ®Šéœ€æ±‚"}

æ–‡çŒ®å…ƒæ•°æ®:
{json.dumps(metadata, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆåˆå§‹ç‰ˆæœ¬çš„æ–‡çŒ®ç»¼è¿°ï¼Œå¹¶åœ¨å®Œæˆåä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "status": "success",
    "research_id": "{research_id}",
    "version": "initial",
    "file_path": "ä¿å­˜çš„æ–‡ä»¶è·¯å¾„",
    "message": "æˆåŠŸç”Ÿæˆåˆå§‹ç‰ˆæœ¬"
}}
"""
                    
                    logger.info("è°ƒç”¨Agentç”Ÿæˆåˆå§‹ç‰ˆæœ¬")
                    agent_response = self._retry_agent_call(agent_input)

                    print("="*100)
                    # Access metrics through the AgentResult
                    print(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                    print(f"Execution time: {sum(agent_response.metrics.cycle_durations):.2f} seconds")
                    print(f"Tools used: {list(agent_response.metrics.tool_metrics.keys())}")
                    
                    agent_text = self._extract_agent_text(agent_response)
                    result = self._parse_agent_json_result(agent_text)
                    print("="*100)
                    print(f"è§£æç»“æœ: {result}")
                    
                    
                    if result and result.get("status") == "success":
                        file_path = result.get("file_path", "")
                        
                        if file_path:
                            logger.info(f"åˆå§‹ç‰ˆæœ¬ä¿å­˜æˆåŠŸ: {file_path}")
                            all_results.append(f"âœ… åˆå§‹ç‰ˆæœ¬ç”ŸæˆæˆåŠŸ\næ–‡ä»¶è·¯å¾„: {file_path}\n")
                            
                            self._mark_all_metadata_processed(research_id)
                            self._update_processing_status(research_id, 
                                                          "initial_marker", "initial", 
                                                          file_path)
                        else:
                            all_results.append(f"âš ï¸ åˆå§‹ç‰ˆæœ¬ç”ŸæˆæˆåŠŸï¼Œä½†æœªè·å–åˆ°æ–‡ä»¶è·¯å¾„")
                            self._mark_all_metadata_processed(research_id)
                            self._update_processing_status(research_id, 
                                                          "initial_marker", "initial", 
                                                          None)
                    else:
                        all_results.append(f"âŒ Agentç”Ÿæˆåˆå§‹ç‰ˆæœ¬å¤±è´¥")
                        break
                
                elif pending_count > 0:
                    # æƒ…å†µBï¼šç»§ç»­å¤„ç†æœªå¤„ç†æ–‡çŒ®
                    logger.info(f"ç»§ç»­å¤„ç†ï¼Œè¿˜æœ‰ {pending_count} ç¯‡æ–‡çŒ®å¾…å¤„ç†")
                    
                    pending_literature = self._get_pending_literature(research_id)
                    
                    if not pending_literature:
                        logger.info("æ²¡æœ‰å¾…å¤„ç†çš„æ–‡çŒ®")
                        break
                    
                    lit_id = pending_literature["pmcid"]
                    logger.info(f"å¤„ç†æ–‡çŒ®: {lit_id}")
                    
                    latest_review = self._get_latest_review(research_id)
                    
                    if not latest_review:
                        all_results.append("âŒ æ— æ³•è·å–æœ€æ–°ç»¼è¿°")
                        break
                    
                    current_version = status.get("current_version", "initial")
                    if current_version == "initial":
                        next_version = 1
                    else:
                        try:
                            next_version = int(current_version) + 1
                        except:
                            next_version = 1                    
                    lit_metadata = pending_literature.get('metadata', {})
                    processed_fulltext = self._remove_reference_from_literature(pending_literature['fulltext'])
                    
                    agent_input = f"""
====================é¡¹ç›®åŸºç¡€ä¿¡æ¯====================
ç ”ç©¶ID: {research_id}
ç°æœ‰ç‰ˆæœ¬æ–‡çŒ®åœ°å€: {status.get('version_file_path', 'N/A')}
æ–°æ–‡çŒ®åœ°å€: .cache/pmc_literature/{research_id}/paper/{lit_id}.txt
è¾“å‡ºè¯­è¨€: {language}
====================ç°æœ‰æ–‡çŒ®ç»¼è¿°å†…å®¹====================
**ç°æœ‰æ–‡çŒ®ç»¼è¿°å†…å®¹:**
{latest_review}
====================æ–°æ–‡çŒ®å…ƒæ•°æ®åŠå…¨æ–‡å†…å®¹====================
**æ–°æ–‡çŒ®å…ƒæ•°æ®:**
- PMCID: {lit_id}
- metadata: {json.dumps(lit_metadata, ensure_ascii=False, indent=2)}
====================è¾“å‡ºè¦æ±‚====================
{{
    "status": "success",
    "research_id": "{research_id}",
    "processed_literature_id": "{lit_id}",
    "version": "{next_version}",
    "file_path": "ä¿å­˜çš„æ–‡ä»¶è·¯å¾„",
    "message": "æˆåŠŸæ›´æ–°ç»¼è¿°"
}}
============================================================
è¯·åŸºäºç°æœ‰æ–‡çŒ®ç»¼è¿°ï¼Œåˆ¤æ–­æ•´åˆæ–°æ–‡çŒ®çš„å†…å®¹æ˜¯å¦å¿…è¦ï¼Œè‹¥å¿…è¦åˆ™æ•´åˆæ–°æ–‡çŒ®çš„å†…å®¹ï¼Œå¹¶åœ¨å®Œæˆåä»¥JSONæ ¼å¼è¿”å›ç»“æœ
å¦‚éœ€è¦æ›´è¯¦ç»†å†…å®¹ï¼Œè¯·ä½¿ç”¨å·¥å…·extract_literature_contentè·å–
"""
                    print(f"agent_input: {agent_input}")
                    logger.info(f"è°ƒç”¨Agentå¤„ç†æ–‡çŒ® {lit_id}")
                    agent_response = self._retry_agent_call(agent_input)
                    
                    # ä» AgentResult å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹
                    agent_text = self._extract_agent_text(agent_response)
                    result = self._parse_agent_json_result(agent_text)
                    print("="*100)
                    print(f"agent_text: {agent_text[:500]}")

                    print("="*100)
                    print(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                    print(f"Execution time: {sum(agent_response.metrics.cycle_durations):.2f} seconds")
                    print(f"Tools used: {list(agent_response.metrics.tool_metrics.keys())}")
                    print(result)
                    print("="*100)
                    if result:
                        logger.info(f"âœ… æˆåŠŸè§£æAgent JSONç»“æœ: status={result.get('status')}")
                        logger.info(f"   æ–‡ä»¶è·¯å¾„: {result.get('file_path')}")
                        logger.info(f"   ç‰ˆæœ¬: {result.get('version')}")
                    else:
                        logger.error(f"âŒ æ— æ³•è§£æAgent JSONç»“æœ")
                        logger.error(f"åŸå§‹å“åº”å‰1000å­—ç¬¦: {str(agent_response)[:1000]}")
                    
                    if result and result.get("status") == "success":
                        logger.info(f"âœ… JSONè§£ææˆåŠŸï¼Œå¼€å§‹æ›´æ–°çŠ¶æ€")
                        file_path = result.get("file_path", "")
                        processed_id = result.get("processed_literature_id", lit_id)
                        
                        if file_path:
                            logger.info(f"ç‰ˆæœ¬ {next_version} ä¿å­˜æˆåŠŸ: {file_path}")
                            all_results.append(f"âœ… å¤„ç†æ–‡çŒ® {lit_id} æˆåŠŸ\nç‰ˆæœ¬: {next_version}\næ–‡ä»¶è·¯å¾„: {file_path}\n")
                            
                            self._update_processing_status(research_id, 
                                                          processed_id, next_version,
                                                          file_path)
                        else:
                            all_results.append(f"âš ï¸ å¤„ç†æ–‡çŒ® {lit_id} æˆåŠŸï¼Œä½†æœªè·å–åˆ°æ–‡ä»¶è·¯å¾„")
                            self._update_processing_status(research_id, 
                                                          processed_id, next_version,
                                                          None)
                    else:
                        all_results.append(f"âŒ å¤„ç†æ–‡çŒ® {lit_id} å¤±è´¥")
                        print("="*100)
                        print(f"Total tokens: {agent_response.metrics.accumulated_usage}")
                        print(f"Execution time: {sum(agent_response.metrics.cycle_durations):.2f} seconds")
                        print(f"Tools used: {list(agent_response.metrics.tool_metrics.keys())}")
                        print("="*100)
                        break
                
                elif pending_count == 0:
                    # æ‰€æœ‰æ–‡çŒ®å·²å¤„ç†å®Œæˆ
                    logger.info("æ‰€æœ‰æ–‡çŒ®å·²å¤„ç†å®Œæˆ")
                    
                    # Agentå·²ç»ä¿å­˜äº†æ¯ä¸ªç‰ˆæœ¬çš„æ–‡ä»¶ï¼Œåªéœ€è¦æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                    status["completed"] = True
                    
                    # è·å–æœ€æ–°çš„æ–‡ä»¶è·¯å¾„
                    reviews_dir = Path(".cache/pmc_literature") / research_id / "reviews"
                    if reviews_dir.exists():
                        review_files = sorted(reviews_dir.glob("review_*.md"), key=lambda x: x.stat().st_mtime, reverse=True)
                        if review_files:
                            latest_file = review_files[0]
                            file_path = str(latest_file)
                            status["version_file_path"] = file_path
                            all_results.append(f"âœ… æ‰€æœ‰æ–‡çŒ®å¤„ç†å®Œæˆæœ€ç»ˆç‰ˆæœ¬: {file_path}")
                    
                    status_file = Path(".cache/pmc_literature") / research_id / "step4.status"
                    
                    with open(status_file, 'w', encoding='utf-8') as f:
                        json.dump(status, f, ensure_ascii=False, indent=2)
                    
                    break
                
                else:
                    logger.warning("æœªçŸ¥çŠ¶æ€ï¼Œåœæ­¢å¤„ç†")
                    break
                sleep(60)
            
            
            return "\n" + "="*80 + "\n" + "\n".join(all_results)
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æ–‡çŒ®ç»¼è¿°ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

    def _remove_reference_from_literature(self, literature: str) -> str:
        """ç§»é™¤æ–‡çŒ®ä¸­çš„å‚è€ƒæ–‡çŒ®"""
        # æŒ‰==== Refsåˆ†å‰²ï¼Œåˆ é™¤==== Refsä¹‹åçš„æ‰€æœ‰å†…å®¹
        refs = literature.split('==== Refs')

        # ä¿ç•™==== Bodyåˆ°==== Refsä¹‹é—´çš„å†…å®¹
        body = refs[0].split('==== Body')[1].strip()
        return body

    def get_review_status(self, research_id: str) -> str:
        """è·å–æ–‡çŒ®ç»¼è¿°å¤„ç†çŠ¶æ€"""
        try:
            status = self._get_processing_status(research_id)
            
            if not status:
                return "æ— æ³•è·å–å¤„ç†çŠ¶æ€"
            
            processed_count = len(status.get("processed_literature", []))
            total_count = status.get("total_literature", 0)
            pending_count = total_count - processed_count
            
            result = f"""
å¤„ç†çŠ¶æ€ä¿¡æ¯:
- ç ”ç©¶ID: {research_id}
- æ€»æ–‡çŒ®æ•°: {total_count}
- å·²å¤„ç†: {processed_count}
- å¾…å¤„ç†: {pending_count}
- å½“å‰ç‰ˆæœ¬: {status.get('current_version', 'N/A')}
- ç‰ˆæœ¬æ–‡ä»¶: {status.get('version_file_path', 'N/A')}
- æ˜¯å¦å®Œæˆ: {status.get('completed', False)}
"""
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            return f"è·å–å¤„ç†çŠ¶æ€è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“')
    parser.add_argument('-r', '--research_id', type=str, required=True,
                       help='ç ”ç©¶IDï¼Œå¯¹åº”.cache/pmc_literatureä¸‹çš„ç›®å½•å')
    parser.add_argument('-q', '--requirement', type=str, default=None,
                       help='ç”¨æˆ·é¢å¤–ç ”ç©¶éœ€æ±‚')
    parser.add_argument('-l', '--language', type=str, default='english',
                       help='è¾“å‡ºè¯­è¨€')
    parser.add_argument('-m', '--mode', type=str,
                       choices=['generate', 'status'],
                       default='generate',
                       help='æ“ä½œæ¨¡å¼')
    args = parser.parse_args()
    
    agent = PubmedLiteratureWritingAssistant()
    print(f"âœ… PubMedæ–‡çŒ®ç¼–å†™æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
    
    if args.mode == 'generate':
        print(f"ğŸ“ å¼€å§‹ç”Ÿæˆæ–‡çŒ®ç»¼è¿°: ç ”ç©¶ID={args.research_id}")
        
        result = agent.generate_literature_review(
            research_id=args.research_id,
            requirement=args.requirement,
            language=args.language
        )
        
    elif args.mode == 'status':
        print(f"ğŸ“Š æŸ¥è¯¢å¤„ç†çŠ¶æ€: ç ”ç©¶ID={args.research_id}")
        result = agent.get_review_status(
            research_id=args.research_id
        )
    
    print(f"ğŸ“‹ å¤„ç†ç»“æœ:\n{result}")
