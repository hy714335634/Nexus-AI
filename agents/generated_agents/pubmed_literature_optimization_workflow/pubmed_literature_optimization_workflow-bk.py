#!/usr/bin/env python3
"""
PubMed Literature Optimization Workflow

ä½¿ç”¨Swarmç¼–æ’editorã€writingã€reviewä¸‰ä¸ªagentï¼Œå®ç°æ–‡çŒ®ä¼˜åŒ–å·¥ä½œæµã€‚
å·¥ä½œæµï¼šreviewç»™å‡ºæ„è§ â†’ writingä¿®æ­£ â†’ reviewé€šè¿‡å â†’ editorç»™å‡ºæ„è§ â†’ writingä¿®æ­£ â†’ ç›´åˆ°æœ€ç»ˆé€šè¿‡
"""

import os
import json
import logging
import uuid
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

from nexus_utils.agent_factory import create_agent_from_prompt_template
from strands.multiagent import Swarm
from strands.telemetry import StrandsTelemetry

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é…ç½®é¥æµ‹
os.environ["BYPASS_TOOL_CONSENT"] = "true"
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "http://localhost:4318"
strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter()


class PubmedLiteratureOptimizationWorkflow:
    """PubMedæ–‡çŒ®ä¼˜åŒ–å·¥ä½œæµç±»"""
    
    def __init__(self, research_id: str, env: str = "production", version: str = "latest", model_id: str = "default"):
        """
        åˆå§‹åŒ–PubMedæ–‡çŒ®ä¼˜åŒ–å·¥ä½œæµ
        
        Args:
            research_id (str): ç ”ç©¶ID
            session_manager: ä¼šè¯ç®¡ç†å™¨å®ä¾‹
            env (str): ç¯å¢ƒé…ç½® (development, production, testing)
            version (str): æ™ºèƒ½ä½“ç‰ˆæœ¬
            model_id (str): ä½¿ç”¨çš„æ¨¡å‹ID
        """
        self.research_id = research_id
        self.env = env
        self.version = version
        self.model_id = model_id
        
        # æ™ºèƒ½ä½“å‚æ•°
        self.agent_params = {
            "env": self.env,
            "version": self.version,
            "model_id": self.model_id
        }
        
        # åˆ›å»ºä¸‰ä¸ªagentå®ä¾‹
        logger.info("æ­£åœ¨åˆ›å»ºagentå®ä¾‹...")
        self.editor_agent = create_agent_from_prompt_template(
            agent_name="generated_agents_prompts/pubmed_literature_editor_assistant/pubmed_literature_editor_assistant",
            **self.agent_params
        )
        
        self.writing_agent = create_agent_from_prompt_template(
            agent_name="generated_agents_prompts/pubmed_literature_writing_assistant/pubmed_literature_writing_assistant",
            **self.agent_params
        )
        
        self.review_agent = create_agent_from_prompt_template(
            agent_name="generated_agents_prompts/pubmed_literature_review_assistant/pubmed_literature_review_assistant",
            **self.agent_params
        )
        
        logger.info(f"Editor agentåˆ›å»ºå®Œæˆ: {self.editor_agent.name}")
        logger.info(f"Writing agentåˆ›å»ºå®Œæˆ: {self.writing_agent.name}")
        logger.info(f"Review agentåˆ›å»ºå®Œæˆ: {self.review_agent.name}")
        
        # åˆ›å»ºSwarmï¼Œè®¾ç½®editorä¸ºå…¥å£agent
        # æ³¨æ„ï¼šè™½ç„¶entry_pointæ˜¯editorï¼Œä½†å®é™…å·¥ä½œæµç”±æˆ‘ä»¬æ§åˆ¶agentçš„è°ƒç”¨é¡ºåº
        self.swarm = Swarm(
            [self.editor_agent, self.writing_agent, self.review_agent],
            entry_point=self.review_agent,
            max_handoffs=20,
            max_iterations=20,
            execution_timeout=6000.0,  # 15 minutes
            node_timeout=3000.0,       # 5 minutes per agent
            repetitive_handoff_detection_window=8,  # There must be >= 3 unique agents in the last 8 handoffs
            repetitive_handoff_min_unique_agents=3
            
        )
        logger.info("Swarmåˆ›å»ºå®Œæˆï¼Œentry_pointä¸ºreviewï¼ˆä½†å·¥ä½œæµç”±ä»£ç æ§åˆ¶ï¼‰")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.base_cache_dir = Path(".cache/pmc_literature")
        self.output_dir = self.base_cache_dir / research_id / "optimization"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def _load_step4_status(self) -> Dict[str, Any]:
        """åŠ è½½step4.statusæ–‡ä»¶"""
        status_path = self.base_cache_dir / self.research_id / "step4.status"
        if not status_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°step4.statusæ–‡ä»¶: {status_path}")
        
        with open(status_path, "r", encoding="utf-8") as f:
            status = json.load(f)
        
        return status
    
    def _get_current_version(self) -> str:
        """ä»step4.statusè·å–å½“å‰ç‰ˆæœ¬å·"""
        step4_status = self._load_step4_status()
        current_version = step4_status.get("current_version", "1")
        logger.info(f"è·å–åˆ°å½“å‰ç‰ˆæœ¬å·: {current_version}")
        return str(current_version)
    
    def _load_version_file(self, version_file_path: str) -> str:
        """åŠ è½½version_file_pathæŒ‡å®šçš„æ–‡ä»¶å†…å®¹"""
        # å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
        if not os.path.isabs(version_file_path):
            file_path = Path(version_file_path)
        else:
            file_path = Path(version_file_path)
        
        # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œå°è¯•ä»å½“å‰ç›®å½•æˆ–ç¼“å­˜ç›®å½•æŸ¥æ‰¾
        if not file_path.is_absolute():
            # å°è¯•ä»ç¼“å­˜ç›®å½•æŸ¥æ‰¾
            cache_path = self.base_cache_dir / self.research_id / file_path
            if cache_path.exists():
                file_path = cache_path
            else:
                # å°è¯•ä»å½“å‰ç›®å½•æŸ¥æ‰¾
                current_path = Path(file_path)
                if current_path.exists():
                    file_path = current_path
                else:
                    raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {version_file_path}")
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶å†…å®¹: {file_path}, é•¿åº¦: {len(content)} å­—ç¬¦")
        return content
    
    def _save_iteration_result(self, iteration: int, stage: str, content: str) -> Path:
        """ä¿å­˜è¿­ä»£ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%dT%H%M%S")
        filename = f"iteration_{iteration}_{stage}_{timestamp}.md"
        file_path = self.output_dir / filename
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"ä¿å­˜è¿­ä»£ç»“æœ: {file_path}")
        return file_path
    
    def _save_status(self, status: Dict[str, Any]) -> None:
        """ä¿å­˜å·¥ä½œæµçŠ¶æ€"""
        status_file = self.output_dir / "workflow_status.json"
        with open(status_file, "w", encoding="utf-8") as f:
            json.dump(status, f, ensure_ascii=False, indent=2)
    
    def _parse_approval_status(self, response: str) -> bool:
        """
        ä»agentå“åº”ä¸­è§£ææ˜¯å¦é€šè¿‡
        ç®€å•åˆ¤æ–­ï¼šå¦‚æœå“åº”ä¸­åŒ…å«æ˜ç¡®çš„é€šè¿‡æ ‡è¯†ï¼Œè¿”å›True
        """
        response_lower = response.lower()
        # æŸ¥æ‰¾é€šè¿‡æ ‡è¯†
        approval_keywords = [
            "é€šè¿‡", "approved", "accept", "å¯ä»¥", "æ»¡æ„", 
            "good", "excellent", "no further changes needed",
            "æ— éœ€ä¿®æ”¹", "ä¸éœ€è¦ä¿®æ”¹"
        ]
        
        rejection_keywords = [
            "ä¸é€šè¿‡", "reject", "æ‹’ç»", "éœ€è¦ä¿®æ”¹", "è¿˜éœ€è¦",
            "needs revision", "requires changes", "ä¿®æ”¹", "improve"
        ]
        
        # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        approval_count = sum(1 for keyword in approval_keywords if keyword in response_lower)
        rejection_count = sum(1 for keyword in rejection_keywords if keyword in response_lower)
        
        # å¦‚æœæ˜ç¡®æœ‰é€šè¿‡æ ‡è¯†ä¸”æ‹’ç»æ ‡è¯†è¾ƒå°‘ï¼Œè®¤ä¸ºé€šè¿‡
        if approval_count > 0 and approval_count > rejection_count:
            return True
        
        # å¦‚æœæœ‰æ˜ç¡®çš„æ‹’ç»æ ‡è¯†ï¼Œè®¤ä¸ºä¸é€šè¿‡
        if rejection_count > approval_count:
            return False
        
        # é»˜è®¤éœ€è¦ç»§ç»­ä¿®æ”¹ï¼ˆä¸é€šè¿‡ï¼‰
        return False
    
    def run_optimization(self, max_iterations: int = 10) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¼˜åŒ–å·¥ä½œæµ
        
        Args:
            max_iterations (int): æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤10æ¬¡
            
        Returns:
            Dict: å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œä¼˜åŒ–å·¥ä½œæµï¼Œresearch_id: {self.research_id}")
            
            # 1. åŠ è½½step4.status
            logger.info("åŠ è½½step4.statusæ–‡ä»¶...")
            step4_status = self._load_step4_status()
            
            # 2. è·å–å½“å‰ç‰ˆæœ¬å·
            current_version = self._get_current_version()
            
            # 3. åŠ è½½version_file_pathæŒ‡å®šçš„æ–‡ä»¶å†…å®¹
            version_file_path = step4_status.get("version_file_path")
            if not version_file_path:
                raise ValueError("step4.statusä¸­æœªæ‰¾åˆ°version_file_pathå­—æ®µ")
            
            logger.info(f"åŠ è½½æ–‡ä»¶å†…å®¹: {version_file_path}")
            initial_content = self._load_version_file(version_file_path)
            
            # 4. æ„å»ºåˆå§‹æç¤ºè¯
            initial_prompt = f"""
è¯·å¯¹ä»¥ä¸‹æ–‡çŒ®å†…å®¹è¿›è¡Œä¼˜åŒ–å·¥ä½œæµï¼š

ç ”ç©¶ID: {self.research_id}
å½“å‰ç‰ˆæœ¬: {current_version}
åŸå§‹æ–‡ä»¶è·¯å¾„: {version_file_path}
**è¯·é€šè¿‡file_readå·¥å…·è¯»å–æ–‡çŒ®/æ–‡ä»¶å†…å®¹**

**é‡è¦ç‰ˆæœ¬ç®¡ç†è¯´æ˜**ï¼š
- æ‰€æœ‰reviewerè¾“å‡ºçš„ä¸´æ—¶æ–‡ä»¶åº”ä¿å­˜åœ¨: .cache/pmc_literature/{self.research_id}/feedback/reviewer/{current_version}/
- æ‰€æœ‰editorè¾“å‡ºçš„ä¸´æ—¶æ–‡ä»¶åº”ä¿å­˜åœ¨: .cache/pmc_literature/{self.research_id}/feedback/editor/{current_version}/
- è¯·ä½¿ç”¨research_id={self.research_id}å’Œversion={current_version}å‚æ•°ï¼Œé™¤éwritingæ›´æ–°äº†version id
- file_writeè¯·éµå¾ªä¸´æ—¶æ–‡ä»¶å­˜å‚¨è¦æ±‚ï¼Œæ— è®ºè¾“å‡ºæ ¼å¼æ˜¯mdè¿˜æ˜¯pngç­‰å…¶ä»–æ ¼å¼ï¼Œéƒ½åº”ä¿å­˜åœ¨.cache/pmc_literature/{self.research_id}/feedback/reviewer/{current_version}/æˆ–.cache/pmc_literature/{self.research_id}/feedback/editor/{current_version}/ç›®å½•ä¸‹

å·¥ä½œæµè¯´æ˜ï¼š
1. reviewer agenté¦–å…ˆå®¡æ ¸æ–‡çŒ®å¹¶ç»™å‡ºæ„è§
2. writing agentæ ¹æ®reviewçš„æ„è§è¿›è¡Œä¿®æ­£ï¼Œå¹¶è¿”å›JSONè¾“å‡º
3. é‡å¤æ­¥éª¤1ï¼Œå¦‚æœé€šè¿‡ï¼Œåˆ™editor agentç»™å‡ºæ„è§
4. writing agentæ ¹æ®editorçš„æ„è§è¿›è¡Œä¿®æ­£
5. å¾ªç¯æ­¤è¿‡ç¨‹ç›´åˆ°editorå’Œreviewéƒ½é€šè¿‡

**æ³¨æ„äº‹é¡¹**
- reviewerå®¡æ ¸é€šè¿‡åº”handoffç»™editor agentï¼Œeditorå®¡æ ¸é€šè¿‡åº”è¿”å›ç»“è®ºå¹¶ç»“æŸå·¥ä½œæµ
- reviewerå’Œeditorå®¡æ ¸æœªé€šè¿‡æ—¶å‡handoffç»™writing agent
- writing agentå®Œæˆä»»åŠ¡ååº”è¿”å›JSONè¾“å‡ºï¼Œå¹¶handoffç»™æœªé€šè¿‡çš„agent

**å¿…é¡»**ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦è¿”å›å…¶ä»–å†…å®¹ï¼š
```json
{{
    "status": "success",
    "research_id": "{self.research_id}",
    "version": "new_version_number",
    "file_path": "ä¿å­˜çš„æ–‡ä»¶è·¯å¾„",
    "message": "æˆåŠŸæ›´æ–°ç»¼è¿°"
}}
```
"""
            
            # åˆå§‹åŒ–çŠ¶æ€
            current_content = initial_content
            iteration = 1
            workflow_status = {
                "research_id": self.research_id,
                "started_at": datetime.now().isoformat(),
                "iterations": [],
                "final_status": "running"
            }
            
            review_passed = False
            editor_passed = False
            stage = "review"  # å½“å‰é˜¶æ®µï¼šreview æˆ– editor
            
            logger.info("å¼€å§‹è¿­ä»£ä¼˜åŒ–...")
            
            while iteration <= max_iterations:
                logger.info(f"\n{'='*80}")
                logger.info(f"è¿­ä»£ {iteration}/{max_iterations} - é˜¶æ®µ: {stage}")
                logger.info(f"{'='*80}")
                
                iteration_result = {
                    "iteration": iteration,
                    "stage": stage,
                    "timestamp": datetime.now().isoformat(),
                    "status": "running"
                }
                result = self.swarm(initial_prompt)
                logger.info(f"Status: {result.status}")
                logger.info(f"Node history: {[node.node_id for node in result.node_history]}")
                
                
            # ä¿å­˜æœ€ç»ˆç»“æœ
            if iteration > max_iterations:
                workflow_status["final_status"] = "max_iterations_reached"
            
            workflow_status["ended_at"] = datetime.now().isoformat()
            workflow_status["final_content"] = current_content
            
            # ä¿å­˜æœ€ç»ˆå†…å®¹
            final_file = self.output_dir / "final_content.md"
            with open(final_file, "w", encoding="utf-8") as f:
                f.write(current_content)
            workflow_status["final_file"] = str(final_file)
            
            self._save_status(workflow_status)
            
            logger.info(f"\n{'='*80}")
            logger.info(f"ä¼˜åŒ–å·¥ä½œæµå®Œæˆ")
            logger.info(f"æœ€ç»ˆçŠ¶æ€: {workflow_status['final_status']}")
            logger.info(f"æ€»è¿­ä»£æ¬¡æ•°: {len(workflow_status['iterations'])}")
            logger.info(f"æœ€ç»ˆæ–‡ä»¶: {final_file}")
            logger.info(f"{'='*80}")
            
            return workflow_status
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PubMedæ–‡çŒ®ä¼˜åŒ–å·¥ä½œæµ')
    parser.add_argument('-r', '--research_id', type=str, required=True,
                       help='ç ”ç©¶IDï¼Œå¯¹åº”.cache/pmc_literatureä¸‹çš„ç›®å½•å')
    parser.add_argument('-m', '--max_iterations', type=int, default=10,
                       help='æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤10æ¬¡')
    parser.add_argument('--env', type=str, default='production',
                       help='ç¯å¢ƒé…ç½® (development, production, testing)')
    parser.add_argument('--version', type=str, default='latest',
                       help='æ™ºèƒ½ä½“ç‰ˆæœ¬')
    parser.add_argument('--model_id', type=str, default='default',
                       help='ä½¿ç”¨çš„æ¨¡å‹ID')
    
    args = parser.parse_args()
    
    
    # åˆ›å»ºå·¥ä½œæµå®ä¾‹
    workflow = PubmedLiteratureOptimizationWorkflow(
        research_id=args.research_id,
        env=args.env,
        version=args.version,
        model_id=args.model_id
    )
    
    print(f"âœ… PubMedæ–‡çŒ®ä¼˜åŒ–å·¥ä½œæµåˆ›å»ºæˆåŠŸ")
    print(f"ğŸ“‹ ç ”ç©¶ID: {args.research_id}")
    print(f"ğŸ”„ æœ€å¤§è¿­ä»£æ¬¡æ•°: {args.max_iterations}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {workflow.output_dir}")
    print(f"{'='*80}\n")
    
    # æ‰§è¡Œä¼˜åŒ–å·¥ä½œæµ
    try:
        result = workflow.run_optimization(max_iterations=args.max_iterations)
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ä¼˜åŒ–å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print(f"ğŸ“Š æœ€ç»ˆçŠ¶æ€: {result['final_status']}")
        print(f"ğŸ”„ æ€»è¿­ä»£æ¬¡æ•°: {len(result['iterations'])}")
        print(f"ğŸ“„ æœ€ç»ˆæ–‡ä»¶: {result.get('final_file', 'N/A')}")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"\nâŒ ä¼˜åŒ–å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()

