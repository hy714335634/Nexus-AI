#!/usr/bin/env python3
"""
PMCæ–‡çŒ®åŠ©æ‰‹ (PMC Literature Assistant)
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ï¼Œå®ç°æ‰¹é‡æ–‡çŒ®æ£€ç´¢ã€åˆ†æå’Œæ ‡è®°
"""

import os
import sys
import json
import argparse
from pathlib import Path
from nexus_utils.agent_factory import create_agent_from_prompt_template

# å¯¼å…¥å·¥å…·ï¼ˆç”¨äºåç»­æ­¥éª¤ï¼‰
from tools.generated_tools.pubmed_literature_screen_assistant.literature_analyzer import analyze_literature_with_query
from tools.generated_tools.pubmed_literature_screen_assistant.mark_literature import mark_literature


def create_literature_assistant():
    """åˆ›å»ºPMCæ–‡çŒ®åŠ©æ‰‹"""
    return create_agent_from_prompt_template(
        agent_name="generated_agents_prompts/pubmed_literature_screen_assistant/pubmed_literature_screen_assistant",
        env="production",
        version="latest",
        model_id="default"
    )


def create_literature_analyzer():
    """åˆ›å»ºæ–‡çŒ®åˆ†æagent"""
    from tools.generated_tools.pubmed_literature_screen_assistant.literature_analyzer import LiteratureAnalyzerAgent
    return LiteratureAnalyzerAgent(env="production")


def step1_search_literature(query: str, research_id: str, api_key: str = None, max_results: int = 50):
    """
    æ­¥éª¤1: æ£€ç´¢æ–‡çŒ® - è®©agentè‡ªä¸»å†³å®šæ£€ç´¢ç­–ç•¥
    """
    print(f"\n{'='*80}")
    print("æ­¥éª¤1: ä½¿ç”¨agentæ£€ç´¢æ–‡çŒ®")
    print(f"{'='*80}")
    print(f"æŸ¥è¯¢: {query}")
    print(f"Research ID: {research_id}")
    print(f"æœ€å¤§ç»“æœæ•°: {max_results}")
    
    # è®¾ç½®APIå¯†é’¥
    if api_key:
        os.environ["NCBI_API_KEY"] = api_key
    
    # åˆ›å»ºagent
    agent = create_literature_assistant()
    print(f"âœ… Agentåˆ›å»ºæˆåŠŸ: {agent.name}")
    
    # æ„é€ æŸ¥è¯¢æç¤ºè¯
    prompt = f"""ç”¨æˆ·æ–‡çŒ®æ”¶é›†åŠç­›é€‰éœ€æ±‚: {query}

è¯·ä½¿ç”¨search_by_keywordsæˆ–search_by_filterså·¥å…·æ£€ç´¢æ–‡çŒ®ï¼Œç ”ç©¶IDä¸º: {research_id}ï¼Œæœ€å¤§æ£€ç´¢ç»“æœæ•°ä¸º: {max_results}ã€‚

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªä¸»å†³å®šä½¿ç”¨å“ªäº›å…³é”®è¯è¿›è¡Œæ£€ç´¢ã€‚
"""
    
    print(f"\nğŸ“ å°†æŸ¥è¯¢ä¼ é€’ç»™agentï¼Œç”±agentè‡ªä¸»å†³å®šæ£€ç´¢ç­–ç•¥...")
    
    # è°ƒç”¨agent
    result = agent(prompt)
    
    print(f"\nâœ… Agentå¤„ç†å®Œæˆ")
    print(f"Agentå“åº”: {result.message[:200]}...")
    
    # ä»ç¼“å­˜ç›®å½•è·å–æ£€ç´¢åˆ°çš„æ–‡çŒ®ID
    cache_dir = Path(f".cache/pmc_literature/{research_id}/meta_data")
    if cache_dir.exists():
        pmc_ids = [f.stem for f in cache_dir.glob("*.json")]
        print(f"âœ… æ£€ç´¢åˆ° {len(pmc_ids)} ç¯‡æ–‡çŒ®")
        return pmc_ids
    else:
        print(f"âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return []


def step2_analyze_literature(pmc_ids: list, research_id: str, user_query: str):
    """
    æ­¥éª¤2: åˆ†ææ–‡çŒ® - åˆ›å»ºç‹¬ç«‹çš„agentå¯¹æ¯ç¯‡æ–‡çŒ®è¿›è¡Œåˆ†æ
    """
    print(f"\n{'='*80}")
    print("æ­¥éª¤2: åˆ†ææ–‡çŒ®")
    print(f"{'='*80}")
    
    # åˆ›å»ºanalysis_resultsç›®å½•
    cache_dir = Path(f".cache/pmc_literature/{research_id}")
    analysis_dir = cache_dir / "analysis_results"
    analysis_dir.mkdir(parents=True, exist_ok=True)
    
    analyzed_results = []
    
    for i, pmc_id in enumerate(pmc_ids, 1):
        print(f"\n[{i}/{len(pmc_ids)}] åˆ†ææ–‡çŒ®: {pmc_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ†æè¿‡
        analysis_file = analysis_dir / f"{pmc_id}.json"
        if analysis_file.exists():
            print(f"  âœ… å·²å­˜åœ¨åˆ†æç»“æœï¼Œè·³è¿‡")
            try:
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    analysis = json.load(f)
                analyzed_results.append({
                    "pmcid": pmc_id,
                    "analysis": analysis
                })
            except Exception as e:
                print(f"  âš ï¸  è¯»å–åˆ†æç»“æœå¤±è´¥: {str(e)}")
            continue
        
        try:
            # è°ƒç”¨åˆ†æå·¥å…·
            result = analyze_literature_with_query(research_id, user_query, [pmc_id])
            
            # å°è¯•è§£æJSON
            try:
                analysis = json.loads(result)
                
                # ä¿å­˜åˆ†æç»“æœ
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis, f, ensure_ascii=False, indent=2)
                
                analyzed_results.append({
                    "pmcid": pmc_id,
                    "analysis": analysis
                })
                
                # æ‰“å°åˆ†æç»“æœ
                should_mark = analysis.get("should_mark", False)
                relevance_score = analysis.get("relevance_score", 0)
                print(f"  -> should_mark: {should_mark}, relevance_score: {relevance_score}")
                
            except json.JSONDecodeError:
                print(f"  âš ï¸  æ— æ³•è§£æJSONç»“æœï¼Œè·³è¿‡")
                analyzed_results.append({
                    "pmcid": pmc_id,
                    "analysis": {"should_mark": False, "error": "Failed to parse JSON"}
                })
                
        except Exception as e:
            print(f"  âŒ åˆ†æå¤±è´¥: {str(e)}")
            analyzed_results.append({
                "pmcid": pmc_id,
                "analysis": {"should_mark": False, "error": str(e)}
            })
    
    return analyzed_results


def step3_mark_literature(research_id: str):
    """
    æ­¥éª¤3: æ ‡è®°ç›¸å…³æ–‡çŒ® - ä»analysis_resultsç›®å½•è¯»å–æ‰€æœ‰åˆ†æç»“æœ
    """
    print(f"\n{'='*80}")
    print("æ­¥éª¤3: æ ‡è®°ç›¸å…³æ–‡çŒ®")
    print(f"{'='*80}")
    
    # è¯»å–analysis_resultsç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
    analysis_dir = Path(f".cache/pmc_literature/{research_id}/analysis_results")
    
    if not analysis_dir.exists():
        print("âŒ analysis_resultsç›®å½•ä¸å­˜åœ¨")
        return
    
    # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    to_mark = []
    reasoning_dict = {}
    
    json_files = list(analysis_dir.glob("*.json"))
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªåˆ†æç»“æœæ–‡ä»¶")
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            pmcid = json_file.stem
            
            if analysis.get("should_mark", False):
                to_mark.append(pmcid)
                reasoning_dict[pmcid] = analysis.get("reasoning", "")
                
        except Exception as e:
            print(f"âš ï¸  è¯»å–åˆ†æç»“æœå¤±è´¥ {json_file.name}: {str(e)}")
    
    print(f"æ‰¾åˆ° {len(to_mark)} ç¯‡ç›¸å…³æ–‡çŒ®éœ€è¦æ ‡è®°")
    
    if not to_mark:
        print("æ²¡æœ‰æ–‡çŒ®éœ€è¦æ ‡è®°")
        return
    
    # æ‰¹é‡æ ‡è®°ï¼ˆæ¯æ‰¹10ç¯‡ï¼‰
    batch_size = 10
    for i in range(0, len(to_mark), batch_size):
        batch = to_mark[i:i+batch_size]
        batch_reasoning = {pmcid: reasoning_dict[pmcid] for pmcid in batch}
        
        print(f"\næ ‡è®°æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ç¯‡æ–‡çŒ®")
        
        try:
            result = mark_literature(batch, research_id, batch_reasoning, auto_download=False)
            result_data = json.loads(result)
            
            if result_data.get("status") == "success":
                print(f"âœ… æˆåŠŸæ ‡è®° {result_data.get('marked_count', 0)} ç¯‡æ–‡çŒ®")
            else:
                print(f"âŒ æ ‡è®°å¤±è´¥: {result_data.get('message', 'Unknown error')}")
                
        except Exception as e:
            print(f"âŒ æ ‡è®°å¤±è´¥: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description='PMCæ–‡çŒ®æ£€ç´¢ã€åˆ†æå’Œæ ‡è®°å·¥å…·')
    parser.add_argument('--query', '-q', type=str, required=True,
                        help='ç”¨æˆ·æŸ¥è¯¢è¯­å¥')
    parser.add_argument('--api-key', '-k', type=str, default=None,
                        help='NCBI APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--research-id', '-r', type=str, default=None,
                        help='ç ”ç©¶IDï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--max-results', '-m', type=int, default=100,
                        help='æœ€å¤§æ£€ç´¢ç»“æœæ•°ï¼ˆé»˜è®¤50ï¼‰')
    parser.add_argument('--search-only', '-s', action='store_true',
                        help='åªæ‰§è¡Œæ£€ç´¢ï¼Œä¸è¿›è¡Œåˆ†æå’Œæ ‡è®°')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆresearch_id
    from datetime import datetime
    if not args.research_id:
        timestamp = datetime.now().strftime("%Y%m%d")
        args.research_id = f"literature_search_{timestamp}"
    
    print(f"\n{'='*80}")
    print("PMCæ–‡çŒ®æ£€ç´¢å’Œåˆ†æå·¥å…·")
    print(f"{'='*80}")
    print(f"Research ID: {args.research_id}")
    print(f"User Query: {args.query}")
    print(f"{'='*80}\n")
    
    # æ­¥éª¤1: æ£€ç´¢æ–‡çŒ®
    pmc_ids = step1_search_literature(args.query, args.research_id, args.api_key, args.max_results)
    
    if not pmc_ids:
        print("\nâŒ æ²¡æœ‰æ£€ç´¢åˆ°æ–‡çŒ®ï¼Œé€€å‡º")
        return
    
    if args.search_only:
        print("\nåªæ‰§è¡Œæ£€ç´¢æ­¥éª¤ï¼Œè·³è¿‡åˆ†æå’Œæ ‡è®°")
        return
    
    # æ­¥éª¤2: åˆ†ææ–‡çŒ®
    step2_analyze_literature(pmc_ids, args.research_id, args.query)
    
    # æ­¥éª¤3: æ ‡è®°æ–‡çŒ®ï¼ˆä»analysis_resultsç›®å½•è¯»å–ç»“æœï¼‰
    step3_mark_literature(args.research_id)
    
    print(f"\n{'='*80}")
    print("âœ… å®Œæˆï¼")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢
    if len(sys.argv) == 1:
        print("ä½¿ç”¨ç¤ºä¾‹:")
        print("  python pubmed_literature_screen_assistant.py --query 'ADMET prediction tools'")
        print("  python pubmed_literature_screen_assistant.py --query 'machine learning' --max-results 100")
        print("  python pubmed_literature_screen_assistant.py --query 'deep learning' --search-only")
        print("\nå¼€å§‹è¿è¡Œé»˜è®¤æŸ¥è¯¢...\n")
        
        # é»˜è®¤æŸ¥è¯¢
        agent = create_literature_assistant()
        print(f"PMCæ–‡çŒ®åŠ©æ‰‹åˆ›å»ºæˆåŠŸ: {agent.name}")
        result = agent("ç”¨æˆ·æ–‡çŒ®æ”¶é›†åŠç­›é€‰éœ€æ±‚:ADMETï¼ˆå¸æ”¶ã€åˆ†å¸ƒã€ä»£è°¢ã€æ’æ³„å’Œæ¯’æ€§ï¼‰é¢„æµ‹æ˜¯æ–°è¯ç ”å‘ä¸­çš„å…³é”®ç¯èŠ‚ã€‚è°ƒç ”ä¸€ä¸‹å½“å‰å¤§å¤šæ•°state of the artçš„ADMETé¢„æµ‹å·¥å…·ï¼Œå†™ä¸€ä»½å…³äºADMETé¢„æµ‹å·¥å…·çš„è°ƒç ”æŠ¥å‘Šï¼ˆæ·±åº¦å­¦ä¹ çš„ï¼Œå¤§æ¨¡å‹çš„ï¼Œå¼ºç®—æ³•ç±»çš„ç­‰ç­‰ï¼‰,è¦æ±‚æŸ¥çœ‹è¿‘äº”å¹´çš„æ–‡çŒ®")
        print(f"æ™ºèƒ½ä½“å“åº”:\n{result}")
    else:
        main()