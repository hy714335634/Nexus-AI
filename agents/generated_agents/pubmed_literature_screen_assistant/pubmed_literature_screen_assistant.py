#!/usr/bin/env python3
"""
PMCæ–‡çŒ®åŠ©æ‰‹ (PMC Literature Assistant)
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ï¼Œå®ç°æ‰¹é‡æ–‡çŒ®æ£€ç´¢ã€åˆ†æå’Œæ ‡è®°
"""

import os
import sys
import json
import argparse
from pathlib import Path
from nexus_utils.agent_factory import create_agent_from_prompt_template
from nexus_utils.config_loader import ConfigLoader
config = ConfigLoader()
# å¯¼å…¥å·¥å…·ï¼ˆç”¨äºåç»­æ­¥éª¤ï¼‰
from tools.generated_tools.pubmed_literature_screen_assistant.literature_analyzer import analyze_literature_with_query
from tools.generated_tools.pubmed_literature_screen_assistant.mark_literature import mark_literature

os.environ["BYPASS_TOOL_CONSENT"] = "true"
otel_endpoint = config.get_with_env_override(
    "OTEL_EXPORTER_OTLP_ENDPOINT",
    "nexus_ai", "OTEL_EXPORTER_OTLP_ENDPOINT",
    default="http://localhost:4318"
)
os.environ.setdefault("OTEL_EXPORTER_OTLP_ENDPOINT", otel_endpoint)
strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter()



def create_literature_assistant():
    """åˆ›å»ºPMCæ–‡çŒ®åŠ©æ‰‹"""
    return create_agent_from_prompt_template(
        agent_name="generated_agents_prompts/pubmed_literature_screen_assistant/pubmed_literature_screen_assistant",
        env="production",
        version="latest",
        model_id="default"
    )

def create_literature_analyzer():
    """åˆ›å»ºæ–‡çŒ®åˆ†æagent"""
    from tools.generated_tools.pubmed_literature_screen_assistant.literature_analyzer import LiteratureAnalyzerAgent
    return LiteratureAnalyzerAgent(env="production")


def step1_search_literature(query: str, research_id: str, api_key: str = None, max_results: int = 50):
    """
    æ­¥éª¤1: æ£€ç´¢æ–‡çŒ® - è®©agentè‡ªä¸»å†³å®šæ£€ç´¢ç­–ç•¥
    """
    print(f"\n{'='*80}")
    print("æ­¥éª¤1: ä½¿ç”¨agentæ£€ç´¢æ–‡çŒ®")
    print(f"{'='*80}")
    print(f"æŸ¥è¯¢: {query}")
    print(f"Research ID: {research_id}")
    print(f"æœ€å¤§ç»“æœæ•°: {max_results}")
    
    # è®¾ç½®APIå¯†é’¥
    if api_key:
        os.environ["NCBI_API_KEY"] = api_key
    
    # åˆ›å»ºagent
    agent = create_literature_assistant()
    print(f"âœ… Agentåˆ›å»ºæˆåŠŸ: {agent.name}")
    
    # æ„é€ æŸ¥è¯¢æç¤ºè¯
    prompt = f"""ç”¨æˆ·æ–‡çŒ®æ”¶é›†åŠç­›é€‰éœ€æ±‚: {query}

è¯·ä½¿ç”¨search_by_keywordsæˆ–search_by_filterså·¥å…·æ£€ç´¢æ–‡çŒ®ï¼Œç ”ç©¶IDä¸º: {research_id}ï¼Œæœ€å¤§æ£€ç´¢ç»“æœæ•°ä¸º: {max_results}ã€‚

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªä¸»å†³å®šä½¿ç”¨å“ªäº›å…³é”®è¯è¿›è¡Œæ£€ç´¢ã€‚
"""
    
    print(f"\nğŸ“ å°†æŸ¥è¯¢ä¼ é€’ç»™agentï¼Œç”±agentè‡ªä¸»å†³å®šæ£€ç´¢ç­–ç•¥...")
    
    # è°ƒç”¨agent
    result = agent(prompt)
    
    print(f"\nâœ… Agentå¤„ç†å®Œæˆ")
    print(f"Agentå“åº”: {result.message}...")
    
    # ä»ç¼“å­˜ç›®å½•è·å–æ£€ç´¢åˆ°çš„æ–‡çŒ®ID
    cache_dir = Path(f".cache/pmc_literature/{research_id}/meta_data")
    if cache_dir.exists():
        pmc_ids = [f.stem for f in cache_dir.glob("*.json")]
        print(f"âœ… æ£€ç´¢åˆ° {len(pmc_ids)} ç¯‡æ–‡çŒ®")
        
        # ä¿å­˜æ­¥éª¤1çŠ¶æ€
        cache_dir.parent.mkdir(parents=True, exist_ok=True)
        status_file = cache_dir.parent / "step1.status"
        with open(status_file, 'w', encoding='utf-8') as f:
            f.write(result.message['content'][0]['text'])
        
        return pmc_ids
    else:
        print(f"âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return []


def step2_analyze_literature(pmc_ids: list, research_id: str, user_query: str):
    """
    æ­¥éª¤2: åˆ†ææ–‡çŒ® - åˆ›å»ºç‹¬ç«‹çš„agentå¯¹æ¯ç¯‡æ–‡çŒ®è¿›è¡Œåˆ†æ
    """
    print(f"\n{'='*80}")
    print("æ­¥éª¤2: åˆ†ææ–‡çŒ®")
    print(f"{'='*80}")
    
    # åˆ›å»ºanalysis_resultsç›®å½•
    cache_dir = Path(f".cache/pmc_literature/{research_id}")
    analysis_dir = cache_dir / "analysis_results"
    analysis_dir.mkdir(parents=True, exist_ok=True)
    
    analyzed_results = []
    skipped_count = 0
    
    for i, pmc_id in enumerate(pmc_ids, 1):
        print(f"\n[{i}/{len(pmc_ids)}] åˆ†ææ–‡çŒ®: {pmc_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ†æè¿‡
        analysis_file = analysis_dir / f"{pmc_id}.json"
        if analysis_file.exists():
            print(f"  âœ… å·²å­˜åœ¨åˆ†æç»“æœï¼Œè·³è¿‡")
            skipped_count += 1
            try:
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    analysis = json.load(f)
                analyzed_results.append({
                    "pmcid": pmc_id,
                    "analysis": analysis
                })
            except Exception as e:
                print(f"  âš ï¸  è¯»å–åˆ†æç»“æœå¤±è´¥: {str(e)}")
            continue
        
        try:
            # è°ƒç”¨åˆ†æå·¥å…·
            result = analyze_literature_with_query(research_id, user_query, [pmc_id])
            
            # å°è¯•è§£æJSON
            try:
                analysis = json.loads(result)
                
                # ä¿å­˜åˆ†æç»“æœ
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis, f, ensure_ascii=False, indent=2)
                
                analyzed_results.append({
                    "pmcid": pmc_id,
                    "analysis": analysis
                })
                
                # æ‰“å°åˆ†æç»“æœ
                should_mark = analysis.get("should_mark", False)
                relevance_score = analysis.get("relevance_score", 0)
                print(f"  -> should_mark: {should_mark}, relevance_score: {relevance_score}")
                
            except json.JSONDecodeError:
                print(f"  âš ï¸  æ— æ³•è§£æJSONç»“æœï¼Œè·³è¿‡")
                analyzed_results.append({
                    "pmcid": pmc_id,
                    "analysis": {"should_mark": False, "error": "Failed to parse JSON"}
                })
                
        except Exception as e:
            print(f"  âŒ åˆ†æå¤±è´¥: {str(e)}")
            analyzed_results.append({
                "pmcid": pmc_id,
                "analysis": {"should_mark": False, "error": str(e)}
            })
    
    # ä¿å­˜æ­¥éª¤2çŠ¶æ€
    if skipped_count > 0:
        print(f"\nğŸ“Š è·³è¿‡ {skipped_count} ä¸ªå·²åˆ†æçš„æ–‡ä»¶ï¼Œæ–°å¢åˆ†æ {len(analyzed_results) - skipped_count} ä¸ª")
    
    status_file = cache_dir / "step2.status"
    with open(status_file, 'w', encoding='utf-8') as f:
        json.dump(analyzed_results, f, ensure_ascii=False, indent=2)
    
    return analyzed_results


def step3_mark_literature(research_id: str):
    """
    æ­¥éª¤3: æ ‡è®°ç›¸å…³æ–‡çŒ® - ä»analysis_resultsç›®å½•è¯»å–æ‰€æœ‰åˆ†æç»“æœ
    """
    print(f"\n{'='*80}")
    print("æ­¥éª¤3: æ ‡è®°ç›¸å…³æ–‡çŒ®")
    print(f"{'='*80}")
    
    # è¯»å–analysis_resultsç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
    analysis_dir = Path(f".cache/pmc_literature/{research_id}/analysis_results")
    
    if not analysis_dir.exists():
        print("âŒ analysis_resultsç›®å½•ä¸å­˜åœ¨")
        return
    
    # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    to_mark = []
    reasoning_dict = {}
    
    json_files = list(analysis_dir.glob("*.json"))
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªåˆ†æç»“æœæ–‡ä»¶")
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
            
            pmcid = json_file.stem
            
            if analysis.get("should_mark", False):
                to_mark.append(pmcid)
                reasoning_dict[pmcid] = analysis.get("reasoning", "")
                
        except Exception as e:
            print(f"âš ï¸  è¯»å–åˆ†æç»“æœå¤±è´¥ {json_file.name}: {str(e)}")
    
    print(f"æ‰¾åˆ° {len(to_mark)} ç¯‡ç›¸å…³æ–‡çŒ®éœ€è¦æ ‡è®°")
    
    if not to_mark:
        print("æ²¡æœ‰æ–‡çŒ®éœ€è¦æ ‡è®°")
        return
    
    # ä¸€æ¬¡æ€§æ ‡è®°æ‰€æœ‰æ–‡çŒ®
    print(f"\nå¼€å§‹æ ‡è®° {len(to_mark)} ç¯‡æ–‡çŒ®...")
    
    try:
        result = mark_literature(to_mark, research_id, reasoning_dict, auto_download=False)
        result_data = json.loads(result)
        
        if result_data.get("status") == "success":
            print(f"âœ… æˆåŠŸæ ‡è®° {result_data.get('marked_count', 0)} ç¯‡æ–‡çŒ®")
            
            # ä¿å­˜æ­¥éª¤3çŠ¶æ€
            cache_dir = Path(f".cache/pmc_literature/{research_id}")
            status_file = cache_dir / "step3.status"
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_to_mark": len(to_mark),
                    "marked_pmcs": to_mark
                }, f, ensure_ascii=False, indent=2)
        else:
            print(f"âŒ æ ‡è®°å¤±è´¥: {result_data.get('message', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ æ ‡è®°å¤±è´¥: {str(e)}")


def check_progress(research_id: str):
    """
    æ£€æŸ¥è¿›åº¦çŠ¶æ€ï¼Œè¿”å›éœ€è¦ä»å“ªä¸ªæ­¥éª¤å¼€å§‹
    æŒ‰ç…§æ­¥éª¤é¡ºåºæ£€æŸ¥ï¼šstep1 -> step2 -> step3 -> manifest.json
    
    Returns:
        è¿”å›éœ€è¦å¼€å§‹çš„æ­¥éª¤å· (1, 2, 3) æˆ– 0 (å…¨éƒ¨å®Œæˆ)
    """
    cache_dir = Path(f".cache/pmc_literature/{research_id}")
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ­¥éª¤1æ˜¯å¦å®Œæˆï¼ˆé€šè¿‡step1.statuså’Œmeta_dataç›®å½•ï¼‰
    step1_status = cache_dir / "step1.status"
    meta_data_dir = cache_dir / "meta_data"
    
    if not step1_status.exists() or not meta_data_dir.exists():
        print(f"\nâš ï¸  æ­¥éª¤1æœªå®Œæˆï¼Œä»æ­¥éª¤1å¼€å§‹")
        return 1
    
    meta_data_files = list(meta_data_dir.glob("*.json"))
    total_pmc_ids = len(meta_data_files)
    if total_pmc_ids == 0:
        print(f"\nâš ï¸  æ­¥éª¤1æœªå®Œæˆï¼ˆæ— æ–‡çŒ®æ•°æ®ï¼‰ï¼Œä»æ­¥éª¤1å¼€å§‹")
        return 1
    
    print(f"\nâœ… æ­¥éª¤1å·²å®Œæˆï¼šå‘ç° {total_pmc_ids} ç¯‡æ–‡çŒ®")
    
    # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ­¥éª¤2æ˜¯å¦å®Œæˆ
    # æ­¥éª¤2åŸºäºpaperæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶è¿›è¡Œåˆ†æ
    step2_status = cache_dir / "step2.status"
    analysis_dir = cache_dir / "analysis_results"
    paper_dir = cache_dir / "paper"
    
    # æ£€æŸ¥paperæ–‡ä»¶å¤¹ä¸‹çš„å¾…å¤„ç†æ–‡ä»¶
    if not paper_dir.exists():
        print(f"\nâš ï¸  paperæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œä»æ­¥éª¤2å¼€å§‹ï¼ˆå¯èƒ½éœ€è¦å…ˆä¸‹è½½å…¨æ–‡ï¼‰")
        return 2
    
    paper_files = list(paper_dir.glob("*.txt"))
    total_papers = len(paper_files)
    
    if total_papers == 0:
        print(f"\nâš ï¸  paperæ–‡ä»¶å¤¹ä¸ºç©ºï¼Œä»æ­¥éª¤2å¼€å§‹")
        return 2
    
    # æ£€æŸ¥analysis_resultsç›®å½•
    if not analysis_dir.exists():
        print(f"\nâš ï¸  æ­¥éª¤2æœªå¼€å§‹ï¼Œä»æ­¥éª¤2å¼€å§‹")
        return 2
    
    analysis_files = list(analysis_dir.glob("*.json"))
    
    # å¦‚æœæ²¡æœ‰step2.statusï¼Œè¯´æ˜æ­¥éª¤2æœªå®Œæˆ
    if not step2_status.exists():
        print(f"\nâš ï¸  æ­¥éª¤2æœªå®Œæˆï¼šå·²åˆ†æ {len(analysis_files)}/{total_papers} ç¯‡æ–‡çŒ®ï¼Œä»æ­¥éª¤2ç»§ç»­")
        return 2
    
    # å¯¹æ¯”paperæ–‡ä»¶å’Œanalysis_resultsï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­åˆ†æ
    paper_pmc_ids = set([f.stem for f in paper_files])
    analysis_pmc_ids = set([f.stem for f in analysis_files])
    
    if paper_pmc_ids == analysis_pmc_ids:
        print(f"\nâœ… æ­¥éª¤2å·²å®Œæˆï¼šåˆ†æäº† {len(analysis_files)} ç¯‡æ–‡çŒ®")
    else:
        print(f"\nâš ï¸  æ­¥éª¤2æœªå®Œæˆï¼šå·²åˆ†æ {len(analysis_files)}/{total_papers} ç¯‡æ–‡çŒ®ï¼Œä»æ­¥éª¤2ç»§ç»­")
        return 2
    
    # ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ­¥éª¤3æ˜¯å¦å®Œæˆï¼ˆé€šè¿‡step3.statusæ–‡ä»¶ï¼‰
    step3_status = cache_dir / "step3.status"
    
    if not step3_status.exists():
        print(f"\nâš ï¸  æ­¥éª¤3æœªå®Œæˆï¼Œä»æ­¥éª¤3å¼€å§‹")
        return 3
    
    # step3.statuså­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
    try:
        with open(step3_status, 'r', encoding='utf-8') as f:
            step3_data = json.load(f)
        marked_pmcs = step3_data.get('marked_pmcs', [])
        if not marked_pmcs:
            print(f"\nâš ï¸  æ­¥éª¤3æœªå®Œæˆï¼Œä»æ­¥éª¤3å¼€å§‹")
            return 3
        print(f"\nâœ… æ­¥éª¤3å·²å®Œæˆï¼šå·²æ ‡è®° {len(marked_pmcs)} ç¯‡æ–‡çŒ®")
    except Exception as e:
        print(f"\nâš ï¸  è¯»å–æ­¥éª¤3çŠ¶æ€å¤±è´¥: {str(e)}ï¼Œä»æ­¥éª¤3å¼€å§‹")
        return 3
    
    # ç¬¬å››æ­¥ï¼šæœ€ç»ˆæ£€æŸ¥manifest.jsonæ–‡ä»¶
    manifest_file = cache_dir / "manifest.json"
    
    if not manifest_file.exists():
        print(f"\nâš ï¸  manifest.jsonä¸å­˜åœ¨ï¼Œä»æ­¥éª¤3å¼€å§‹")
        return 3
    
    # manifest.jsonå­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
    try:
        with open(manifest_file, 'r', encoding='utf-8') as f:
            manifest = json.load(f)
        total_count = manifest.get("statistics", {}).get("total_count", 0)
        if total_count > 0:
            print(f"\nâœ… æ‰€æœ‰æ­¥éª¤å·²å®Œæˆï¼æ€»å…±æ ‡è®° {total_count} ç¯‡æ–‡çŒ®")
            return 0
        else:
            print(f"\nâš ï¸  manifest.jsonå­˜åœ¨ä½†æ— æ ‡è®°æ–‡çŒ®ï¼Œä»æ­¥éª¤3å¼€å§‹")
            return 3
    except Exception as e:
        print(f"\nâš ï¸  è¯»å–manifest.jsonå¤±è´¥: {str(e)}ï¼Œä»æ­¥éª¤3å¼€å§‹")
        return 3


def main():
    parser = argparse.ArgumentParser(description='PMCæ–‡çŒ®æ£€ç´¢ã€åˆ†æå’Œæ ‡è®°å·¥å…·')
    parser.add_argument('--query', '-q', type=str, required=True,
                        help='ç”¨æˆ·æŸ¥è¯¢è¯­å¥')
    parser.add_argument('--api-key', '-k', type=str, default=None,
                        help='NCBI APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--research-id', '-r', type=str, default=None,
                        help='ç ”ç©¶IDï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--max-results', '-m', type=int, default=100,
                        help='æœ€å¤§æ£€ç´¢ç»“æœæ•°ï¼ˆé»˜è®¤50ï¼‰')
    parser.add_argument('--search-only', '-s', action='store_true',
                        help='åªæ‰§è¡Œæ£€ç´¢ï¼Œä¸è¿›è¡Œåˆ†æå’Œæ ‡è®°')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆresearch_id
    from datetime import datetime
    if not args.research_id:
        timestamp = datetime.now().strftime("%Y%m%d")
        args.research_id = f"literature_search_{timestamp}"
    
    print(f"\n{'='*80}")
    print("PMCæ–‡çŒ®æ£€ç´¢å’Œåˆ†æå·¥å…·")
    print(f"{'='*80}")
    print(f"Research ID: {args.research_id}")
    print(f"User Query: {args.query}")
    print(f"{'='*80}\n")
    
    # æ£€æŸ¥è¿›åº¦
    start_step = check_progress(args.research_id)
    
    # æ ¹æ®è¿›åº¦å†³å®šä»å“ªä¸ªæ­¥éª¤å¼€å§‹
    if start_step == 0:
        print("\nâœ… æ‰€æœ‰æ­¥éª¤å·²å®Œæˆï¼Œæ— éœ€é‡æ–°è¿è¡Œ")
        return
    elif start_step == 3:
        # ä»æ­¥éª¤3å¼€å§‹
        print("\nä»æ–­ç‚¹ç»§ç»­ï¼šæ­¥éª¤3 - æ ‡è®°æ–‡çŒ®")
        step3_mark_literature(args.research_id)
        print(f"\n{'='*80}")
        print("âœ… å®Œæˆï¼")
        print(f"{'='*80}\n")
        return
    elif start_step == 2:
        # ä»æ­¥éª¤2å¼€å§‹
        print("\nä»æ–­ç‚¹ç»§ç»­ï¼šæ­¥éª¤2 - åˆ†ææ–‡çŒ®")
        
        # ä»paperæ–‡ä»¶å¤¹è·å–å¾…å¤„ç†çš„æ–‡çŒ®åˆ—è¡¨
        paper_dir = Path(f".cache/pmc_literature/{args.research_id}/paper")
        if not paper_dir.exists():
            print("âŒ paperç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•ç»§ç»­")
            return
        
        paper_files = list(paper_dir.glob("*.txt"))
        if not paper_files:
            print("âŒ æœªæ‰¾åˆ°æ–‡çŒ®å…¨æ–‡æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­")
            return
        
        pmc_ids = [f.stem for f in paper_files]
        print(f"æ‰¾åˆ° {len(pmc_ids)} ç¯‡æ–‡çŒ®éœ€è¦åˆ†æ")
        
        # æ­¥éª¤2: åˆ†ææ–‡çŒ®
        step2_analyze_literature(pmc_ids, args.research_id, args.query)
        
        # æ­¥éª¤3: æ ‡è®°æ–‡çŒ®
        step3_mark_literature(args.research_id)
        
        print(f"\n{'='*80}")
        print("âœ… å®Œæˆï¼")
        print(f"{'='*80}\n")
        return
    else:
        # ä»æ­¥éª¤1å¼€å§‹
        print("\nä»æ­¥éª¤1å¼€å§‹æ‰§è¡Œ")
    
    # æ­¥éª¤1: æ£€ç´¢æ–‡çŒ®
    pmc_ids = step1_search_literature(args.query, args.research_id, args.api_key, args.max_results)
    
    if not pmc_ids:
        print("\nâŒ æ²¡æœ‰æ£€ç´¢åˆ°æ–‡çŒ®ï¼Œé€€å‡º")
        return
    
    if args.search_only:
        print("\nåªæ‰§è¡Œæ£€ç´¢æ­¥éª¤ï¼Œè·³è¿‡åˆ†æå’Œæ ‡è®°")
        return
    
    # æ­¥éª¤2: åˆ†ææ–‡çŒ®
    step2_analyze_literature(pmc_ids, args.research_id, args.query)
    
    # æ­¥éª¤3: æ ‡è®°æ–‡çŒ®ï¼ˆä»analysis_resultsç›®å½•è¯»å–ç»“æœï¼‰
    step3_mark_literature(args.research_id)
    
    print(f"\n{'='*80}")
    print("âœ… å®Œæˆï¼")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢
    if len(sys.argv) == 1:
        print("ä½¿ç”¨ç¤ºä¾‹:")
        print("  python pubmed_literature_screen_assistant.py --query 'ADMET prediction tools'")
        print("  python pubmed_literature_screen_assistant.py --query 'machine learning' --max-results 100")
        print("  python pubmed_literature_screen_assistant.py --query 'deep learning' --search-only")
        print("\nå¼€å§‹è¿è¡Œé»˜è®¤æŸ¥è¯¢...\n")
        
        # é»˜è®¤æŸ¥è¯¢
        agent = create_literature_assistant()
        print(f"PMCæ–‡çŒ®åŠ©æ‰‹åˆ›å»ºæˆåŠŸ: {agent.name}")
        result = agent("ç”¨æˆ·æ–‡çŒ®æ”¶é›†åŠç­›é€‰éœ€æ±‚:ADMETï¼ˆå¸æ”¶ã€åˆ†å¸ƒã€ä»£è°¢ã€æ’æ³„å’Œæ¯’æ€§ï¼‰é¢„æµ‹æ˜¯æ–°è¯ç ”å‘ä¸­çš„å…³é”®ç¯èŠ‚ã€‚è°ƒç ”ä¸€ä¸‹å½“å‰å¤§å¤šæ•°state of the artçš„ADMETé¢„æµ‹å·¥å…·ï¼Œå†™ä¸€ä»½å…³äºADMETé¢„æµ‹å·¥å…·çš„è°ƒç ”æŠ¥å‘Šï¼ˆæ·±åº¦å­¦ä¹ çš„ï¼Œå¤§æ¨¡å‹çš„ï¼Œå¼ºç®—æ³•ç±»çš„ç­‰ç­‰ï¼‰,è¦æ±‚æŸ¥çœ‹è¿‘äº”å¹´çš„æ–‡çŒ®")
        print(f"æ™ºèƒ½ä½“å“åº”:\n{result}")
    else:
        main()