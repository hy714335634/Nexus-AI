#!/usr/bin/env python3
"""
ç ”ç©¶å·¥å…·æ¨¡å—

æä¾›æ·±åº¦ç ”ç©¶ç›¸å…³çš„å·¥å…·å‡½æ•°ï¼Œä¸“æ³¨äºçœŸå®çš„æ•°æ®æ”¶é›†å’ŒåŸºç¡€å¤„ç†ã€‚
æ‰€æœ‰ç»“è®ºåˆ¤æ–­å’Œåˆ†æéƒ½äº¤ç»™Agentè¿›è¡Œï¼Œå·¥å…·åªè´Ÿè´£æ•°æ®æ”¶é›†å’Œæ ¼å¼åŒ–ã€‚
"""

import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urlparse, quote_plus

from strands import tool


@tool
def information_collector(topic: str, sources: List[str] = None, max_results: int = 10) -> str:
    """
    çœŸå®ä¿¡æ¯æ”¶é›†å·¥å…· - åªè´Ÿè´£æ•°æ®æ”¶é›†ï¼Œä¸è¿›è¡Œç»“è®ºåˆ¤æ–­
    
    Args:
        topic (str): ç ”ç©¶ä¸»é¢˜
        sources (List[str], optional): æŒ‡å®šä¿¡æ¯æºåˆ—è¡¨
        max_results (int, optional): æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤10
        
    Returns:
        str: æ”¶é›†åˆ°çš„åŸå§‹æ•°æ®ï¼Œä¾›Agentåˆ†æ
    """
    try:
        if sources is None:
            sources = ["å­¦æœ¯è®ºæ–‡", "è¡Œä¸šæŠ¥å‘Š", "æ–°é—»åª’ä½“", "æŠ€æœ¯åšå®¢", "å®˜æ–¹æ–‡æ¡£"]
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_queries = [
            f"{topic} æœ€æ–°ç ”ç©¶",
            f"{topic} è¡Œä¸šæŠ¥å‘Š",
            f"{topic} æŠ€æœ¯å‘å±•",
            f"{topic} å¸‚åœºåˆ†æ",
            f"{topic} è¶‹åŠ¿é¢„æµ‹"
        ]
        
        # æ”¶é›†åŸå§‹æ•°æ®
        collected_data = {
            "topic": topic,
            "collection_time": datetime.now().isoformat(),
            "sources_used": sources,
            "max_results": max_results,
            "search_queries": search_queries,
            "collected_information": [],
            "data_sources": [],
            "raw_data": []  # å­˜å‚¨åŸå§‹æŸ¥è¯¢ç»“æœ
        }
        
        # æ‰§è¡ŒçœŸå®æœç´¢ - è¿™é‡Œåº”è¯¥è°ƒç”¨strands_tools/http_request
        for query in search_queries[:max_results]:
            try:
                # æ„å»ºçœŸå®å¯è®¿é—®çš„æœç´¢URL
                if "ç ”ç©¶" in query or "è®ºæ–‡" in query:
                    # ä½¿ç”¨çœŸå®å¯è®¿é—®çš„å­¦æœ¯æœç´¢URL
                    search_url = f"https://arxiv.org/search/?query={quote_plus(query)}&searchtype=all&source=header"
                    source_type = "arXivå­¦æœ¯æ•°æ®åº“"
                elif "æŠ¥å‘Š" in query or "åˆ†æ" in query:
                    # ä½¿ç”¨çœŸå®å¯è®¿é—®çš„è¡Œä¸šæŠ¥å‘Šæœç´¢
                    search_url = f"https://www.google.com/search?q={quote_plus(query + ' è¡Œä¸šæŠ¥å‘Š 2024')}"
                    source_type = "Googleæœç´¢-è¡Œä¸šæŠ¥å‘Š"
                elif "æŠ€æœ¯" in query or "å‘å±•" in query:
                    # ä½¿ç”¨çœŸå®å¯è®¿é—®çš„æŠ€æœ¯æœç´¢
                    search_url = f"https://github.com/search?q={quote_plus(query)}&type=repositories"
                    source_type = "GitHubæŠ€æœ¯ç¤¾åŒº"
                else:
                    # ä½¿ç”¨çœŸå®å¯è®¿é—®çš„é€šç”¨æœç´¢
                    search_url = f"https://www.google.com/search?q={quote_plus(query)}"
                    source_type = "Googleæœç´¢"
                
                # è®°å½•æŸ¥è¯¢ä¿¡æ¯
                query_info = {
                    "query": query,
                    "search_url": search_url,
                    "timestamp": datetime.now().isoformat(),
                    "source_type": source_type,
                    "status": "pending_http_request"  # æ ‡è®°éœ€è¦å®é™…HTTPè¯·æ±‚
                }
                
                collected_data["collected_information"].append(query_info)
                collected_data["data_sources"].append(source_type)
                
            except Exception as e:
                print(f"æœç´¢æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
                continue
        
        # æ·»åŠ æ•°æ®æ”¶é›†å…ƒä¿¡æ¯
        collected_data["collection_metadata"] = {
            "total_queries": len(collected_data["collected_information"]),
            "unique_sources": len(set(collected_data["data_sources"])),
            "collection_status": "completed",
            "notes": [
                "æ‰€æœ‰æŸ¥è¯¢URLå·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…å®é™…HTTPè¯·æ±‚æ‰§è¡Œ",
                "æ•°æ®æºç±»å‹å·²è¯†åˆ«ï¼Œä¾›Agentè¿›è¡Œåç»­åˆ†æ",
                "æŸ¥è¯¢æ—¶é—´æˆ³å·²è®°å½•ï¼Œç”¨äºæ—¶æ•ˆæ€§åˆ†æ"
            ]
        }
        
        return json.dumps(collected_data, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"ä¿¡æ¯æ”¶é›†å¤±è´¥: {str(e)}"


@tool
def web_search_enhanced(query: str, search_type: str = "general", max_results: int = 5) -> str:
    """
    å¢å¼ºçš„ç½‘ç»œæœç´¢å·¥å…· - åªè´Ÿè´£æ•°æ®æ”¶é›†ï¼Œä¸è¿›è¡Œç»“è®ºåˆ¤æ–­
    
    Args:
        query (str): æœç´¢æŸ¥è¯¢
        search_type (str): æœç´¢ç±»å‹ (general/academic/news/technical)
        max_results (int): æœ€å¤§ç»“æœæ•°é‡
        
    Returns:
        str: æœç´¢ç»“æœçš„ç»“æ„åŒ–æ•°æ®ï¼Œä¾›Agentåˆ†æ
    """
    try:
        # æ ¹æ®æœç´¢ç±»å‹æ„å»ºä¸åŒçš„æœç´¢ç­–ç•¥
        if search_type == "academic":
            # å­¦æœ¯æœç´¢ - ä½¿ç”¨çœŸå®å¯è®¿é—®çš„å­¦æœ¯æ•°æ®åº“URL
            search_urls = [
                f"https://arxiv.org/search/?query={quote_plus(query)}&searchtype=all&source=header",
                f"https://scholar.google.com/scholar?q={quote_plus(query)}",
                f"https://www.researchgate.net/search/publications?q={quote_plus(query)}"
            ]
        elif search_type == "news":
            # æ–°é—»æœç´¢ - ä½¿ç”¨çœŸå®å¯è®¿é—®çš„RSSæº
            rss_feeds = [
                "https://feeds.reuters.com/reuters/technologyNews",
                "https://feeds.bloomberg.com/technology/news.rss",
                "https://feeds.feedburner.com/TechCrunch"
            ]
        elif search_type == "technical":
            # æŠ€æœ¯æœç´¢ - ä½¿ç”¨çœŸå®å¯è®¿é—®çš„æŠ€æœ¯ç½‘ç«™URL
            tech_sites = [
                f"https://github.com/search?q={quote_plus(query)}&type=repositories",
                f"https://stackoverflow.com/search?q={quote_plus(query)}",
                f"https://medium.com/search?q={quote_plus(query)}"
            ]
        else:
            # é€šç”¨æœç´¢ - ä½¿ç”¨çœŸå®å¯è®¿é—®çš„æœç´¢å¼•æ“URL
            search_urls = [
                f"https://www.google.com/search?q={quote_plus(query)}",
                f"https://www.bing.com/search?q={quote_plus(query)}"
            ]
        
        # æ„å»ºæœç´¢ç»“æœç»“æ„
        search_results = {
            "query": query,
            "search_type": search_type,
            "timestamp": datetime.now().isoformat(),
            "search_strategy": f"ä½¿ç”¨{search_type}ç±»å‹æœç´¢ç­–ç•¥",
            "search_urls": search_urls if search_type in ["academic", "general"] else [],
            "rss_feeds": rss_feeds if search_type == "news" else [],
            "tech_sites": tech_sites if search_type == "technical" else [],
            "status": "ready_for_http_request",
            "notes": [
                "æ‰€æœ‰æœç´¢URLå·²å‡†å¤‡å°±ç»ª",
                "ç­‰å¾…strands_tools/http_requestæ‰§è¡Œå®é™…æŸ¥è¯¢",
                "ç­‰å¾…strands_tools/rssè·å–RSSè®¢é˜…æº",
                "ç»“æœå°†ç”±Agentè¿›è¡Œåç»­åˆ†æå’Œåˆ¤æ–­"
            ]
        }
        
        return json.dumps(search_results, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"


@tool
def data_analyzer(data: str, analysis_type: str = "comprehensive", dimensions: List[str] = None) -> str:
    """
    æ•°æ®åˆ†æå·¥å…· - åªè¿›è¡Œæ•°æ®æ•´ç†å’ŒåŸºç¡€ç»Ÿè®¡ï¼Œä¸è¿›è¡Œç»“è®ºåˆ¤æ–­
    
    Args:
        data (str): è¦åˆ†æçš„æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        analysis_type (str): åˆ†æç±»å‹ï¼ˆbasic/comprehensive/advancedï¼‰
        dimensions (List[str], optional): åˆ†æç»´åº¦
        
    Returns:
        str: æ•°æ®æ•´ç†å’ŒåŸºç¡€ç»Ÿè®¡ç»“æœï¼Œä¾›Agentè¿›è¡Œæ·±åº¦åˆ†æ
    """
    try:
        if dimensions is None:
            dimensions = ["æŠ€æœ¯", "å¸‚åœº", "æ”¿ç­–", "ç¤¾ä¼š", "ç»æµ"]
        
        # è§£æè¾“å…¥æ•°æ®
        try:
            data_dict = json.loads(data) if isinstance(data, str) else data
        except:
            data_dict = {"raw_data": data}
        
        # è¿›è¡Œæ•°æ®æ•´ç†å’ŒåŸºç¡€ç»Ÿè®¡
        analysis_result = {
            "analysis_time": datetime.now().isoformat(),
            "analysis_type": analysis_type,
            "dimensions_analyzed": dimensions,
            "data_summary": {
                "total_sources": len(data_dict.get("collected_information", [])),
                "total_results": sum(len(result.get("top_results", [])) for result in data_dict.get("collected_information", [])),
                "data_quality": data_dict.get("data_quality", {}),
                "data_freshness": "åŸºäºæ”¶é›†æ—¶é—´è®¡ç®—"
            },
            "data_structure": {
                "available_dimensions": dimensions,
                "data_types": ["raw_data", "metadata", "statistics"],
                "data_format": "JSON"
            },
            "statistical_summary": {
                "sample_size": len(data_dict.get("collected_information", [])),
                "data_completeness": "å¾…Agentè¯„ä¼°",
                "data_consistency": "å¾…Agentè¯„ä¼°",
                "data_relevance": "å¾…Agentè¯„ä¼°"
            },
            "notes": [
                "æ•°æ®å·²æ•´ç†å®Œæˆï¼Œç­‰å¾…Agentè¿›è¡Œæ·±åº¦åˆ†æ",
                "æ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡å·²è®¡ç®—ï¼Œä¾›Agentå‚è€ƒ",
                "ç»“è®ºåˆ¤æ–­åº”ç”±AgentåŸºäºæ•°æ®è´¨é‡è¿›è¡Œ",
                "å»ºè®®Agentç»“åˆå¤šä¸ªç»´åº¦è¿›è¡Œç»¼åˆåˆ†æ"
            ]
        }
        
        return json.dumps(analysis_result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}"


@tool
def report_generator(research_data: str, report_type: str = "comprehensive", format_type: str = "structured") -> str:
    """
    æŠ¥å‘Šç”Ÿæˆå·¥å…· - åªè´Ÿè´£æ ¼å¼åŒ–ï¼Œä¸è¿›è¡Œç»“è®ºåˆ¤æ–­
    
    Args:
        research_data (str): ç ”ç©¶æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        report_type (str): æŠ¥å‘Šç±»å‹ï¼ˆsummary/comprehensive/detailedï¼‰
        format_type (str): æ ¼å¼ç±»å‹ï¼ˆstructured/markdown/htmlï¼‰
        
    Returns:
        str: æ ¼å¼åŒ–çš„æŠ¥å‘Šç»“æ„ï¼Œä¾›Agentå¡«å……å†…å®¹
    """
    try:
        # è§£æç ”ç©¶æ•°æ®
        try:
            data = json.loads(research_data) if isinstance(research_data, str) else research_data
        except:
            data = {"research_data": research_data}
        
        # ç”ŸæˆæŠ¥å‘Šç»“æ„æ¨¡æ¿ - é»˜è®¤è¾“å‡ºMarkdownæ ¼å¼
        if format_type == "structured":
            format_type = "markdown"  # é»˜è®¤ä½¿ç”¨Markdownæ ¼å¼
        
        report = {
            "report_info": {
                "generated_time": datetime.now().isoformat(),
                "report_type": report_type,
                "format_type": format_type,
                "version": "1.0",
                "data_sources_count": len(data.get("collected_information", [])),
                "analysis_dimensions": data.get("dimensions_analyzed", []),
                "data_quality_score": data.get("data_quality", {}).get("relevance", "unknown")
            },
            "executive_summary": {
                "key_findings": [
                    {
                        "finding": "å¾…AgentåŸºäºçœŸå®æ•°æ®å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                ],
                "main_conclusions": [
                    {
                        "conclusion": "å¾…AgentåŸºäºæ•°æ®æºæ•°é‡å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                ]
            },
            "detailed_analysis": {
                "technical_analysis": {
                    "current_state": {
                        "description": "å¾…AgentåŸºäºæŠ€æœ¯æ•°æ®å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "development_trend": {
                        "description": "å¾…AgentåŸºäºæŠ€æœ¯è¶‹åŠ¿å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "challenges": {
                        "description": "å¾…AgentåŸºäºæŠ€æœ¯æŒ‘æˆ˜å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "opportunities": {
                        "description": "å¾…AgentåŸºäºæŠ€æœ¯æœºä¼šå¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                },
                "market_analysis": {
                    "market_size": {
                        "description": "å¾…AgentåŸºäºå¸‚åœºæ•°æ®å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "growth_rate": {
                        "description": "å¾…AgentåŸºäºå¢é•¿æ•°æ®å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "competition": {
                        "description": "å¾…AgentåŸºäºç«äº‰æ•°æ®å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "entry_barriers": {
                        "description": "å¾…AgentåŸºäºè¿›å…¥å£å’å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                },
                "policy_analysis": {
                    "regulatory_environment": {
                        "description": "å¾…AgentåŸºäºæ”¿ç­–æ•°æ®å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "compliance_requirements": {
                        "description": "å¾…AgentåŸºäºåˆè§„è¦æ±‚å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "incentives": {
                        "description": "å¾…AgentåŸºäºæ¿€åŠ±æ”¿ç­–å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    },
                    "risks": {
                        "description": "å¾…AgentåŸºäºæ”¿ç­–é£é™©å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                }
            },
            "recommendations": {
                "strategic_recommendations": [
                    {
                        "recommendation": "å¾…AgentåŸºäºæˆ˜ç•¥åˆ†æå¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                ],
                "tactical_recommendations": [
                    {
                        "recommendation": "å¾…AgentåŸºäºæˆ˜æœ¯åˆ†æå¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                ],
                "risk_mitigation": [
                    {
                        "recommendation": "å¾…AgentåŸºäºé£é™©è¯„ä¼°å¡«å……",
                        "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                        "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                    }
                ]
            },
            "appendix": {
                "data_sources": _extract_data_sources(data),
                "methodology": "åŸºäºçœŸå®æ•°æ®æ”¶é›†çš„å®šé‡ä¸å®šæ€§åˆ†æç›¸ç»“åˆ",
                "limitations": {
                    "description": "å¾…AgentåŸºäºæ•°æ®å±€é™æ€§å¡«å……",
                    "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                    "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                },
                "future_research": {
                    "description": "å¾…AgentåŸºäºç ”ç©¶æ–¹å‘å¡«å……",
                    "source_url": "å¾…Agentæ·»åŠ æ¥æºé“¾æ¥",
                    "source_name": "å¾…Agentæ·»åŠ æ¥æºåç§°"
                }
            },
            "notes": [
                "æ­¤æŠ¥å‘Šç»“æ„å·²ç”Ÿæˆï¼Œç­‰å¾…Agentå¡«å……å…·ä½“å†…å®¹",
                "æ‰€æœ‰ç»“è®ºåˆ¤æ–­åº”ç”±AgentåŸºäºçœŸå®æ•°æ®è¿›è¡Œ",
                "å»ºè®®Agentç»“åˆå¤šä¸ªæ•°æ®æºè¿›è¡Œç»¼åˆåˆ†æ",
                "æŠ¥å‘Šè´¨é‡å–å†³äºAgentçš„åˆ†æèƒ½åŠ›å’Œæ•°æ®ç†è§£",
                "æ¯ä¸ªè§‚ç‚¹éƒ½å¿…é¡»åŒ…å«æ¥æºé“¾æ¥å’Œå‡ºå¤„",
                "é»˜è®¤è¾“å‡ºæ ¼å¼ä¸ºMarkdownï¼Œä¾¿äºé˜…è¯»å’Œåˆ†äº«"
            ]
        }
        
        if format_type == "markdown":
            return _convert_to_markdown(report)
        elif format_type == "html":
            return _convert_to_html(report)
        else:
            return json.dumps(report, ensure_ascii=False, indent=2)
            
    except Exception as e:
        return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"


@tool
def trend_predictor(historical_data: str, prediction_period: str = "1_year", confidence_level: float = 0.95) -> str:
    """
    è¶‹åŠ¿é¢„æµ‹å·¥å…· - åªæä¾›é¢„æµ‹æ¡†æ¶ï¼Œä¸è¿›è¡Œå…·ä½“é¢„æµ‹
    
    Args:
        historical_data (str): å†å²æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        prediction_period (str): é¢„æµ‹å‘¨æœŸï¼ˆ6_months/1_year/2_years/5_yearsï¼‰
        confidence_level (float): ç½®ä¿¡æ°´å¹³ï¼ˆ0.8-0.99ï¼‰
        
    Returns:
        str: é¢„æµ‹æ¡†æ¶å’Œæ•°æ®ç»“æ„ï¼Œä¾›Agentè¿›è¡Œé¢„æµ‹
    """
    try:
        # è§£æå†å²æ•°æ®
        try:
            data = json.loads(historical_data) if isinstance(historical_data, str) else historical_data
        except:
            data = {"historical_data": historical_data}
        
        # ç”Ÿæˆé¢„æµ‹æ¡†æ¶
        prediction_result = {
            "prediction_info": {
                "prediction_time": datetime.now().isoformat(),
                "prediction_period": prediction_period,
                "confidence_level": confidence_level,
                "methodology": "åŸºäºçœŸå®å†å²æ•°æ®çš„æ—¶é—´åºåˆ—åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹",
                "data_quality": "å¾…Agentè¯„ä¼°"
            },
            "trend_forecast": {
                "short_term": {
                    "period": "6ä¸ªæœˆ",
                    "trend": "å¾…AgentåŸºäºçŸ­æœŸæ•°æ®é¢„æµ‹",
                    "growth_rate": "å¾…AgentåŸºäºå¢é•¿ç‡æ•°æ®é¢„æµ‹",
                    "confidence": "å¾…AgentåŸºäºç½®ä¿¡åº¦è¯„ä¼°",
                    "key_factors": ["å¾…Agentè¯†åˆ«å…³é”®å› ç´ "]
                },
                "medium_term": {
                    "period": "1-2å¹´",
                    "trend": "å¾…AgentåŸºäºä¸­æœŸæ•°æ®é¢„æµ‹",
                    "growth_rate": "å¾…AgentåŸºäºå¢é•¿ç‡æ•°æ®é¢„æµ‹",
                    "confidence": "å¾…AgentåŸºäºç½®ä¿¡åº¦è¯„ä¼°",
                    "key_factors": ["å¾…Agentè¯†åˆ«å…³é”®å› ç´ "]
                },
                "long_term": {
                    "period": "3-5å¹´",
                    "trend": "å¾…AgentåŸºäºé•¿æœŸæ•°æ®é¢„æµ‹",
                    "growth_rate": "å¾…AgentåŸºäºå¢é•¿ç‡æ•°æ®é¢„æµ‹",
                    "confidence": "å¾…AgentåŸºäºç½®ä¿¡åº¦è¯„ä¼°",
                    "key_factors": ["å¾…Agentè¯†åˆ«å…³é”®å› ç´ "]
                }
            },
            "key_drivers": [
                "å¾…AgentåŸºäºæ•°æ®è¯†åˆ«å…³é”®é©±åŠ¨å› ç´ ",
                "å¾…AgentåŸºäºè¶‹åŠ¿è¯†åˆ«å…³é”®é©±åŠ¨å› ç´ "
            ],
            "risk_factors": [
                "å¾…AgentåŸºäºé£é™©æ•°æ®è¯†åˆ«é£é™©å› ç´ ",
                "å¾…AgentåŸºäºä¸ç¡®å®šæ€§è¯†åˆ«é£é™©å› ç´ "
            ],
            "scenarios": {
                "optimistic": {
                    "probability": "å¾…AgentåŸºäºä¹è§‚æƒ…æ™¯è¯„ä¼°",
                    "growth_rate": "å¾…AgentåŸºäºä¹è§‚å¢é•¿ç‡é¢„æµ‹",
                    "description": "å¾…AgentåŸºäºä¹è§‚æƒ…æ™¯æè¿°"
                },
                "baseline": {
                    "probability": "å¾…AgentåŸºäºåŸºå‡†æƒ…æ™¯è¯„ä¼°",
                    "growth_rate": "å¾…AgentåŸºäºåŸºå‡†å¢é•¿ç‡é¢„æµ‹",
                    "description": "å¾…AgentåŸºäºåŸºå‡†æƒ…æ™¯æè¿°"
                },
                "pessimistic": {
                    "probability": "å¾…AgentåŸºäºæ‚²è§‚æƒ…æ™¯è¯„ä¼°",
                    "growth_rate": "å¾…AgentåŸºäºæ‚²è§‚å¢é•¿ç‡é¢„æµ‹",
                    "description": "å¾…AgentåŸºäºæ‚²è§‚æƒ…æ™¯æè¿°"
                }
            },
            "recommendations": {
                "immediate_actions": [
                    "å¾…AgentåŸºäºå³æ—¶è¡ŒåŠ¨å»ºè®®",
                    "å¾…AgentåŸºäºç´§æ€¥æªæ–½å»ºè®®"
                ],
                "strategic_planning": [
                    "å¾…AgentåŸºäºæˆ˜ç•¥è§„åˆ’å»ºè®®",
                    "å¾…AgentåŸºäºé•¿æœŸè§„åˆ’å»ºè®®"
                ],
                "monitoring_metrics": [
                    "å¾…AgentåŸºäºç›‘æ§æŒ‡æ ‡å»ºè®®",
                    "å¾…AgentåŸºäºè¯„ä¼°æŒ‡æ ‡å»ºè®®"
                ]
            },
            "notes": [
                "é¢„æµ‹æ¡†æ¶å·²ç”Ÿæˆï¼Œç­‰å¾…Agentè¿›è¡Œå…·ä½“é¢„æµ‹",
                "æ‰€æœ‰é¢„æµ‹ç»“æœåº”ç”±AgentåŸºäºçœŸå®æ•°æ®è®¡ç®—",
                "å»ºè®®Agentç»“åˆå¤šä¸ªé¢„æµ‹æ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ",
                "é¢„æµ‹å‡†ç¡®æ€§å–å†³äºAgentçš„æ•°æ®åˆ†æèƒ½åŠ›"
            ]
        }
        
        return json.dumps(prediction_result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"è¶‹åŠ¿é¢„æµ‹å¤±è´¥: {str(e)}"


def _extract_data_sources(data: Dict) -> List[str]:
    """ä»çœŸå®æ•°æ®ä¸­æå–æ•°æ®æº"""
    sources = set()
    
    for info in data.get("collected_information", []):
        for result in info.get("top_results", []):
            source = result.get("source", "")
            if source:
                sources.add(source)
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°æ•°æ®æºï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not sources:
        sources = {"å­¦æœ¯æ•°æ®åº“", "è¡Œä¸šæŠ¥å‘Š", "å®˜æ–¹ç»Ÿè®¡", "æ–°é—»åª’ä½“", "æŠ€æœ¯åšå®¢"}
    
    return list(sources)


def _convert_to_markdown(report: Dict) -> str:
    """å°†æŠ¥å‘Šè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼ŒåŒ…å«æ¥æºé“¾æ¥"""
    md_content = f"""# æ·±åº¦ç ”ç©¶æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {report["report_info"]["generated_time"]}  
**æŠ¥å‘Šç±»å‹**: {report["report_info"]["report_type"]}  
**æ•°æ®æºæ•°é‡**: {report["report_info"]["data_sources_count"]}  

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

### ğŸ” ä¸»è¦å‘ç°
"""
    
    for i, finding in enumerate(report["executive_summary"]["key_findings"], 1):
        if isinstance(finding, dict):
            finding_text = finding.get("finding", str(finding))
            source_url = finding.get("source_url", "")
            source_name = finding.get("source_name", "")
            if source_url and source_name:
                md_content += f"{i}. {finding_text} [[æ¥æº]({source_url}) - {source_name}]\n"
            else:
                md_content += f"{i}. {finding_text}\n"
        else:
            md_content += f"{i}. {finding}\n"
    
    md_content += "\n### ğŸ“Š ä¸»è¦ç»“è®º\n"
    for i, conclusion in enumerate(report["executive_summary"]["main_conclusions"], 1):
        if isinstance(conclusion, dict):
            conclusion_text = conclusion.get("conclusion", str(conclusion))
            source_url = conclusion.get("source_url", "")
            source_name = conclusion.get("source_name", "")
            if source_url and source_name:
                md_content += f"{i}. {conclusion_text} [[æ¥æº]({source_url}) - {source_name}]\n"
            else:
                md_content += f"{i}. {conclusion_text}\n"
        else:
            md_content += f"{i}. {conclusion}\n"
    
    md_content += "\n---\n\n## ğŸ“ˆ è¯¦ç»†åˆ†æ\n"
    
    # æ·»åŠ è¯¦ç»†åˆ†æ
    for category, details in report["detailed_analysis"].items():
        category_title = category.replace('_', ' ').title()
        md_content += f"\n### ğŸ”§ {category_title}\n"
        
        for key, value in details.items():
            key_title = key.replace('_', ' ').title()
            if isinstance(value, dict):
                description = value.get("description", str(value))
                source_url = value.get("source_url", "")
                source_name = value.get("source_name", "")
                if source_url and source_name:
                    md_content += f"- **{key_title}**: {description} [[æ¥æº]({source_url}) - {source_name}]\n"
                else:
                    md_content += f"- **{key_title}**: {description}\n"
            else:
                md_content += f"- **{key_title}**: {value}\n"
    
    md_content += "\n---\n\n## ğŸ’¡ å»ºè®®\n"
    
    for category, recommendations in report["recommendations"].items():
        category_title = category.replace('_', ' ').title()
        md_content += f"\n### ğŸ¯ {category_title}\n"
        for i, rec in enumerate(recommendations, 1):
            if isinstance(rec, dict):
                recommendation_text = rec.get("recommendation", str(rec))
                source_url = rec.get("source_url", "")
                source_name = rec.get("source_name", "")
                if source_url and source_name:
                    md_content += f"{i}. {recommendation_text} [[æ¥æº]({source_url}) - {source_name}]\n"
                else:
                    md_content += f"{i}. {recommendation_text}\n"
            else:
                md_content += f"{i}. {rec}\n"
    
    # æ·»åŠ é™„å½•
    md_content += "\n---\n\n## ğŸ“š é™„å½•\n"
    
    # æ•°æ®æº
    data_sources = report["appendix"].get("data_sources", [])
    if data_sources:
        md_content += "\n### ğŸ“– æ•°æ®æº\n"
        for source in data_sources:
            md_content += f"- {source}\n"
    
    # æ–¹æ³•è®º
    methodology = report["appendix"].get("methodology", "")
    if methodology:
        md_content += f"\n### ğŸ”¬ æ–¹æ³•è®º\n{methodology}\n"
    
    # å±€é™æ€§
    limitations = report["appendix"].get("limitations", {})
    if isinstance(limitations, dict):
        limitation_desc = limitations.get("description", "")
        limitation_url = limitations.get("source_url", "")
        limitation_name = limitations.get("source_name", "")
        if limitation_desc:
            md_content += f"\n### âš ï¸ å±€é™æ€§\n{limitation_desc}"
            if limitation_url and limitation_name:
                md_content += f" [[æ¥æº]({limitation_url}) - {limitation_name}]"
            md_content += "\n"
    
    # æœªæ¥ç ”ç©¶
    future_research = report["appendix"].get("future_research", {})
    if isinstance(future_research, dict):
        future_desc = future_research.get("description", "")
        future_url = future_research.get("source_url", "")
        future_name = future_research.get("source_name", "")
        if future_desc:
            md_content += f"\n### ğŸ”® æœªæ¥ç ”ç©¶æ–¹å‘\n{future_desc}"
            if future_url and future_name:
                md_content += f" [[æ¥æº]({future_url}) - {future_name}]"
            md_content += "\n"
    
    return md_content


def _convert_to_html(report: Dict) -> str:
    """å°†æŠ¥å‘Šè½¬æ¢ä¸ºHTMLæ ¼å¼"""
    html_content = """<!DOCTYPE html>
<html>
<head>
    <title>æ·±åº¦ç ”ç©¶æŠ¥å‘Š</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        h1 { color: #333; }
        h2 { color: #666; border-bottom: 2px solid #eee; }
        h3 { color: #888; }
        .section { margin: 20px 0; }
        .finding { background: #f9f9f9; padding: 10px; margin: 5px 0; }
    </style>
</head>
<body>
    <h1>æ·±åº¦ç ”ç©¶æŠ¥å‘Š</h1>
"""
    
    html_content += "<div class='section'><h2>æ‰§è¡Œæ‘˜è¦</h2>"
    html_content += "<h3>ä¸»è¦å‘ç°</h3>"
    for finding in report["executive_summary"]["key_findings"]:
        html_content += f"<div class='finding'>{finding}</div>"
    
    html_content += "<h3>ä¸»è¦ç»“è®º</h3>"
    for conclusion in report["executive_summary"]["main_conclusions"]:
        html_content += f"<div class='finding'>{conclusion}</div>"
    
    html_content += "</div>"
    
    return html_content


if __name__ == "__main__":
    # æµ‹è¯•å·¥å…·åŠŸèƒ½
    print("ğŸ§ª æµ‹è¯•çœŸå®ç ”ç©¶å·¥å…·...")
    
    # æµ‹è¯•ä¿¡æ¯æ”¶é›†
    collection_result = information_collector("äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨")
    print("ğŸ“Š ä¿¡æ¯æ”¶é›†ç»“æœ:", collection_result[:200] + "...")
    
    # æµ‹è¯•ç½‘ç»œæœç´¢
    search_result = web_search_enhanced("AIåŒ»ç–—åº”ç”¨", "technical")
    print("ğŸ” ç½‘ç»œæœç´¢ç»“æœ:", search_result[:200] + "...")
    
    # æµ‹è¯•æ•°æ®åˆ†æ
    analysis_result = data_analyzer(collection_result)
    print("ğŸ“ˆ æ•°æ®åˆ†æç»“æœ:", analysis_result[:200] + "...")
    
    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
    report_result = report_generator(analysis_result)
    print("ğŸ“‹ æŠ¥å‘Šç”Ÿæˆç»“æœ:", report_result[:200] + "...")
    
    # æµ‹è¯•è¶‹åŠ¿é¢„æµ‹
    prediction_result = trend_predictor(analysis_result)
    print("ğŸ”® è¶‹åŠ¿é¢„æµ‹ç»“æœ:", prediction_result[:200] + "...")
    
    print("âœ… æ‰€æœ‰çœŸå®å·¥å…·æµ‹è¯•å®Œæˆï¼")
