#!/usr/bin/env python3
"""
Web scraper tool using Playwright for dynamic web page crawling.
Supports JavaScript-rendered pages and anti-scraping measures.
"""

import json
import asyncio
import re
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse

from strands import tool

try:
    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False


@tool
def scrape_webpage(
    url: str,
    wait_for_selector: Optional[str] = None,
    scroll_to_bottom: bool = False,
    timeout: int = 30000,
    use_playwright: bool = True
) -> str:
    """
    æŠ“å–ç½‘é¡µå†…å®¹ï¼Œæ”¯æŒåŠ¨æ€JavaScriptæ¸²æŸ“
    
    Args:
        url (str): ç›®æ ‡ç½‘é¡µURL
        wait_for_selector (str, optional): ç­‰å¾…ç‰¹å®šCSSé€‰æ‹©å™¨å‡ºç°
        scroll_to_bottom (bool): æ˜¯å¦æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ï¼ˆåŠ è½½æ‡’åŠ è½½å†…å®¹ï¼‰
        timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        use_playwright (bool): æ˜¯å¦ä½¿ç”¨Playwrightï¼ˆæ”¯æŒåŠ¨æ€ç½‘é¡µï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„æŠ“å–ç»“æœ
    """
    try:
        if use_playwright and PLAYWRIGHT_AVAILABLE:
            # ä½¿ç”¨å¼‚æ­¥Playwright
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(_scrape_with_playwright(
                url, wait_for_selector, scroll_to_bottom, timeout
            ))
            loop.close()
            return result
        elif REQUESTS_AVAILABLE:
            # é™çº§åˆ°requests + BeautifulSoup
            return _scrape_with_requests(url, timeout // 1000)
        else:
            return json.dumps({
                "status": "error",
                "message": "ç½‘é¡µæŠ“å–åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install playwright requests beautifulsoup4"
            }, ensure_ascii=False)
            
    except Exception as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"æŠ“å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)


async def _scrape_with_playwright(
    url: str,
    wait_for_selector: Optional[str],
    scroll_to_bottom: bool,
    timeout: int
) -> str:
    """ä½¿ç”¨PlaywrightæŠ“å–åŠ¨æ€ç½‘é¡µ"""
    try:
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨ï¼ˆä½¿ç”¨å¯è§†åŒ–æ¨¡å¼è¿›è¡Œè°ƒè¯•ï¼‰
            # é€‰é¡¹1: ä½¿ç”¨ Playwright çš„ Chromium
            # browser = await p.chromium.launch(
            #     headless=False,      # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            #     slow_mo=500          # æ¯æ­¥æ“ä½œå»¶è¿Ÿ500msï¼Œä¾¿äºè§‚å¯Ÿ
            # )
            # é€‰é¡¹2: ä½¿ç”¨æœ¬åœ° Chromeï¼ˆå–æ¶ˆä¸‹é¢æ³¨é‡Šï¼‰
            browser = await p.chromium.launch(
                headless=False,
                slow_mo=500,
                channel="chrome"  # ä½¿ç”¨æœ¬åœ° Chrome
            )
            # é€‰é¡¹3: ä½¿ç”¨ Firefoxï¼ˆå–æ¶ˆä¸‹é¢æ³¨é‡Šï¼‰
            # browser = await p.firefox.launch(
            #     headless=False,
            #     slow_mo=500
            # )
            
            # åˆ›å»ºæ–°é¡µé¢
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                viewport={"width": 1920, "height": 1080}
            )
            page = await context.new_page()
            
            # è®¾ç½®è¶…æ—¶
            page.set_default_timeout(timeout)
            
            # è®¿é—®é¡µé¢
            await page.goto(url, wait_until="domcontentloaded")
            
            # ç­‰å¾…ç‰¹å®šé€‰æ‹©å™¨
            if wait_for_selector:
                try:
                    await page.wait_for_selector(wait_for_selector, timeout=timeout)
                except PlaywrightTimeout:
                    pass  # ç»§ç»­æ‰§è¡Œï¼Œå³ä½¿é€‰æ‹©å™¨æœªå‡ºç°
            
            # æ»šåŠ¨åˆ°åº•éƒ¨ï¼ˆåŠ è½½æ‡’åŠ è½½å†…å®¹ï¼‰
            if scroll_to_bottom:
                await page.evaluate("""
                    async () => {
                        await new Promise((resolve) => {
                            let totalHeight = 0;
                            const distance = 100;
                            const timer = setInterval(() => {
                                const scrollHeight = document.body.scrollHeight;
                                window.scrollBy(0, distance);
                                totalHeight += distance;
                                if (totalHeight >= scrollHeight) {
                                    clearInterval(timer);
                                    resolve();
                                }
                            }, 100);
                        });
                    }
                """)
                await page.wait_for_timeout(1000)
            
            # æå–é¡µé¢å†…å®¹
            title = await page.title()
            html_content = await page.content()
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'lxml')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
                script.decompose()
            
            # æå–ä¸»è¦å†…å®¹
            main_content = _extract_main_content(soup)
            
            # æå–é“¾æ¥
            links = _extract_links(soup, url)
            
            # æå–æ—¥æœŸ
            publish_date = _extract_date(soup, html_content)
            
            # å…³é—­æµè§ˆå™¨
            await browser.close()
            
            result = {
                "status": "success",
                "url": url,
                "title": title,
                "content": main_content,
                "content_length": len(main_content),
                "links": links,
                "publish_date": publish_date,
                "extraction_time": datetime.now().isoformat(),
                "source_domain": urlparse(url).netloc,
                "method": "playwright"
            }
            
            return json.dumps(result, ensure_ascii=False, indent=2)
            
    except PlaywrightTimeout:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆ{timeout}msï¼‰"
        }, ensure_ascii=False)
    except Exception as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"PlaywrightæŠ“å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)


def _scrape_with_requests(url: str, timeout: int) -> str:
    """ä½¿ç”¨requests + BeautifulSoupæŠ“å–é™æ€ç½‘é¡µ"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        
        soup = BeautifulSoup(response.content, 'lxml')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script.decompose()
        
        # æå–ä¸»è¦å†…å®¹
        main_content = _extract_main_content(soup)
        
        # æå–é“¾æ¥
        links = _extract_links(soup, url)
        
        # æå–æ—¥æœŸ
        publish_date = _extract_date(soup, response.text)
        
        # æå–æ ‡é¢˜
        title = soup.title.string if soup.title else ""
        
        result = {
            "status": "success",
            "url": url,
            "title": title,
            "content": main_content,
            "content_length": len(main_content),
            "links": links,
            "publish_date": publish_date,
            "extraction_time": datetime.now().isoformat(),
            "source_domain": urlparse(url).netloc,
            "method": "requests"
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except requests.Timeout:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰"
        }, ensure_ascii=False)
    except requests.RequestException as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)
    except Exception as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"å†…å®¹æå–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)


def _extract_main_content(soup: BeautifulSoup) -> str:
    """æå–ç½‘é¡µä¸»è¦å†…å®¹"""
    # å°è¯•æå–articleã€mainã€contentç­‰ä¸»è¦å†…å®¹åŒºåŸŸ
    main_content = None
    for selector in ['article', 'main', '[role="main"]', '.content', '#content', '.main-content']:
        main_content = soup.select_one(selector)
        if main_content:
            break
    
    if not main_content:
        main_content = soup.body if soup.body else soup
    
    # æå–æ–‡æœ¬
    text = main_content.get_text(separator="\n", strip=True)
    
    # æ¸…ç†æ–‡æœ¬
    lines = []
    for line in text.split("\n"):
        line = line.strip()
        if line and len(line) > 10:  # è¿‡æ»¤çŸ­è¡Œ
            lines.append(line)
    
    return "\n".join(lines)


def _extract_links(soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
    """æå–ç½‘é¡µä¸­çš„é“¾æ¥"""
    links = []
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        text = a_tag.get_text(strip=True)
        
        # è½¬æ¢ç›¸å¯¹é“¾æ¥ä¸ºç»å¯¹é“¾æ¥
        absolute_url = urljoin(base_url, href)
        
        # è¿‡æ»¤æ— æ•ˆé“¾æ¥
        if absolute_url.startswith(('http://', 'https://')) and text:
            links.append({
                "url": absolute_url,
                "text": text
            })
    
    return links[:50]  # é™åˆ¶é“¾æ¥æ•°é‡


def _extract_date(soup: BeautifulSoup, html_content: str) -> Optional[str]:
    """æå–æ–‡ç« å‘å¸ƒæ—¥æœŸ"""
    # å°è¯•å¤šç§æ—¥æœŸæå–ç­–ç•¥
    date_patterns = [
        r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?',
        r'\d{4}\.\d{1,2}\.\d{1,2}',
        r'\d{1,2}/\d{1,2}/\d{4}'
    ]
    
    # ä»metaæ ‡ç­¾æå–
    for meta_tag in ['article:published_time', 'datePublished', 'publishdate']:
        meta = soup.find('meta', attrs={'property': meta_tag}) or soup.find('meta', attrs={'name': meta_tag})
        if meta and meta.get('content'):
            return meta['content']
    
    # ä»timeæ ‡ç­¾æå–
    time_tag = soup.find('time')
    if time_tag and time_tag.get('datetime'):
        return time_tag['datetime']
    
    # ä»æ–‡æœ¬ä¸­æå–
    for pattern in date_patterns:
        match = re.search(pattern, html_content)
        if match:
            return match.group(0)
    
    return None


@tool
def batch_scrape_webpages(
    urls: List[str],
    wait_for_selector: Optional[str] = None,
    max_concurrent: int = 3,
    timeout: int = 30000,
    use_playwright: bool = True
) -> str:
    """
    æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µå†…å®¹
    
    Args:
        urls (List[str]): ç½‘é¡µURLåˆ—è¡¨
        wait_for_selector (str, optional): ç­‰å¾…ç‰¹å®šCSSé€‰æ‹©å™¨å‡ºç°
        max_concurrent (int): æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼Œé¿å…èµ„æºæ¶ˆè€—è¿‡å¤§ï¼‰
        timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        use_playwright (bool): æ˜¯å¦ä½¿ç”¨Playwright
        
    Returns:
        str: JSONæ ¼å¼çš„æ‰¹é‡æŠ“å–ç»“æœ
    """
    try:
        results = []
        successful = 0
        failed = 0
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å¹¶å‘è¿‡å¤š
        for i in range(0, len(urls), max_concurrent):
            batch_urls = urls[i:i+max_concurrent]
            
            for url in batch_urls:
                result_json = scrape_webpage(
                    url,
                    wait_for_selector=wait_for_selector,
                    scroll_to_bottom=False,
                    timeout=timeout,
                    use_playwright=use_playwright
                )
                result = json.loads(result_json)
                
                if result["status"] == "success":
                    successful += 1
                else:
                    failed += 1
                
                results.append(result)
                
                # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                time.sleep(2)
        
        return json.dumps({
            "status": "success",
            "total_urls": len(urls),
            "processed_urls": len(results),
            "successful": successful,
            "failed": failed,
            "results": results,
            "extraction_time": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æ‰¹é‡æŠ“å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)


@tool
def extract_article_list(
    url: str,
    article_selector: str = "article, .news-item, .article-item",
    title_selector: str = "h1, h2, h3, .title",
    link_selector: str = "a",
    date_selector: Optional[str] = None,
    max_articles: int = 20,
    use_playwright: bool = True
) -> str:
    """
    ä»åˆ—è¡¨é¡µæå–æ–‡ç« åˆ—è¡¨
    
    Args:
        url (str): åˆ—è¡¨é¡µURL
        article_selector (str): æ–‡ç« å®¹å™¨çš„CSSé€‰æ‹©å™¨
        title_selector (str): æ ‡é¢˜çš„CSSé€‰æ‹©å™¨
        link_selector (str): é“¾æ¥çš„CSSé€‰æ‹©å™¨
        date_selector (str, optional): æ—¥æœŸçš„CSSé€‰æ‹©å™¨
        max_articles (int): æœ€å¤§æå–æ–‡ç« æ•°
        use_playwright (bool): æ˜¯å¦ä½¿ç”¨Playwright
        
    Returns:
        str: JSONæ ¼å¼çš„æ–‡ç« åˆ—è¡¨
    """
    try:
        # å…ˆæŠ“å–åˆ—è¡¨é¡µ
        page_result_json = scrape_webpage(url, timeout=30000, use_playwright=use_playwright)
        page_result = json.loads(page_result_json)
        
        if page_result["status"] != "success":
            return json.dumps({
                "status": "error",
                "message": f"åˆ—è¡¨é¡µæŠ“å–å¤±è´¥: {page_result.get('message', 'Unknown error')}"
            }, ensure_ascii=False)
        
        # è§£æHTML
        soup = BeautifulSoup(page_result.get("content", ""), 'lxml')
        
        # æå–æ–‡ç« åˆ—è¡¨
        articles = []
        article_elements = soup.select(article_selector)[:max_articles]
        
        for idx, article_elem in enumerate(article_elements, 1):
            try:
                # æå–æ ‡é¢˜
                title_elem = article_elem.select_one(title_selector)
                title = title_elem.get_text(strip=True) if title_elem else ""
                
                # æå–é“¾æ¥
                link_elem = article_elem.select_one(link_selector)
                if link_elem and link_elem.get('href'):
                    article_url = urljoin(url, link_elem['href'])
                else:
                    article_url = ""
                
                # æå–æ—¥æœŸ
                publish_date = None
                if date_selector:
                    date_elem = article_elem.select_one(date_selector)
                    if date_elem:
                        publish_date = date_elem.get_text(strip=True)
                
                if title and article_url:
                    articles.append({
                        "index": idx,
                        "title": title,
                        "url": article_url,
                        "publish_date": publish_date,
                        "source_page": url
                    })
                    
            except Exception as e:
                print(f"æå–æ–‡ç«  {idx} å¤±è´¥: {e}")
                continue
        
        return json.dumps({
            "status": "success",
            "list_url": url,
            "total_articles": len(articles),
            "articles": articles,
            "extraction_time": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"æ–‡ç« åˆ—è¡¨æå–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)


@tool
def scrape_energy_news_sites(
    keywords: str,
    data_sources: Optional[List[str]] = None,
    max_articles_per_source: int = 10
) -> str:
    """
    ä»èƒ½æºè¡Œä¸šæ–°é—»ç½‘ç«™æŠ“å–å†…å®¹
    
    Args:
        keywords (str): æœç´¢å…³é”®è¯
        data_sources (List[str], optional): æ•°æ®æºåˆ—è¡¨
        max_articles_per_source (int): æ¯ä¸ªæ•°æ®æºæœ€å¤§æ–‡ç« æ•°
        
    Returns:
        str: JSONæ ¼å¼çš„æŠ“å–ç»“æœ
    """
    try:
        # é»˜è®¤æ•°æ®æº
        if not data_sources:
            data_sources = [
                "https://energy.bjx.com.cn/nyxny/",  # åŒ—ææ˜Ÿèƒ½æºç½‘
                "https://www.nea.gov.cn/",  # å›½å®¶èƒ½æºå±€
                "https://www.ndrc.gov.cn/"  # å›½å®¶å‘æ”¹å§”
            ]
        
        all_results = {
            "keywords": keywords,
            "data_sources": data_sources,
            "scraping_time": datetime.now().isoformat(),
            "results": [],
            "summary": {
                "total_sources": len(data_sources),
                "total_articles": 0,
                "successful_sources": 0,
                "failed_sources": 0
            }
        }
        
        # é’ˆå¯¹ä¸åŒæ•°æ®æºä½¿ç”¨ä¸åŒçš„æŠ“å–ç­–ç•¥
        for source_url in data_sources:
            try:
                domain = urlparse(source_url).netloc
                
                # æ ¹æ®åŸŸåé€‰æ‹©æŠ“å–ç­–ç•¥
                if "bjx.com.cn" in domain:
                    # åŒ—ææ˜Ÿèƒ½æºç½‘ - éœ€è¦Playwright
                    source_result = _scrape_bjx_energy(source_url, keywords, max_articles_per_source)
                elif "nea.gov.cn" in domain:
                    # å›½å®¶èƒ½æºå±€ - å¯ç”¨requests
                    source_result = _scrape_nea_gov(source_url, keywords, max_articles_per_source)
                elif "ndrc.gov.cn" in domain:
                    # å›½å®¶å‘æ”¹å§” - å¯ç”¨requests
                    source_result = _scrape_ndrc_gov(source_url, keywords, max_articles_per_source)
                else:
                    # å…¶ä»–æ¥æº - é€šç”¨æŠ“å–
                    source_result = _scrape_generic_site(source_url, keywords, max_articles_per_source)
                
                if source_result["status"] == "success":
                    all_results["results"].append(source_result)
                    all_results["summary"]["total_articles"] += len(source_result.get("articles", []))
                    all_results["summary"]["successful_sources"] += 1
                else:
                    all_results["results"].append(source_result)
                    all_results["summary"]["failed_sources"] += 1
                
                # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                time.sleep(3)
                
            except Exception as e:
                all_results["results"].append({
                    "status": "error",
                    "source_url": source_url,
                    "message": f"æ•°æ®æºæŠ“å–å¤±è´¥: {str(e)}"
                })
                all_results["summary"]["failed_sources"] += 1
        
        return json.dumps(all_results, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æ‰¹é‡æŠ“å–å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)


def _scrape_bjx_energy(url: str, keywords: str, max_articles: int) -> Dict[str, Any]:
    """æŠ“å–åŒ—ææ˜Ÿèƒ½æºç½‘"""
    try:
        # æ„å»ºæœç´¢URL
        search_url = f"https://energy.bjx.com.cn/search.aspx?keyword={keywords}"
        
        # ä½¿ç”¨PlaywrightæŠ“å–
        result_json = scrape_webpage(
            search_url,
            wait_for_selector=".news-list",
            scroll_to_bottom=True,
            timeout=30000,
            use_playwright=True
        )
        result = json.loads(result_json)
        
        if result["status"] != "success":
            return {
                "status": "error",
                "source_url": url,
                "source_name": "åŒ—ææ˜Ÿèƒ½æºç½‘",
                "message": "é¡µé¢æŠ“å–å¤±è´¥"
            }
        
        # æå–æ–‡ç« åˆ—è¡¨
        articles_json = extract_article_list(
            search_url,
            article_selector=".news-item, .article-item",
            title_selector="h3, .title",
            link_selector="a",
            date_selector=".date, .time",
            max_articles=max_articles,
            use_playwright=False  # å·²ç»æŠ“å–è¿‡é¡µé¢ï¼Œä½¿ç”¨ç¼“å­˜
        )
        articles_result = json.loads(articles_json)
        
        return {
            "status": "success",
            "source_url": url,
            "source_name": "åŒ—ææ˜Ÿèƒ½æºç½‘",
            "articles": articles_result.get("articles", []),
            "total_articles": len(articles_result.get("articles", []))
        }
        
    except Exception as e:
        return {
            "status": "error",
            "source_url": url,
            "source_name": "åŒ—ææ˜Ÿèƒ½æºç½‘",
            "message": f"æŠ“å–å¤±è´¥: {str(e)}"
        }


def _scrape_nea_gov(url: str, keywords: str, max_articles: int) -> Dict[str, Any]:
    """æŠ“å–å›½å®¶èƒ½æºå±€"""
    try:
        # å›½å®¶èƒ½æºå±€é€šå¸¸æœ‰æ–°é—»åˆ—è¡¨é¡µ
        news_url = urljoin(url, "/xwfb/")
        
        result_json = scrape_webpage(news_url, timeout=30000, use_playwright=False)
        result = json.loads(result_json)
        
        if result["status"] != "success":
            return {
                "status": "error",
                "source_url": url,
                "source_name": "å›½å®¶èƒ½æºå±€",
                "message": "é¡µé¢æŠ“å–å¤±è´¥"
            }
        
        # ä»å†…å®¹ä¸­è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–‡ç« 
        soup = BeautifulSoup(result.get("content", ""), 'lxml')
        articles = []
        
        for a_tag in soup.find_all('a', href=True):
            title = a_tag.get_text(strip=True)
            if keywords in title and len(title) > 10:
                article_url = urljoin(news_url, a_tag['href'])
                articles.append({
                    "title": title,
                    "url": article_url,
                    "source_page": news_url
                })
                
                if len(articles) >= max_articles:
                    break
        
        return {
            "status": "success",
            "source_url": url,
            "source_name": "å›½å®¶èƒ½æºå±€",
            "articles": articles,
            "total_articles": len(articles)
        }
        
    except Exception as e:
        return {
            "status": "error",
            "source_url": url,
            "source_name": "å›½å®¶èƒ½æºå±€",
            "message": f"æŠ“å–å¤±è´¥: {str(e)}"
        }


def _scrape_ndrc_gov(url: str, keywords: str, max_articles: int) -> Dict[str, Any]:
    """æŠ“å–å›½å®¶å‘æ”¹å§”"""
    try:
        # å›½å®¶å‘æ”¹å§”é€šå¸¸æœ‰æ–°é—»åˆ—è¡¨é¡µ
        news_url = urljoin(url, "/xwzx/")
        
        result_json = scrape_webpage(news_url, timeout=30000, use_playwright=False)
        result = json.loads(result_json)
        
        if result["status"] != "success":
            return {
                "status": "error",
                "source_url": url,
                "source_name": "å›½å®¶å‘æ”¹å§”",
                "message": "é¡µé¢æŠ“å–å¤±è´¥"
            }
        
        # ä»å†…å®¹ä¸­è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–‡ç« 
        soup = BeautifulSoup(result.get("content", ""), 'lxml')
        articles = []
        
        for a_tag in soup.find_all('a', href=True):
            title = a_tag.get_text(strip=True)
            if keywords in title and len(title) > 10:
                article_url = urljoin(news_url, a_tag['href'])
                articles.append({
                    "title": title,
                    "url": article_url,
                    "source_page": news_url
                })
                
                if len(articles) >= max_articles:
                    break
        
        return {
            "status": "success",
            "source_url": url,
            "source_name": "å›½å®¶å‘æ”¹å§”",
            "articles": articles,
            "total_articles": len(articles)
        }
        
    except Exception as e:
        return {
            "status": "error",
            "source_url": url,
            "source_name": "å›½å®¶å‘æ”¹å§”",
            "message": f"æŠ“å–å¤±è´¥: {str(e)}"
        }


def _scrape_generic_site(url: str, keywords: str, max_articles: int) -> Dict[str, Any]:
    """é€šç”¨ç½‘ç«™æŠ“å–"""
    try:
        result_json = scrape_webpage(url, timeout=30000, use_playwright=True)
        result = json.loads(result_json)
        
        if result["status"] != "success":
            return {
                "status": "error",
                "source_url": url,
                "source_name": urlparse(url).netloc,
                "message": "é¡µé¢æŠ“å–å¤±è´¥"
            }
        
        # ä»é“¾æ¥ä¸­è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–‡ç« 
        articles = []
        for link in result.get("links", []):
            if keywords in link["text"]:
                articles.append({
                    "title": link["text"],
                    "url": link["url"],
                    "source_page": url
                })
                
                if len(articles) >= max_articles:
                    break
        
        return {
            "status": "success",
            "source_url": url,
            "source_name": urlparse(url).netloc,
            "articles": articles,
            "total_articles": len(articles)
        }
        
    except Exception as e:
        return {
            "status": "error",
            "source_url": url,
            "source_name": urlparse(url).netloc,
            "message": f"æŠ“å–å¤±è´¥: {str(e)}"
        }


@tool
def scrape_with_retry(
    url: str,
    max_retries: int = 3,
    retry_delay: int = 5,
    timeout: int = 30000
) -> str:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„ç½‘é¡µæŠ“å–
    
    Args:
        url (str): ç›®æ ‡ç½‘é¡µURL
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay (int): é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„æŠ“å–ç»“æœ
    """
    for attempt in range(max_retries):
        try:
            result_json = scrape_webpage(url, timeout=timeout, use_playwright=True)
            result = json.loads(result_json)
            
            if result["status"] == "success":
                result["retry_attempts"] = attempt
                return json.dumps(result, ensure_ascii=False, indent=2)
            
            # å¦‚æœå¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (attempt + 1))
            else:
                return json.dumps({
                    "status": "error",
                    "url": url,
                    "message": f"é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {str(e)}",
                    "retry_attempts": max_retries
                }, ensure_ascii=False)
    
    return json.dumps({
        "status": "error",
        "url": url,
        "message": f"é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥",
        "retry_attempts": max_retries
    }, ensure_ascii=False)


if __name__ == "__main__":
    # æµ‹è¯•å·¥å…·
    print("ğŸ§ª æµ‹è¯•ç½‘é¡µæŠ“å–å·¥å…·...")
    
    # æµ‹è¯•å•é¡µæŠ“å–
    test_url = "https://www.nea.gov.cn/"
    result = scrape_webpage(test_url, use_playwright=False)
    print("ğŸ“„ å•é¡µæŠ“å–ç»“æœ:", json.loads(result)["status"])
    
    # æµ‹è¯•æ–‡ç« åˆ—è¡¨æå–
    list_url = "https://energy.bjx.com.cn/nyxny/"
    articles = extract_article_list(list_url)
    print("ğŸ“‹ æ–‡ç« åˆ—è¡¨æå–:", json.loads(articles)["status"])
    
    print("âœ… å·¥å…·æµ‹è¯•å®Œæˆï¼")
