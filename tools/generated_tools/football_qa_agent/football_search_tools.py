#!/usr/bin/env python3
"""
è¶³çƒé—®ç­”æœç´¢æ•´ç†å·¥å…·é›†

ä¸“ä¸ºfootball_search_organizer Agentè®¾è®¡çš„å·¥å…·é›†ï¼Œæä¾›ç½‘ç»œæœç´¢ã€ä¿¡æ¯æ”¶é›†ã€
æ•°æ®åˆ†æã€æŠ¥å‘Šç”Ÿæˆç­‰åŠŸèƒ½ï¼Œæ”¯æŒè¶³çƒç›¸å…³é—®ç­”çš„æœç´¢å’Œæ•´ç†ã€‚

å·¥å…·åˆ—è¡¨ï¼š
1. web_search_enhanced - å¢å¼ºçš„ç½‘ç»œæœç´¢å·¥å…·
2. information_collector - ä¿¡æ¯æ”¶é›†å·¥å…·
3. data_analyzer - æ•°æ®åˆ†æå·¥å…·
4. report_generator - æŠ¥å‘Šç”Ÿæˆå·¥å…·
5. http_request - HTTPè¯·æ±‚å·¥å…·

æ³¨æ„ï¼šcurrent_timeå·¥å…·ä½¿ç”¨Strandsæ¡†æ¶çš„å†…ç½®å·¥å…·
"""

import json
import re
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
from urllib.parse import quote_plus, urlparse, urljoin
import requests
from bs4 import BeautifulSoup
from ddgs import DDGS
from ddgs.exceptions import DDGSException, RatelimitException

from strands import tool


@tool
def web_search_enhanced(
    query: str,
    search_scope: str = "general",
    max_results: int = 5,
    language: str = "zh-cn"
) -> str:
    """
    å¢å¼ºçš„ç½‘ç»œæœç´¢å·¥å…· - æ”¯æŒå¤šæºè¶³çƒä¿¡æ¯æœç´¢
    
    ä¸“ä¸ºè¶³çƒé—®ç­”è®¾è®¡ï¼Œæ”¯æŒå¤šç§æœç´¢èŒƒå›´å’Œè¯­è¨€ï¼Œè¿”å›ç»“æ„åŒ–çš„æœç´¢ç»“æœã€‚
    
    Args:
        query (str): æœç´¢å…³é”®è¯ï¼ˆæ”¯æŒè¶³çƒé˜Ÿåã€çƒå‘˜åã€æ¯”èµ›ä¿¡æ¯ç­‰ï¼‰
        search_scope (str): æœç´¢èŒƒå›´
            - general: é€šç”¨æœç´¢
            - news: æ–°é—»æœç´¢ï¼ˆæœ€æ–°è¶³çƒæ–°é—»ï¼‰
            - statistics: ç»Ÿè®¡æ•°æ®æœç´¢ï¼ˆçƒå‘˜ã€çƒé˜Ÿæ•°æ®ï¼‰
            - matches: æ¯”èµ›ä¿¡æ¯æœç´¢
            - transfers: è½¬ä¼šä¿¡æ¯æœç´¢
            - historical: å†å²æ•°æ®æœç´¢
        max_results (int): æœ€å¤§ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼ŒèŒƒå›´1-20ï¼‰
        language (str): æœç´¢è¯­è¨€ï¼ˆé»˜è®¤zh-cnï¼Œæ”¯æŒen-usç­‰ï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„æœç´¢ç»“æœï¼ŒåŒ…å«ï¼š
            - query: æœç´¢å…³é”®è¯
            - search_scope: æœç´¢èŒƒå›´
            - total_results: ç»“æœæ€»æ•°
            - results: æœç´¢ç»“æœåˆ—è¡¨
                - title: æ ‡é¢˜
                - url: URLé“¾æ¥
                - snippet: æ‘˜è¦
                - source: æ¥æº
                - published_date: å‘å¸ƒæ—¶é—´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - relevance_score: ç›¸å…³åº¦è¯„åˆ†
            - search_metadata: æœç´¢å…ƒæ•°æ®
    """
    try:
        # å‚æ•°éªŒè¯
        max_results = max(1, min(max_results, 20))
        
        # æ ¹æ®æœç´¢èŒƒå›´ä¼˜åŒ–æŸ¥è¯¢
        enhanced_query = _enhance_football_query(query, search_scope)
        
        # è®¾ç½®æœç´¢åŒºåŸŸ
        region = "cn-zh" if language == "zh-cn" else "us-en"
        
        results_list = []
        search_metadata = {
            "original_query": query,
            "enhanced_query": enhanced_query,
            "search_scope": search_scope,
            "language": language,
            "search_time": datetime.now().isoformat(),
            "search_engine": "DuckDuckGo"
        }
        
        try:
            # æ ¹æ®æœç´¢èŒƒå›´é€‰æ‹©æœç´¢ç±»å‹
            if search_scope == "news":
                # æ–°é—»æœç´¢
                ddgs_results = DDGS().news(
                    enhanced_query,
                    region=region,
                    max_results=max_results
                )
                
                for idx, result in enumerate(ddgs_results, 1):
                    results_list.append({
                        "rank": idx,
                        "title": result.get("title", "æ— æ ‡é¢˜"),
                        "url": result.get("url", ""),
                        "snippet": result.get("body", "æ— æ‘˜è¦"),
                        "source": result.get("source", "æœªçŸ¥æ¥æº"),
                        "published_date": result.get("date", ""),
                        "relevance_score": _calculate_relevance_score(
                            result.get("title", "") + " " + result.get("body", ""),
                            query
                        )
                    })
            else:
                # é€šç”¨ç½‘é¡µæœç´¢
                ddgs_results = DDGS().text(
                    enhanced_query,
                    region=region,
                    max_results=max_results
                )
                
                for idx, result in enumerate(ddgs_results, 1):
                    results_list.append({
                        "rank": idx,
                        "title": result.get("title", "æ— æ ‡é¢˜"),
                        "url": result.get("href", ""),
                        "snippet": result.get("body", "æ— æ‘˜è¦"),
                        "source": _extract_domain(result.get("href", "")),
                        "published_date": "",
                        "relevance_score": _calculate_relevance_score(
                            result.get("title", "") + " " + result.get("body", ""),
                            query
                        )
                    })
            
            # æŒ‰ç›¸å…³åº¦æ’åº
            results_list.sort(key=lambda x: x["relevance_score"], reverse=True)
            
        except RatelimitException:
            search_metadata["warning"] = "æœç´¢é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
        except DDGSException as e:
            search_metadata["error"] = f"æœç´¢å¼•æ“é”™è¯¯: {str(e)}"
        except Exception as e:
            search_metadata["error"] = f"æœç´¢å¼‚å¸¸: {str(e)}"
        
        # æ„å»ºè¿”å›ç»“æœ
        response = {
            "query": query,
            "search_scope": search_scope,
            "total_results": len(results_list),
            "results": results_list,
            "search_metadata": search_metadata,
            "suggestions": _generate_search_suggestions(query, search_scope)
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "error": f"æœç´¢å¤±è´¥: {str(e)}",
            "query": query,
            "search_scope": search_scope,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)


@tool
def information_collector(
    urls: List[str],
    extract_options: Dict[str, bool] = None,
    timeout: int = 30
) -> str:
    """
    ä¿¡æ¯æ”¶é›†å·¥å…· - ä»ç½‘é¡µURLè·å–å®Œæ•´å†…å®¹å¹¶è¿›è¡Œè§£æ
    
    æ”¯æŒä»å¤šä¸ªURLæ‰¹é‡è·å–å†…å®¹ï¼Œè‡ªåŠ¨è§£æç½‘é¡µç»“æ„ï¼Œæå–æ ‡é¢˜ã€æ­£æ–‡ã€
    è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç»“æ„åŒ–ä¿¡æ¯ã€‚ä¸“ä¸ºè¶³çƒæ•°æ®æ”¶é›†ä¼˜åŒ–ã€‚
    
    Args:
        urls (List[str]): URLåˆ—è¡¨ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼Œæœ€å¤š10ä¸ªï¼‰
        extract_options (Dict[str, bool]): å†…å®¹æå–é€‰é¡¹
            - extract_text: æå–æ­£æ–‡æ–‡æœ¬ï¼ˆé»˜è®¤Trueï¼‰
            - extract_tables: æå–è¡¨æ ¼æ•°æ®ï¼ˆé»˜è®¤Trueï¼‰
            - extract_lists: æå–åˆ—è¡¨æ•°æ®ï¼ˆé»˜è®¤Trueï¼‰
            - extract_images: æå–å›¾ç‰‡é“¾æ¥ï¼ˆé»˜è®¤Falseï¼‰
            - extract_metadata: æå–å…ƒæ•°æ®ï¼ˆé»˜è®¤Trueï¼‰
        timeout (int): å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30ï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„æ”¶é›†ç»“æœï¼ŒåŒ…å«ï¼š
            - total_urls: URLæ€»æ•°
            - successful_count: æˆåŠŸè·å–æ•°é‡
            - failed_count: å¤±è´¥æ•°é‡
            - collected_data: æ”¶é›†åˆ°çš„æ•°æ®åˆ—è¡¨
                - url: åŸå§‹URL
                - title: ç½‘é¡µæ ‡é¢˜
                - content: ä¸»è¦å†…å®¹
                - tables: è¡¨æ ¼æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - lists: åˆ—è¡¨æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - images: å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - metadata: å…ƒæ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - word_count: å­—æ•°ç»Ÿè®¡
                - collection_time: æ”¶é›†æ—¶é—´
                - status: æ”¶é›†çŠ¶æ€
            - collection_summary: æ”¶é›†æ‘˜è¦
    """
    try:
        # å‚æ•°éªŒè¯
        if not urls or len(urls) == 0:
            raise ValueError("URLåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        urls = urls[:10]  # é™åˆ¶æœ€å¤š10ä¸ªURL
        
        if extract_options is None:
            extract_options = {
                "extract_text": True,
                "extract_tables": True,
                "extract_lists": True,
                "extract_images": False,
                "extract_metadata": True
            }
        
        collected_data = []
        successful_count = 0
        failed_count = 0
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive"
        }
        
        for url in urls:
            try:
                # å‘èµ·HTTPè¯·æ±‚
                response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
                response.raise_for_status()
                
                # æ£€æµ‹ç¼–ç 
                response.encoding = response.apparent_encoding or 'utf-8'
                
                # è§£æHTML
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–æ ‡é¢˜
                title = ""
                if soup.title:
                    title = soup.title.string.strip() if soup.title.string else ""
                if not title:
                    h1 = soup.find('h1')
                    title = h1.get_text(strip=True) if h1 else "æ— æ ‡é¢˜"
                
                # åˆå§‹åŒ–æ•°æ®ç»“æ„
                page_data = {
                    "url": url,
                    "title": title,
                    "content": "",
                    "tables": [],
                    "lists": [],
                    "images": [],
                    "metadata": {},
                    "word_count": 0,
                    "collection_time": datetime.now().isoformat(),
                    "status": "success"
                }
                
                # æå–æ­£æ–‡æ–‡æœ¬
                if extract_options.get("extract_text", True):
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in soup(["script", "style", "nav", "footer", "header"]):
                        script.decompose()
                    
                    # æå–ä¸»è¦å†…å®¹
                    main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|article|main'))
                    if main_content:
                        content_text = main_content.get_text(separator='\n', strip=True)
                    else:
                        content_text = soup.get_text(separator='\n', strip=True)
                    
                    # æ¸…ç†æ–‡æœ¬
                    content_text = re.sub(r'\n\s*\n+', '\n\n', content_text)
                    page_data["content"] = content_text[:10000]  # é™åˆ¶é•¿åº¦
                    page_data["word_count"] = len(content_text)
                
                # æå–è¡¨æ ¼æ•°æ®
                if extract_options.get("extract_tables", True):
                    tables = soup.find_all('table')
                    for idx, table in enumerate(tables[:5]):  # æœ€å¤š5ä¸ªè¡¨æ ¼
                        table_data = _extract_table_data(table)
                        if table_data:
                            page_data["tables"].append({
                                "table_index": idx + 1,
                                "rows": len(table_data),
                                "columns": len(table_data[0]) if table_data else 0,
                                "data": table_data
                            })
                
                # æå–åˆ—è¡¨æ•°æ®
                if extract_options.get("extract_lists", True):
                    lists = soup.find_all(['ul', 'ol'])
                    for idx, list_elem in enumerate(lists[:10]):  # æœ€å¤š10ä¸ªåˆ—è¡¨
                        list_items = [li.get_text(strip=True) for li in list_elem.find_all('li')]
                        if list_items:
                            page_data["lists"].append({
                                "list_index": idx + 1,
                                "type": list_elem.name,
                                "items_count": len(list_items),
                                "items": list_items[:20]  # æœ€å¤š20é¡¹
                            })
                
                # æå–å›¾ç‰‡é“¾æ¥
                if extract_options.get("extract_images", False):
                    images = soup.find_all('img')
                    for img in images[:20]:  # æœ€å¤š20å¼ å›¾ç‰‡
                        img_src = img.get('src', '')
                        if img_src:
                            # å¤„ç†ç›¸å¯¹URL
                            img_url = urljoin(url, img_src)
                            page_data["images"].append({
                                "url": img_url,
                                "alt": img.get('alt', ''),
                                "title": img.get('title', '')
                            })
                
                # æå–å…ƒæ•°æ®
                if extract_options.get("extract_metadata", True):
                    metadata = {}
                    
                    # Open Graphæ ‡ç­¾
                    og_tags = soup.find_all('meta', property=re.compile(r'^og:'))
                    for tag in og_tags:
                        property_name = tag.get('property', '').replace('og:', '')
                        metadata[property_name] = tag.get('content', '')
                    
                    # Twitter Cardæ ‡ç­¾
                    twitter_tags = soup.find_all('meta', attrs={'name': re.compile(r'^twitter:')})
                    for tag in twitter_tags:
                        name = tag.get('name', '').replace('twitter:', '')
                        metadata[f"twitter_{name}"] = tag.get('content', '')
                    
                    # æ ‡å‡†metaæ ‡ç­¾
                    description = soup.find('meta', attrs={'name': 'description'})
                    if description:
                        metadata['description'] = description.get('content', '')
                    
                    keywords = soup.find('meta', attrs={'name': 'keywords'})
                    if keywords:
                        metadata['keywords'] = keywords.get('content', '')
                    
                    author = soup.find('meta', attrs={'name': 'author'})
                    if author:
                        metadata['author'] = author.get('content', '')
                    
                    page_data["metadata"] = metadata
                
                collected_data.append(page_data)
                successful_count += 1
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)
                
            except requests.exceptions.Timeout:
                collected_data.append({
                    "url": url,
                    "status": "timeout",
                    "error": f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰",
                    "collection_time": datetime.now().isoformat()
                })
                failed_count += 1
                
            except requests.exceptions.RequestException as e:
                collected_data.append({
                    "url": url,
                    "status": "failed",
                    "error": f"è¯·æ±‚å¤±è´¥: {str(e)}",
                    "collection_time": datetime.now().isoformat()
                })
                failed_count += 1
                
            except Exception as e:
                collected_data.append({
                    "url": url,
                    "status": "error",
                    "error": f"è§£æé”™è¯¯: {str(e)}",
                    "collection_time": datetime.now().isoformat()
                })
                failed_count += 1
        
        # æ„å»ºè¿”å›ç»“æœ
        response = {
            "total_urls": len(urls),
            "successful_count": successful_count,
            "failed_count": failed_count,
            "success_rate": f"{(successful_count / len(urls) * 100):.1f}%",
            "collected_data": collected_data,
            "collection_summary": {
                "total_words": sum(d.get("word_count", 0) for d in collected_data),
                "total_tables": sum(len(d.get("tables", [])) for d in collected_data),
                "total_lists": sum(len(d.get("lists", [])) for d in collected_data),
                "total_images": sum(len(d.get("images", [])) for d in collected_data),
                "collection_time": datetime.now().isoformat()
            },
            "extract_options": extract_options
        }
        
        return json.dumps(response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "error": f"ä¿¡æ¯æ”¶é›†å¤±è´¥: {str(e)}",
            "total_urls": len(urls) if urls else 0,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)


@tool
def data_analyzer(
    raw_data: str,
    analysis_dimension: str = "comprehensive",
    data_type: str = "auto"
) -> str:
    """
    æ•°æ®åˆ†æå·¥å…· - åˆ†æè¶³çƒç»Ÿè®¡æ•°æ®ï¼Œæå–å…³é”®ä¿¡æ¯
    
    æ”¯æŒå¤šç§è¶³çƒæ•°æ®åˆ†æï¼ŒåŒ…æ‹¬çƒå‘˜æ•°æ®ã€çƒé˜Ÿæ•°æ®ã€æ¯”èµ›æ•°æ®ç­‰ã€‚
    è‡ªåŠ¨è¯†åˆ«æ•°æ®ç±»å‹å¹¶è¿›è¡Œç›¸åº”çš„åˆ†æå¤„ç†ã€‚
    
    Args:
        raw_data (str): åŸå§‹æ•°æ®æˆ–æ–‡æœ¬ï¼ˆæ”¯æŒJSONã€æ–‡æœ¬ã€è¡¨æ ¼ç­‰æ ¼å¼ï¼‰
        analysis_dimension (str): åˆ†æç»´åº¦
            - player: çƒå‘˜åˆ†æï¼ˆè¿›çƒã€åŠ©æ”»ã€è¯„åˆ†ç­‰ï¼‰
            - team: çƒé˜Ÿåˆ†æï¼ˆç§¯åˆ†ã€æ’åã€æˆ˜ç»©ç­‰ï¼‰
            - match: æ¯”èµ›åˆ†æï¼ˆæ¯”åˆ†ã€æ•°æ®ã€äº‹ä»¶ç­‰ï¼‰
            - comprehensive: ç»¼åˆåˆ†æï¼ˆé»˜è®¤ï¼‰
            - statistics: ç»Ÿè®¡åˆ†æï¼ˆæ•°å€¼ç»Ÿè®¡ï¼‰
            - trend: è¶‹åŠ¿åˆ†æï¼ˆæ—¶é—´åºåˆ—ï¼‰
        data_type (str): æ•°æ®ç±»å‹
            - auto: è‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ï¼‰
            - json: JSONæ ¼å¼
            - text: çº¯æ–‡æœ¬
            - table: è¡¨æ ¼æ•°æ®
            - mixed: æ··åˆæ ¼å¼
        
    Returns:
        str: JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
            - analysis_metadata: åˆ†æå…ƒæ•°æ®
            - data_summary: æ•°æ®æ‘˜è¦
            - key_metrics: å…³é”®æŒ‡æ ‡
            - statistical_analysis: ç»Ÿè®¡åˆ†æ
            - insights: æ´å¯Ÿå‘ç°
            - recommendations: å»ºè®®
    """
    try:
        # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
        if data_type == "auto":
            data_type = _detect_data_type(raw_data)
        
        # è§£ææ•°æ®
        parsed_data = _parse_data(raw_data, data_type)
        
        # åˆå§‹åŒ–åˆ†æç»“æœ
        analysis_result = {
            "analysis_metadata": {
                "analysis_time": datetime.now().isoformat(),
                "analysis_dimension": analysis_dimension,
                "data_type": data_type,
                "data_size": len(raw_data),
                "analyzer_version": "1.0"
            },
            "data_summary": {},
            "key_metrics": {},
            "statistical_analysis": {},
            "insights": [],
            "recommendations": []
        }
        
        # æ ¹æ®åˆ†æç»´åº¦æ‰§è¡Œä¸åŒçš„åˆ†æ
        if analysis_dimension == "player":
            analysis_result.update(_analyze_player_data(parsed_data))
        elif analysis_dimension == "team":
            analysis_result.update(_analyze_team_data(parsed_data))
        elif analysis_dimension == "match":
            analysis_result.update(_analyze_match_data(parsed_data))
        elif analysis_dimension == "statistics":
            analysis_result.update(_analyze_statistics(parsed_data))
        elif analysis_dimension == "trend":
            analysis_result.update(_analyze_trend(parsed_data))
        else:  # comprehensive
            analysis_result.update(_analyze_comprehensive(parsed_data))
        
        return json.dumps(analysis_result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "error": f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}",
            "analysis_dimension": analysis_dimension,
            "data_type": data_type,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)


@tool
def report_generator(
    analysis_data: str,
    report_type: str = "qa_answer",
    output_format: str = "json",
    include_sources: bool = True
) -> str:
    """
    æŠ¥å‘Šç”Ÿæˆå·¥å…· - ç”Ÿæˆç»“æ„åŒ–çš„è¶³çƒé—®ç­”æŠ¥å‘Š
    
    æ ¹æ®åˆ†ææ•°æ®ç”Ÿæˆæ ¼å¼åŒ–çš„æŠ¥å‘Šï¼Œæ”¯æŒå¤šç§æŠ¥å‘Šç±»å‹å’Œè¾“å‡ºæ ¼å¼ã€‚
    ä¸“ä¸ºè¶³çƒé—®ç­”åœºæ™¯ä¼˜åŒ–ï¼Œç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€æ¥æºå¯é ã€‚
    
    Args:
        analysis_data (str): åˆ†ææ•°æ®ï¼ˆJSONæ ¼å¼ï¼Œæ¥è‡ªdata_analyzeræˆ–å…¶ä»–å·¥å…·ï¼‰
        report_type (str): æŠ¥å‘Šç±»å‹
            - qa_answer: é—®ç­”ç­”æ¡ˆæŠ¥å‘Šï¼ˆé»˜è®¤ï¼‰
            - summary: æ‘˜è¦æŠ¥å‘Š
            - detailed: è¯¦ç»†æŠ¥å‘Š
            - comparison: å¯¹æ¯”æŠ¥å‘Š
            - ranking: æ’åæŠ¥å‘Š
        output_format (str): è¾“å‡ºæ ¼å¼
            - json: JSONæ ¼å¼ï¼ˆé»˜è®¤ï¼Œç»“æ„åŒ–æ•°æ®ï¼‰
            - markdown: Markdownæ ¼å¼ï¼ˆæ˜“è¯»ï¼‰
            - html: HTMLæ ¼å¼ï¼ˆç½‘é¡µå±•ç¤ºï¼‰
            - text: çº¯æ–‡æœ¬æ ¼å¼
        include_sources (bool): æ˜¯å¦åŒ…å«ä¿¡æ¯æ¥æºï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        str: æ ¼å¼åŒ–çš„æŠ¥å‘Šå†…å®¹
    """
    try:
        # è§£æè¾“å…¥æ•°æ®
        try:
            data = json.loads(analysis_data) if isinstance(analysis_data, str) else analysis_data
        except:
            data = {"raw_data": analysis_data}
        
        # ç”ŸæˆæŠ¥å‘Š
        if report_type == "qa_answer":
            report = _generate_qa_answer_report(data, include_sources)
        elif report_type == "summary":
            report = _generate_summary_report(data, include_sources)
        elif report_type == "detailed":
            report = _generate_detailed_report(data, include_sources)
        elif report_type == "comparison":
            report = _generate_comparison_report(data, include_sources)
        elif report_type == "ranking":
            report = _generate_ranking_report(data, include_sources)
        else:
            report = _generate_qa_answer_report(data, include_sources)
        
        # æ ¼å¼åŒ–è¾“å‡º
        if output_format == "markdown":
            return _format_as_markdown(report)
        elif output_format == "html":
            return _format_as_html(report)
        elif output_format == "text":
            return _format_as_text(report)
        else:  # json
            return json.dumps(report, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "error": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}",
            "report_type": report_type,
            "output_format": output_format,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)


@tool
def http_request(
    url: str,
    method: str = "GET",
    headers: Dict[str, str] = None,
    params: Dict[str, Any] = None,
    data: Union[Dict[str, Any], str] = None,
    json_data: Dict[str, Any] = None,
    timeout: int = 30,
    allow_redirects: bool = True,
    verify_ssl: bool = True
) -> str:
    """
    HTTPè¯·æ±‚å·¥å…· - æ‰§è¡ŒHTTPè¯·æ±‚è·å–æ•°æ®
    
    é€šç”¨çš„HTTPå®¢æˆ·ç«¯å·¥å…·ï¼Œæ”¯æŒå„ç§HTTPæ–¹æ³•å’Œé…ç½®é€‰é¡¹ã€‚
    æä¾›å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ã€‚
    
    Args:
        url (str): è¯·æ±‚URL
        method (str): è¯·æ±‚æ–¹æ³•ï¼ˆGET, POST, PUT, DELETE, PATCH, HEAD, OPTIONSï¼‰
        headers (Dict[str, str]): è¯·æ±‚å¤´ï¼ˆå¯é€‰ï¼‰
        params (Dict[str, Any]): URLæŸ¥è¯¢å‚æ•°ï¼ˆå¯é€‰ï¼‰
        data (Union[Dict[str, Any], str]): è¯·æ±‚ä½“æ•°æ®ï¼ˆè¡¨å•æ•°æ®ï¼‰
        json_data (Dict[str, Any]): JSONè¯·æ±‚ä½“æ•°æ®
        timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30ï¼‰
        allow_redirects (bool): æ˜¯å¦å…è®¸é‡å®šå‘ï¼ˆé»˜è®¤Trueï¼‰
        verify_ssl (bool): æ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„å“åº”ç»“æœï¼ŒåŒ…å«ï¼š
            - status_code: HTTPçŠ¶æ€ç 
            - status_text: çŠ¶æ€æ–‡æœ¬
            - headers: å“åº”å¤´
            - content: å“åº”å†…å®¹
            - content_type: å†…å®¹ç±»å‹
            - encoding: ç¼–ç 
            - url: æœ€ç»ˆURLï¼ˆé‡å®šå‘åï¼‰
            - elapsed_time: è¯·æ±‚è€—æ—¶ï¼ˆç§’ï¼‰
            - request_metadata: è¯·æ±‚å…ƒæ•°æ®
    """
    try:
        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        if headers is None:
            headers = {}
        
        if "User-Agent" not in headers:
            headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        
        # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å‘èµ·HTTPè¯·æ±‚
        response = requests.request(
            method=method.upper(),
            url=url,
            headers=headers,
            params=params,
            data=data,
            json=json_data,
            timeout=timeout,
            allow_redirects=allow_redirects,
            verify=verify_ssl
        )
        
        # è®¡ç®—è¯·æ±‚è€—æ—¶
        elapsed_time = time.time() - start_time
        
        # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        if response.encoding is None or response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding or 'utf-8'
        
        # è§£æå“åº”å†…å®¹
        content_type = response.headers.get('Content-Type', '')
        
        if 'application/json' in content_type:
            try:
                content = response.json()
            except:
                content = response.text
        else:
            content = response.text
        
        # æ„å»ºå“åº”ç»“æœ
        result = {
            "status_code": response.status_code,
            "status_text": response.reason,
            "success": 200 <= response.status_code < 300,
            "headers": dict(response.headers),
            "content": content,
            "content_type": content_type,
            "encoding": response.encoding,
            "url": response.url,
            "elapsed_time": round(elapsed_time, 3),
            "request_metadata": {
                "method": method.upper(),
                "original_url": url,
                "redirected": url != response.url,
                "timestamp": datetime.now().isoformat(),
                "content_length": len(response.content)
            }
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except requests.exceptions.Timeout:
        error_response = {
            "error": "è¯·æ±‚è¶…æ—¶",
            "error_type": "timeout",
            "url": url,
            "timeout": timeout,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)
        
    except requests.exceptions.SSLError as e:
        error_response = {
            "error": f"SSLè¯ä¹¦éªŒè¯å¤±è´¥: {str(e)}",
            "error_type": "ssl_error",
            "url": url,
            "suggestion": "å°è¯•è®¾ç½®verify_ssl=False",
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)
        
    except requests.exceptions.ConnectionError as e:
        error_response = {
            "error": f"è¿æ¥å¤±è´¥: {str(e)}",
            "error_type": "connection_error",
            "url": url,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)
        
    except requests.exceptions.RequestException as e:
        error_response = {
            "error": f"è¯·æ±‚å¼‚å¸¸: {str(e)}",
            "error_type": "request_exception",
            "url": url,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_response = {
            "error": f"æœªçŸ¥é”™è¯¯: {str(e)}",
            "error_type": "unknown_error",
            "url": url,
            "timestamp": datetime.now().isoformat()
        }
        return json.dumps(error_response, ensure_ascii=False, indent=2)


# ==================== è¾…åŠ©å‡½æ•° ====================

def _enhance_football_query(query: str, search_scope: str) -> str:
    """å¢å¼ºè¶³çƒæœç´¢æŸ¥è¯¢"""
    enhancements = {
        "statistics": f"{query} æ•°æ® ç»Ÿè®¡",
        "matches": f"{query} æ¯”èµ› èµ›ç¨‹ ç»“æœ",
        "transfers": f"{query} è½¬ä¼š äº¤æ˜“",
        "historical": f"{query} å†å² è®°å½•",
        "news": f"{query} æœ€æ–° æ–°é—»"
    }
    return enhancements.get(search_scope, query)


def _calculate_relevance_score(text: str, query: str) -> float:
    """è®¡ç®—ç›¸å…³åº¦è¯„åˆ†"""
    if not text or not query:
        return 0.0
    
    text_lower = text.lower()
    query_terms = query.lower().split()
    
    # è®¡ç®—æŸ¥è¯¢è¯å‡ºç°æ¬¡æ•°
    matches = sum(text_lower.count(term) for term in query_terms)
    
    # å½’ä¸€åŒ–è¯„åˆ†ï¼ˆ0-1ï¼‰
    score = min(matches / (len(query_terms) * 3), 1.0)
    
    return round(score, 3)


def _extract_domain(url: str) -> str:
    """æå–åŸŸå"""
    try:
        parsed = urlparse(url)
        return parsed.netloc or "æœªçŸ¥æ¥æº"
    except:
        return "æœªçŸ¥æ¥æº"


def _generate_search_suggestions(query: str, search_scope: str) -> List[str]:
    """ç”Ÿæˆæœç´¢å»ºè®®"""
    suggestions = [
        f"{query} æœ€æ–°æ•°æ®",
        f"{query} è¯¦ç»†ä¿¡æ¯",
        f"{query} å†å²è®°å½•"
    ]
    
    if search_scope == "statistics":
        suggestions.extend([
            f"{query} æœ¬èµ›å­£ç»Ÿè®¡",
            f"{query} èŒä¸šç”Ÿæ¶¯æ•°æ®"
        ])
    elif search_scope == "matches":
        suggestions.extend([
            f"{query} è¿‘æœŸæ¯”èµ›",
            f"{query} èµ›ç¨‹å®‰æ’"
        ])
    
    return suggestions[:5]


def _extract_table_data(table) -> List[List[str]]:
    """æå–è¡¨æ ¼æ•°æ®"""
    try:
        rows = []
        for tr in table.find_all('tr'):
            cells = []
            for cell in tr.find_all(['th', 'td']):
                cells.append(cell.get_text(strip=True))
            if cells:
                rows.append(cells)
        return rows
    except:
        return []


def _detect_data_type(data: str) -> str:
    """æ£€æµ‹æ•°æ®ç±»å‹"""
    try:
        json.loads(data)
        return "json"
    except:
        pass
    
    if '<table' in data.lower() or '<tr' in data.lower():
        return "table"
    
    return "text"


def _parse_data(raw_data: str, data_type: str) -> Any:
    """è§£ææ•°æ®"""
    if data_type == "json":
        try:
            return json.loads(raw_data)
        except:
            return {"text": raw_data}
    elif data_type == "table":
        # ç®€å•çš„è¡¨æ ¼è§£æ
        return {"text": raw_data, "type": "table"}
    else:
        return {"text": raw_data, "type": "text"}


def _analyze_player_data(data: Dict) -> Dict:
    """åˆ†æçƒå‘˜æ•°æ®"""
    return {
        "data_summary": {
            "analysis_type": "çƒå‘˜åˆ†æ",
            "data_points": len(str(data))
        },
        "key_metrics": {
            "metric_type": "çƒå‘˜è¡¨ç°æŒ‡æ ‡",
            "categories": ["è¿›çƒ", "åŠ©æ”»", "è¯„åˆ†", "å‡ºåœºæ¬¡æ•°"]
        },
        "insights": [
            "çƒå‘˜æ•°æ®åˆ†æéœ€è¦åŸºäºå…·ä½“çš„ç»Ÿè®¡æ•°æ®",
            "å»ºè®®å…³æ³¨å…³é”®æŒ‡æ ‡å¦‚è¿›çƒã€åŠ©æ”»ã€è¯„åˆ†ç­‰"
        ],
        "recommendations": [
            "æ”¶é›†æ›´å¤šæ¯”èµ›æ•°æ®ä»¥è¿›è¡Œæ·±åº¦åˆ†æ",
            "å¯¹æ¯”åŒä½ç½®çƒå‘˜æ•°æ®"
        ]
    }


def _analyze_team_data(data: Dict) -> Dict:
    """åˆ†æçƒé˜Ÿæ•°æ®"""
    return {
        "data_summary": {
            "analysis_type": "çƒé˜Ÿåˆ†æ",
            "data_points": len(str(data))
        },
        "key_metrics": {
            "metric_type": "çƒé˜Ÿè¡¨ç°æŒ‡æ ‡",
            "categories": ["ç§¯åˆ†", "æ’å", "èƒœç‡", "è¿›å¤±çƒ"]
        },
        "insights": [
            "çƒé˜Ÿæ•°æ®åˆ†æéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªç»´åº¦",
            "å»ºè®®å…³æ³¨ç§¯åˆ†æ¦œæ’åå’Œè¿‘æœŸæˆ˜ç»©"
        ],
        "recommendations": [
            "åˆ†æä¸»å®¢åœºæ•°æ®å·®å¼‚",
            "å…³æ³¨çƒé˜Ÿé˜µå®¹å˜åŒ–"
        ]
    }


def _analyze_match_data(data: Dict) -> Dict:
    """åˆ†ææ¯”èµ›æ•°æ®"""
    return {
        "data_summary": {
            "analysis_type": "æ¯”èµ›åˆ†æ",
            "data_points": len(str(data))
        },
        "key_metrics": {
            "metric_type": "æ¯”èµ›å…³é”®æ•°æ®",
            "categories": ["æ¯”åˆ†", "æ§çƒç‡", "å°„é—¨", "ä¼ çƒ"]
        },
        "insights": [
            "æ¯”èµ›æ•°æ®åˆ†æåº”å…³æ³¨å…³é”®äº‹ä»¶å’Œç»Ÿè®¡",
            "å»ºè®®å¯¹æ¯”åŒæ–¹æ•°æ®ä¼˜åŠ¿"
        ],
        "recommendations": [
            "åˆ†ææ¯”èµ›èŠ‚å¥å’Œå…³é”®æ—¶åˆ»",
            "å…³æ³¨çƒå‘˜ä¸ªäººè¡¨ç°"
        ]
    }


def _analyze_statistics(data: Dict) -> Dict:
    """ç»Ÿè®¡åˆ†æ"""
    text = str(data)
    numbers = re.findall(r'\d+\.?\d*', text)
    
    return {
        "data_summary": {
            "analysis_type": "ç»Ÿè®¡åˆ†æ",
            "data_points": len(text),
            "numeric_values": len(numbers)
        },
        "key_metrics": {
            "total_numbers": len(numbers),
            "sample_values": numbers[:10] if numbers else []
        },
        "statistical_analysis": {
            "count": len(numbers),
            "note": "è¯¦ç»†ç»Ÿè®¡éœ€è¦ç»“æ„åŒ–æ•°æ®"
        },
        "insights": [
            f"æ•°æ®ä¸­åŒ…å« {len(numbers)} ä¸ªæ•°å€¼",
            "å»ºè®®ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼è¿›è¡Œæ·±åº¦ç»Ÿè®¡åˆ†æ"
        ]
    }


def _analyze_trend(data: Dict) -> Dict:
    """è¶‹åŠ¿åˆ†æ"""
    return {
        "data_summary": {
            "analysis_type": "è¶‹åŠ¿åˆ†æ",
            "data_points": len(str(data))
        },
        "key_metrics": {
            "metric_type": "æ—¶é—´åºåˆ—æŒ‡æ ‡",
            "note": "éœ€è¦æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æ"
        },
        "insights": [
            "è¶‹åŠ¿åˆ†æéœ€è¦åŒ…å«æ—¶é—´ç»´åº¦çš„æ•°æ®",
            "å»ºè®®æ”¶é›†å¤šä¸ªæ—¶é—´ç‚¹çš„æ•°æ®"
        ],
        "recommendations": [
            "æŒ‰æ—¶é—´é¡ºåºç»„ç»‡æ•°æ®",
            "è¯†åˆ«ä¸Šå‡æˆ–ä¸‹é™è¶‹åŠ¿"
        ]
    }


def _analyze_comprehensive(data: Dict) -> Dict:
    """ç»¼åˆåˆ†æ"""
    text = str(data)
    
    return {
        "data_summary": {
            "analysis_type": "ç»¼åˆåˆ†æ",
            "data_size": len(text),
            "data_structure": type(data).__name__
        },
        "key_metrics": {
            "content_length": len(text),
            "has_structure": isinstance(data, dict)
        },
        "insights": [
            "æ•°æ®å·²æ¥æ”¶å¹¶è¿›è¡Œåˆæ­¥åˆ†æ",
            "å»ºè®®æ ¹æ®å…·ä½“åˆ†æéœ€æ±‚é€‰æ‹©ä¸“é—¨çš„åˆ†æç»´åº¦"
        ],
        "recommendations": [
            "æ˜ç¡®åˆ†æç›®æ ‡å’Œç»´åº¦",
            "ä½¿ç”¨ç»“æ„åŒ–æ•°æ®æ ¼å¼"
        ]
    }


def _generate_qa_answer_report(data: Dict, include_sources: bool) -> Dict:
    """ç”Ÿæˆé—®ç­”ç­”æ¡ˆæŠ¥å‘Š"""
    report = {
        "report_type": "é—®ç­”ç­”æ¡ˆ",
        "generated_time": datetime.now().isoformat(),
        "answer": {
            "summary": "åŸºäºæ”¶é›†çš„æ•°æ®ç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦",
            "details": data.get("insights", []),
            "confidence": "high"
        },
        "supporting_data": {
            "key_metrics": data.get("key_metrics", {}),
            "statistical_analysis": data.get("statistical_analysis", {})
        }
    }
    
    if include_sources:
        report["sources"] = {
            "data_sources": ["æœç´¢å¼•æ“", "å®˜æ–¹ç½‘ç«™", "ä½“è‚²æ•°æ®å¹³å°"],
            "collection_time": data.get("analysis_metadata", {}).get("analysis_time", ""),
            "reliability": "éœ€è¦éªŒè¯"
        }
    
    return report


def _generate_summary_report(data: Dict, include_sources: bool) -> Dict:
    """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
    return {
        "report_type": "æ‘˜è¦æŠ¥å‘Š",
        "generated_time": datetime.now().isoformat(),
        "summary": data.get("data_summary", {}),
        "key_points": data.get("insights", [])[:5],
        "sources_included": include_sources
    }


def _generate_detailed_report(data: Dict, include_sources: bool) -> Dict:
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
    return {
        "report_type": "è¯¦ç»†æŠ¥å‘Š",
        "generated_time": datetime.now().isoformat(),
        "full_analysis": data,
        "sections": {
            "metadata": data.get("analysis_metadata", {}),
            "summary": data.get("data_summary", {}),
            "metrics": data.get("key_metrics", {}),
            "statistics": data.get("statistical_analysis", {}),
            "insights": data.get("insights", []),
            "recommendations": data.get("recommendations", [])
        },
        "sources_included": include_sources
    }


def _generate_comparison_report(data: Dict, include_sources: bool) -> Dict:
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    return {
        "report_type": "å¯¹æ¯”æŠ¥å‘Š",
        "generated_time": datetime.now().isoformat(),
        "comparison": {
            "note": "å¯¹æ¯”æŠ¥å‘Šéœ€è¦å¤šç»„æ•°æ®",
            "data_provided": 1
        },
        "recommendation": "è¯·æä¾›è‡³å°‘ä¸¤ç»„æ•°æ®è¿›è¡Œå¯¹æ¯”",
        "sources_included": include_sources
    }


def _generate_ranking_report(data: Dict, include_sources: bool) -> Dict:
    """ç”Ÿæˆæ’åæŠ¥å‘Š"""
    return {
        "report_type": "æ’åæŠ¥å‘Š",
        "generated_time": datetime.now().isoformat(),
        "ranking": {
            "note": "æ’åæŠ¥å‘Šéœ€è¦åŒ…å«æ’åæ•°æ®",
            "data_analysis": data.get("key_metrics", {})
        },
        "sources_included": include_sources
    }


def _format_as_markdown(report: Dict) -> str:
    """æ ¼å¼åŒ–ä¸ºMarkdown"""
    md = f"# {report.get('report_type', 'æŠ¥å‘Š')}\n\n"
    md += f"**ç”Ÿæˆæ—¶é—´**: {report.get('generated_time', '')}\n\n"
    
    if 'answer' in report:
        md += "## ç­”æ¡ˆ\n\n"
        md += f"{report['answer'].get('summary', '')}\n\n"
        
        if 'details' in report['answer']:
            md += "### è¯¦ç»†ä¿¡æ¯\n\n"
            for detail in report['answer']['details']:
                md += f"- {detail}\n"
            md += "\n"
    
    if 'sources' in report:
        md += "## ä¿¡æ¯æ¥æº\n\n"
        for source in report['sources'].get('data_sources', []):
            md += f"- {source}\n"
    
    return md


def _format_as_html(report: Dict) -> str:
    """æ ¼å¼åŒ–ä¸ºHTML"""
    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{report.get('report_type', 'æŠ¥å‘Š')}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        .section {{ margin: 20px 0; }}
    </style>
</head>
<body>
    <h1>{report.get('report_type', 'æŠ¥å‘Š')}</h1>
    <p><strong>ç”Ÿæˆæ—¶é—´</strong>: {report.get('generated_time', '')}</p>
"""
    
    if 'answer' in report:
        html += f"""
    <div class="section">
        <h2>ç­”æ¡ˆ</h2>
        <p>{report['answer'].get('summary', '')}</p>
    </div>
"""
    
    html += """
</body>
</html>
"""
    return html


def _format_as_text(report: Dict) -> str:
    """æ ¼å¼åŒ–ä¸ºçº¯æ–‡æœ¬"""
    text = f"{report.get('report_type', 'æŠ¥å‘Š')}\n"
    text += "=" * 50 + "\n\n"
    text += f"ç”Ÿæˆæ—¶é—´: {report.get('generated_time', '')}\n\n"
    
    if 'answer' in report:
        text += "ç­”æ¡ˆ:\n"
        text += f"{report['answer'].get('summary', '')}\n\n"
    
    return text


if __name__ == "__main__":
    print("âœ… è¶³çƒé—®ç­”æœç´¢æ•´ç†å·¥å…·é›†åŠ è½½æˆåŠŸ")
    print("ğŸ“¦ åŒ…å«å·¥å…·:")
    print("  1. web_search_enhanced - å¢å¼ºçš„ç½‘ç»œæœç´¢")
    print("  2. information_collector - ä¿¡æ¯æ”¶é›†")
    print("  3. data_analyzer - æ•°æ®åˆ†æ")
    print("  4. report_generator - æŠ¥å‘Šç”Ÿæˆ")
    print("  5. http_request - HTTPè¯·æ±‚")
    print("\nğŸ’¡ æç¤º: current_timeå·¥å…·è¯·ä½¿ç”¨Strandsæ¡†æ¶çš„å†…ç½®å·¥å…·")
