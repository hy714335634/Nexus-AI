#!/usr/bin/env python3
"""
è¯ç‰©åé¦ˆæ”¶é›†å·¥å…·æ¨¡å—

æä¾›è¯ç‰©åé¦ˆæ”¶é›†æ‰€éœ€çš„æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
1. ç½‘ç»œæœç´¢å·¥å…·ï¼ˆä½¿ç”¨DuckDuckGoï¼‰
2. ç½‘é¡µæŠ“å–å·¥å…·
3. å†…å®¹åˆ†æå·¥å…·ï¼ˆä½¿ç”¨Claudeï¼‰
4. æŠ¥å‘Šç”Ÿæˆå·¥å…·
5. ç¼“å­˜ç®¡ç†å·¥å…·
"""

import json
import hashlib
import os
import re
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urlparse, quote_plus
import traceback

from strands import tool

# DuckDuckGoæœç´¢ï¼ˆæ›¿ä»£Tavilyï¼‰
try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS_AVAILABLE = False

# ç½‘é¡µæŠ“å–
try:
    import requests
    from bs4 import BeautifulSoup
    SCRAPING_AVAILABLE = True
except ImportError:
    SCRAPING_AVAILABLE = False

# AWS Bedrockç”¨äºAIåˆ†æ
try:
    import boto3
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False


# ==================== å·¥å…·1: è¯ç‰©åç§°éªŒè¯å’Œæ ‡å‡†åŒ– ====================

@tool
def validate_drug_name(drug_name: str) -> str:
    """
    éªŒè¯å’Œæ ‡å‡†åŒ–è¯ç‰©åç§°
    
    Args:
        drug_name (str): ç”¨æˆ·è¾“å…¥çš„è¯ç‰©åç§°
        
    Returns:
        str: JSONæ ¼å¼çš„éªŒè¯ç»“æœï¼ŒåŒ…å«æ ‡å‡†åŒ–åç§°å’Œå˜ä½“
    """
    try:
        if not drug_name or not drug_name.strip():
            return json.dumps({
                "status": "error",
                "message": "è¯·æä¾›æœ‰æ•ˆçš„è¯ç‰©åç§°ã€‚ç¤ºä¾‹ï¼šé˜¿å¸åŒ¹æ—ã€å¸ƒæ´›èŠ¬ã€Aspirin",
                "valid": False
            }, ensure_ascii=False)
        
        # æ¸…ç†è¾“å…¥
        cleaned_name = drug_name.strip()
        
        # ç”Ÿæˆè¯ç‰©åç§°å˜ä½“ï¼ˆç”¨äºæœç´¢ï¼‰
        variants = [cleaned_name]
        
        # æ·»åŠ å¸¸è§åç¼€å˜ä½“
        common_suffixes = ["ç‰‡", "èƒ¶å›Š", "æ³¨å°„æ¶²", "é¢—ç²’", "ä¸¸", "ç¼“é‡Šç‰‡", "è‚ æº¶ç‰‡"]
        for suffix in common_suffixes:
            if cleaned_name.endswith(suffix):
                base_name = cleaned_name[:-len(suffix)]
                variants.append(base_name)
                break
            else:
                variants.append(f"{cleaned_name}{suffix}")
        
        # æ·»åŠ è‹±æ–‡/ä¸­æ–‡å˜ä½“ï¼ˆç®€å•è§„åˆ™ï¼‰
        if re.match(r'^[a-zA-Z\s]+$', cleaned_name):
            # è‹±æ–‡åç§°ï¼Œæ·»åŠ å¸¸è§ä¸­æ–‡ç¿»è¯‘æç¤º
            variants.append(f"{cleaned_name} ä¸­æ–‡å")
        elif re.match(r'^[\u4e00-\u9fa5]+$', cleaned_name):
            # ä¸­æ–‡åç§°ï¼Œæ·»åŠ è‹±æ–‡ç¿»è¯‘æç¤º
            variants.append(f"{cleaned_name} è‹±æ–‡å")
        
        result = {
            "status": "success",
            "valid": True,
            "original_name": drug_name,
            "standardized_name": cleaned_name,
            "search_variants": list(set(variants)),
            "validation_time": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"è¯ç‰©åç§°éªŒè¯å¤±è´¥: {str(e)}",
            "valid": False
        }, ensure_ascii=False)


# ==================== å·¥å…·2: ç½‘ç»œæœç´¢å·¥å…· ====================

@tool
def search_drug_feedback(
    drug_name: str,
    search_keywords: List[str] = None,
    max_results_per_keyword: int = 10
) -> str:
    """
    ä½¿ç”¨DuckDuckGoæœç´¢è¯ç‰©åé¦ˆä¿¡æ¯
    
    Args:
        drug_name (str): è¯ç‰©åç§°
        search_keywords (List[str]): æœç´¢å…³é”®è¯åˆ—è¡¨ï¼Œé»˜è®¤ä¸º["è¯„ä»·", "å‰¯ä½œç”¨", "ä½“éªŒ", "åé¦ˆ"]
        max_results_per_keyword (int): æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
        
    Returns:
        str: JSONæ ¼å¼çš„æœç´¢ç»“æœ
    """
    try:
        if not DDGS_AVAILABLE:
            return json.dumps({
                "status": "error",
                "message": "DuckDuckGoæœç´¢åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install duckduckgo-search"
            }, ensure_ascii=False)
        
        if search_keywords is None:
            search_keywords = ["è¯„ä»·", "å‰¯ä½œç”¨", "ä½“éªŒ", "åé¦ˆ", "æ•ˆæœ"]
        
        all_results = []
        search_metadata = {
            "drug_name": drug_name,
            "keywords_used": search_keywords,
            "search_time": datetime.now().isoformat(),
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0
        }
        
        ddgs = DDGS()
        
        for keyword in search_keywords:
            query = f"{drug_name} {keyword}"
            search_metadata["total_queries"] += 1
            
            try:
                # æ‰§è¡Œæœç´¢
                results = ddgs.text(query, max_results=max_results_per_keyword)
                
                if results:
                    search_metadata["successful_queries"] += 1
                    for result in results:
                        all_results.append({
                            "keyword": keyword,
                            "title": result.get("title", ""),
                            "url": result.get("href", ""),
                            "snippet": result.get("body", ""),
                            "source": urlparse(result.get("href", "")).netloc
                        })
                
                # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                time.sleep(0.5)
                
            except Exception as e:
                search_metadata["failed_queries"] += 1
                print(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                continue
        
        return json.dumps({
            "status": "success",
            "metadata": search_metadata,
            "results": all_results,
            "total_results": len(all_results)
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æœç´¢å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


# ==================== å·¥å…·3: ç½‘é¡µå†…å®¹æŠ“å–å·¥å…· ====================

@tool
def extract_webpage_content(
    url: str,
    max_content_length: int = 10240,
    timeout: int = 10
) -> str:
    """
    æŠ“å–ç½‘é¡µå†…å®¹å¹¶æå–æ­£æ–‡
    
    Args:
        url (str): ç½‘é¡µURL
        max_content_length (int): æœ€å¤§å†…å®¹é•¿åº¦ï¼ˆå­—èŠ‚ï¼‰
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„æŠ“å–ç»“æœ
    """
    try:
        if not SCRAPING_AVAILABLE:
            return json.dumps({
                "status": "error",
                "message": "ç½‘é¡µæŠ“å–åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install requests beautifulsoup4 lxml"
            }, ensure_ascii=False)
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'lxml')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # æå–æ­£æ–‡å†…å®¹
        # ä¼˜å…ˆæå–articleã€mainã€contentç­‰ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = None
        for tag in ['article', 'main', '[role="main"]', '.content', '#content']:
            main_content = soup.select_one(tag)
            if main_content:
                break
        
        if not main_content:
            main_content = soup.body if soup.body else soup
        
        # æå–æ–‡æœ¬
        text = main_content.get_text(separator="\n", strip=True)
        
        # æ¸…ç†æ–‡æœ¬
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        cleaned_text = "\n".join(lines)
        
        # é™åˆ¶é•¿åº¦
        if len(cleaned_text) > max_content_length:
            cleaned_text = cleaned_text[:max_content_length] + "..."
        
        # æå–æ ‡é¢˜
        title = soup.title.string if soup.title else ""
        
        result = {
            "status": "success",
            "url": url,
            "title": title,
            "content": cleaned_text,
            "content_length": len(cleaned_text),
            "extraction_time": datetime.now().isoformat(),
            "source_domain": urlparse(url).netloc
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except requests.Timeout:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰"
        }, ensure_ascii=False)
    except requests.RequestException as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        }, ensure_ascii=False)
    except Exception as e:
        return json.dumps({
            "status": "error",
            "url": url,
            "message": f"å†…å®¹æå–å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


@tool
def batch_extract_webpages(
    urls: List[str],
    max_content_length: int = 10240,
    timeout: int = 10,
    max_concurrent: int = 5
) -> str:
    """
    æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µå†…å®¹
    
    Args:
        urls (List[str]): ç½‘é¡µURLåˆ—è¡¨
        max_content_length (int): æ¯ä¸ªç½‘é¡µçš„æœ€å¤§å†…å®¹é•¿åº¦
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´
        max_concurrent (int): æœ€å¤§å¹¶å‘æ•°ï¼ˆå½“å‰ç‰ˆæœ¬é¡ºåºæ‰§è¡Œï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„æ‰¹é‡æŠ“å–ç»“æœ
    """
    try:
        results = []
        successful = 0
        failed = 0
        
        for url in urls[:max_concurrent]:  # é™åˆ¶å¹¶å‘æ•°
            result_json = extract_webpage_content(url, max_content_length, timeout)
            result = json.loads(result_json)
            
            if result["status"] == "success":
                successful += 1
            else:
                failed += 1
            
            results.append(result)
            
            # é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
            time.sleep(1)
        
        return json.dumps({
            "status": "success",
            "total_urls": len(urls),
            "processed_urls": len(results),
            "successful": successful,
            "failed": failed,
            "results": results,
            "extraction_time": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æ‰¹é‡æŠ“å–å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


# ==================== å·¥å…·4: AIå†…å®¹åˆ†æå·¥å…· ====================

@tool
def analyze_feedback_with_claude(
    content: str,
    drug_name: str,
    analysis_type: str = "comprehensive"
) -> str:
    """
    ä½¿ç”¨Claudeæ¨¡å‹åˆ†æè¯ç‰©åé¦ˆå†…å®¹
    
    Args:
        content (str): è¦åˆ†æçš„æ–‡æœ¬å†…å®¹
        drug_name (str): è¯ç‰©åç§°
        analysis_type (str): åˆ†æç±»å‹ï¼ˆbasic/comprehensive/detailedï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        if not BOTO3_AVAILABLE:
            return json.dumps({
                "status": "error",
                "message": "boto3åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install boto3"
            }, ensure_ascii=False)
        
        # åˆ›å»ºBedrockå®¢æˆ·ç«¯
        bedrock_runtime = boto3.client(
            service_name='bedrock-runtime',
            region_name='us-east-1'
        )
        
        # æ„å»ºåˆ†ææç¤ºè¯
        analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹å…³äºè¯ç‰©"{drug_name}"çš„ç”¨æˆ·åé¦ˆå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶åˆ†ç±»ã€‚

å†…å®¹ï¼š
{content[:8000]}  # é™åˆ¶è¾“å…¥é•¿åº¦

è¯·æŒ‰ä»¥ä¸‹ç»“æ„æå–ä¿¡æ¯ï¼š
1. åé¦ˆç±»å‹ï¼šç–—æ•ˆè¯„ä»·ã€å‰¯ä½œç”¨ã€ä½¿ç”¨ä½“éªŒã€ä»·æ ¼è¯„ä»·
2. æƒ…æ„Ÿå€¾å‘ï¼šæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
3. å…³é”®ä¿¡æ¯ï¼šæå–å…·ä½“çš„ç–—æ•ˆæè¿°ã€å‰¯ä½œç”¨æè¿°ã€ä½¿ç”¨ä½“éªŒæè¿°
4. å¯ä¿¡åº¦ï¼šæ ¹æ®æè¿°çš„å…·ä½“æ€§å’Œé€»è¾‘æ€§è¯„ä¼°ï¼ˆé«˜/ä¸­/ä½ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "feedback_type": "ç–—æ•ˆè¯„ä»·/å‰¯ä½œç”¨/ä½¿ç”¨ä½“éªŒ/ä»·æ ¼è¯„ä»·",
  "sentiment": "positive/negative/neutral",
  "efficacy": {{"description": "ç–—æ•ˆæè¿°", "mentioned": true/false}},
  "side_effects": {{"description": "å‰¯ä½œç”¨æè¿°", "mentioned": true/false}},
  "experience": {{"description": "ä½¿ç”¨ä½“éªŒæè¿°", "mentioned": true/false}},
  "price": {{"description": "ä»·æ ¼è¯„ä»·", "mentioned": true/false}},
  "credibility": "high/medium/low",
  "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2"]
}}
"""
        
        # è°ƒç”¨Claudeæ¨¡å‹
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 2000,
            "messages": [
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            "temperature": 0.3
        }
        
        response = bedrock_runtime.invoke_model(
            modelId="anthropic.claude-3-sonnet-20240229-v1:0",
            body=json.dumps(request_body)
        )
        
        response_body = json.loads(response['body'].read())
        analysis_text = response_body['content'][0]['text']
        
        # å°è¯•è§£æJSONç»“æœ
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{[\s\S]*\}', analysis_text)
            if json_match:
                analysis_result = json.loads(json_match.group())
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                analysis_result = {
                    "feedback_type": "unknown",
                    "sentiment": "neutral",
                    "raw_analysis": analysis_text,
                    "parsing_note": "æ— æ³•è§£æä¸ºç»“æ„åŒ–JSONï¼Œè¿”å›åŸå§‹åˆ†æ"
                }
        except:
            analysis_result = {
                "feedback_type": "unknown",
                "sentiment": "neutral",
                "raw_analysis": analysis_text,
                "parsing_note": "JSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ†æ"
            }
        
        return json.dumps({
            "status": "success",
            "drug_name": drug_name,
            "analysis_type": analysis_type,
            "analysis_result": analysis_result,
            "analysis_time": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"AIåˆ†æå¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


@tool
def batch_analyze_feedback(
    contents: List[Dict[str, str]],
    drug_name: str
) -> str:
    """
    æ‰¹é‡åˆ†æå¤šä¸ªåé¦ˆå†…å®¹
    
    Args:
        contents (List[Dict[str, str]]): å†…å®¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«urlå’Œcontent
        drug_name (str): è¯ç‰©åç§°
        
    Returns:
        str: JSONæ ¼å¼çš„æ‰¹é‡åˆ†æç»“æœ
    """
    try:
        analyzed_results = []
        successful = 0
        failed = 0
        
        for item in contents:
            content = item.get("content", "")
            url = item.get("url", "")
            
            if not content:
                failed += 1
                continue
            
            # åˆ†æå•ä¸ªå†…å®¹
            result_json = analyze_feedback_with_claude(content, drug_name, "basic")
            result = json.loads(result_json)
            
            if result["status"] == "success":
                successful += 1
                result["source_url"] = url
                analyzed_results.append(result)
            else:
                failed += 1
            
            # é¿å…è§¦å‘APIé€Ÿç‡é™åˆ¶
            time.sleep(1)
        
        return json.dumps({
            "status": "success",
            "drug_name": drug_name,
            "total_items": len(contents),
            "successful": successful,
            "failed": failed,
            "results": analyzed_results,
            "analysis_time": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


# ==================== å·¥å…·5: åé¦ˆæŠ¥å‘Šç”Ÿæˆå·¥å…· ====================

@tool
def generate_feedback_report(
    drug_name: str,
    analyzed_feedbacks: List[Dict[str, Any]],
    include_sources: bool = True
) -> str:
    """
    ç”Ÿæˆè¯ç‰©åé¦ˆæŠ¥å‘Š
    
    Args:
        drug_name (str): è¯ç‰©åç§°
        analyzed_feedbacks (List[Dict]): å·²åˆ†æçš„åé¦ˆåˆ—è¡¨
        include_sources (bool): æ˜¯å¦åŒ…å«æ¥æºä¿¡æ¯
        
    Returns:
        str: JSONæ ¼å¼çš„åé¦ˆæŠ¥å‘Š
    """
    try:
        # ç»Ÿè®¡åˆ†æ
        total_feedbacks = len(analyzed_feedbacks)
        
        # æŒ‰æƒ…æ„Ÿåˆ†ç±»
        sentiment_stats = {"positive": 0, "negative": 0, "neutral": 0}
        
        # æŒ‰åé¦ˆç±»å‹åˆ†ç±»
        type_stats = {
            "ç–—æ•ˆè¯„ä»·": 0,
            "å‰¯ä½œç”¨": 0,
            "ä½¿ç”¨ä½“éªŒ": 0,
            "ä»·æ ¼è¯„ä»·": 0,
            "å…¶ä»–": 0
        }
        
        # æ”¶é›†å…·ä½“åé¦ˆ
        efficacy_feedbacks = []
        side_effect_feedbacks = []
        experience_feedbacks = []
        price_feedbacks = []
        
        # æ¥æºåˆ—è¡¨
        sources = []
        
        for feedback in analyzed_feedbacks:
            analysis = feedback.get("analysis_result", {})
            
            # ç»Ÿè®¡æƒ…æ„Ÿ
            sentiment = analysis.get("sentiment", "neutral")
            if sentiment in sentiment_stats:
                sentiment_stats[sentiment] += 1
            
            # ç»Ÿè®¡ç±»å‹
            feedback_type = analysis.get("feedback_type", "å…¶ä»–")
            if feedback_type in type_stats:
                type_stats[feedback_type] += 1
            else:
                type_stats["å…¶ä»–"] += 1
            
            # æ”¶é›†å…·ä½“åé¦ˆ
            if analysis.get("efficacy", {}).get("mentioned"):
                efficacy_feedbacks.append({
                    "description": analysis["efficacy"]["description"],
                    "sentiment": sentiment,
                    "source_url": feedback.get("source_url", "")
                })
            
            if analysis.get("side_effects", {}).get("mentioned"):
                side_effect_feedbacks.append({
                    "description": analysis["side_effects"]["description"],
                    "sentiment": sentiment,
                    "source_url": feedback.get("source_url", "")
                })
            
            if analysis.get("experience", {}).get("mentioned"):
                experience_feedbacks.append({
                    "description": analysis["experience"]["description"],
                    "sentiment": sentiment,
                    "source_url": feedback.get("source_url", "")
                })
            
            if analysis.get("price", {}).get("mentioned"):
                price_feedbacks.append({
                    "description": analysis["price"]["description"],
                    "sentiment": sentiment,
                    "source_url": feedback.get("source_url", "")
                })
            
            # æ”¶é›†æ¥æº
            if include_sources and feedback.get("source_url"):
                sources.append({
                    "url": feedback["source_url"],
                    "title": feedback.get("title", ""),
                    "credibility": analysis.get("credibility", "unknown")
                })
        
        # è®¡ç®—ç»Ÿè®¡ç™¾åˆ†æ¯”
        sentiment_percentages = {
            k: round(v / total_feedbacks * 100, 1) if total_feedbacks > 0 else 0
            for k, v in sentiment_stats.items()
        }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "report_metadata": {
                "drug_name": drug_name,
                "generation_time": datetime.now().isoformat(),
                "total_feedbacks_analyzed": total_feedbacks,
                "report_version": "1.0"
            },
            "summary": {
                "overall_sentiment": _determine_overall_sentiment(sentiment_stats),
                "sentiment_distribution": {
                    "positive": f"{sentiment_percentages['positive']}%",
                    "negative": f"{sentiment_percentages['negative']}%",
                    "neutral": f"{sentiment_percentages['neutral']}%"
                },
                "feedback_type_distribution": type_stats,
                "key_insights": [
                    f"å…±æ”¶é›†{total_feedbacks}æ¡åé¦ˆ",
                    f"æ­£é¢åé¦ˆå {sentiment_percentages['positive']}%",
                    f"è´Ÿé¢åé¦ˆå {sentiment_percentages['negative']}%",
                    f"æåˆ°ç–—æ•ˆçš„åé¦ˆæœ‰{len(efficacy_feedbacks)}æ¡",
                    f"æåˆ°å‰¯ä½œç”¨çš„åé¦ˆæœ‰{len(side_effect_feedbacks)}æ¡"
                ]
            },
            "detailed_feedback": {
                "efficacy": {
                    "count": len(efficacy_feedbacks),
                    "feedbacks": efficacy_feedbacks[:20]  # é™åˆ¶æ•°é‡
                },
                "side_effects": {
                    "count": len(side_effect_feedbacks),
                    "feedbacks": side_effect_feedbacks[:20]
                },
                "experience": {
                    "count": len(experience_feedbacks),
                    "feedbacks": experience_feedbacks[:20]
                },
                "price": {
                    "count": len(price_feedbacks),
                    "feedbacks": price_feedbacks[:20]
                }
            },
            "sources": sources if include_sources else [],
            "disclaimer": "æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€ç½‘ç»œä¿¡æ¯æ•´ç†ï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆåŒ»ç–—å»ºè®®ã€‚ä½¿ç”¨ä»»ä½•è¯ç‰©å‰è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚"
        }
        
        return json.dumps(report, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


def _determine_overall_sentiment(sentiment_stats: Dict[str, int]) -> str:
    """æ ¹æ®æƒ…æ„Ÿç»Ÿè®¡ç¡®å®šæ•´ä½“æƒ…æ„Ÿå€¾å‘"""
    total = sum(sentiment_stats.values())
    if total == 0:
        return "neutral"
    
    positive_ratio = sentiment_stats["positive"] / total
    negative_ratio = sentiment_stats["negative"] / total
    
    if positive_ratio > 0.6:
        return "positive"
    elif negative_ratio > 0.6:
        return "negative"
    else:
        return "mixed"


# ==================== å·¥å…·6: ç¼“å­˜ç®¡ç†å·¥å…· ====================

@tool
def check_cache(
    drug_name: str,
    cache_type: str = "report",
    max_age_days: int = 7
) -> str:
    """
    æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    
    Args:
        drug_name (str): è¯ç‰©åç§°
        cache_type (str): ç¼“å­˜ç±»å‹ï¼ˆsearch_results/reportï¼‰
        max_age_days (int): æœ€å¤§ç¼“å­˜å¤©æ•°
        
    Returns:
        str: JSONæ ¼å¼çš„ç¼“å­˜æ£€æŸ¥ç»“æœ
    """
    try:
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.sha256(drug_name.lower().strip().encode()).hexdigest()
        
        # ç¼“å­˜ç›®å½•
        cache_dir = Path(f".cache/drug_feedback_collector/{cache_key}")
        
        if not cache_dir.exists():
            return json.dumps({
                "status": "not_found",
                "cache_exists": False,
                "message": "ç¼“å­˜ä¸å­˜åœ¨"
            }, ensure_ascii=False)
        
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
        cache_file = cache_dir / f"{cache_type}.json"
        
        if not cache_file.exists():
            return json.dumps({
                "status": "not_found",
                "cache_exists": False,
                "message": f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_type}.json"
            }, ensure_ascii=False)
        
        # æ£€æŸ¥ç¼“å­˜æ—¶é—´
        cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
        cache_age = datetime.now() - cache_mtime
        
        if cache_age.days > max_age_days:
            return json.dumps({
                "status": "expired",
                "cache_exists": True,
                "cache_age_days": cache_age.days,
                "max_age_days": max_age_days,
                "message": f"ç¼“å­˜å·²è¿‡æœŸï¼ˆ{cache_age.days}å¤©ï¼‰"
            }, ensure_ascii=False)
        
        # è¯»å–ç¼“å­˜
        with open(cache_file, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
        
        return json.dumps({
            "status": "valid",
            "cache_exists": True,
            "cache_age_days": cache_age.days,
            "cache_path": str(cache_file),
            "cached_data": cached_data,
            "message": "ç¼“å­˜æœ‰æ•ˆ"
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


@tool
def save_to_cache(
    drug_name: str,
    data: Dict[str, Any],
    cache_type: str = "report"
) -> str:
    """
    ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
    
    Args:
        drug_name (str): è¯ç‰©åç§°
        data (Dict): è¦ç¼“å­˜çš„æ•°æ®
        cache_type (str): ç¼“å­˜ç±»å‹ï¼ˆsearch_results/reportï¼‰
        
    Returns:
        str: JSONæ ¼å¼çš„ä¿å­˜ç»“æœ
    """
    try:
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.sha256(drug_name.lower().strip().encode()).hexdigest()
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_dir = Path(f".cache/drug_feedback_collector/{cache_key}")
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ç¼“å­˜æ–‡ä»¶
        cache_file = cache_dir / f"{cache_type}.json"
        
        # æ·»åŠ å…ƒæ•°æ®
        cache_data = {
            "drug_name": drug_name,
            "cache_type": cache_type,
            "cached_time": datetime.now().isoformat(),
            "data": data
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        return json.dumps({
            "status": "success",
            "cache_path": str(cache_file),
            "cache_size": cache_file.stat().st_size,
            "message": "ç¼“å­˜ä¿å­˜æˆåŠŸ"
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


@tool
def clear_cache(
    drug_name: str = None,
    older_than_days: int = None
) -> str:
    """
    æ¸…ç†ç¼“å­˜
    
    Args:
        drug_name (str): ç‰¹å®šè¯ç‰©åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™æ¸…ç†æ‰€æœ‰ç¼“å­˜
        older_than_days (int): æ¸…ç†æŒ‡å®šå¤©æ•°ä¹‹å‰çš„ç¼“å­˜
        
    Returns:
        str: JSONæ ¼å¼çš„æ¸…ç†ç»“æœ
    """
    try:
        cache_base_dir = Path(".cache/drug_feedback_collector")
        
        if not cache_base_dir.exists():
            return json.dumps({
                "status": "success",
                "message": "ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†",
                "deleted_count": 0
            }, ensure_ascii=False)
        
        deleted_count = 0
        deleted_items = []
        
        if drug_name:
            # æ¸…ç†ç‰¹å®šè¯ç‰©çš„ç¼“å­˜
            cache_key = hashlib.sha256(drug_name.lower().strip().encode()).hexdigest()
            cache_dir = cache_base_dir / cache_key
            
            if cache_dir.exists():
                import shutil
                shutil.rmtree(cache_dir)
                deleted_count = 1
                deleted_items.append(drug_name)
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜æˆ–è¿‡æœŸç¼“å­˜
            for cache_dir in cache_base_dir.iterdir():
                if cache_dir.is_dir():
                    should_delete = False
                    
                    if older_than_days is not None:
                        # æ£€æŸ¥ç¼“å­˜å¹´é¾„
                        report_file = cache_dir / "report.json"
                        if report_file.exists():
                            cache_mtime = datetime.fromtimestamp(report_file.stat().st_mtime)
                            cache_age = datetime.now() - cache_mtime
                            if cache_age.days > older_than_days:
                                should_delete = True
                    else:
                        # æ¸…ç†æ‰€æœ‰ç¼“å­˜
                        should_delete = True
                    
                    if should_delete:
                        import shutil
                        shutil.rmtree(cache_dir)
                        deleted_count += 1
                        deleted_items.append(cache_dir.name)
        
        return json.dumps({
            "status": "success",
            "deleted_count": deleted_count,
            "deleted_items": deleted_items,
            "message": f"æˆåŠŸæ¸…ç†{deleted_count}ä¸ªç¼“å­˜é¡¹"
        }, ensure_ascii=False)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


@tool
def get_cache_statistics() -> str:
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        str: JSONæ ¼å¼çš„ç¼“å­˜ç»Ÿè®¡
    """
    try:
        cache_base_dir = Path(".cache/drug_feedback_collector")
        
        if not cache_base_dir.exists():
            return json.dumps({
                "status": "success",
                "total_cached_drugs": 0,
                "total_cache_size": 0,
                "message": "ç¼“å­˜ç›®å½•ä¸å­˜åœ¨"
            }, ensure_ascii=False)
        
        total_drugs = 0
        total_size = 0
        cache_items = []
        
        for cache_dir in cache_base_dir.iterdir():
            if cache_dir.is_dir():
                total_drugs += 1
                
                # è®¡ç®—ç›®å½•å¤§å°
                dir_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
                total_size += dir_size
                
                # è·å–è¯ç‰©åç§°å’Œç¼“å­˜æ—¶é—´
                report_file = cache_dir / "report.json"
                if report_file.exists():
                    with open(report_file, 'r', encoding='utf-8') as f:
                        report_data = json.load(f)
                        cache_items.append({
                            "drug_name": report_data.get("drug_name", "unknown"),
                            "cached_time": report_data.get("cached_time", "unknown"),
                            "cache_size": dir_size,
                            "cache_key": cache_dir.name
                        })
        
        return json.dumps({
            "status": "success",
            "total_cached_drugs": total_drugs,
            "total_cache_size": total_size,
            "total_cache_size_mb": round(total_size / 1024 / 1024, 2),
            "cache_items": cache_items,
            "cache_base_dir": str(cache_base_dir)
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}",
            "traceback": traceback.format_exc()
        }, ensure_ascii=False)


# ==================== è¾…åŠ©å‡½æ•° ====================

def _format_markdown_report(report: Dict[str, Any]) -> str:
    """å°†æŠ¥å‘Šè½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    md = f"""# {report['report_metadata']['drug_name']} - ç”¨æˆ·åé¦ˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {report['report_metadata']['generation_time']}  
**åˆ†æåé¦ˆæ•°**: {report['report_metadata']['total_feedbacks_analyzed']}æ¡

---

## ğŸ“Š æ•´ä½“è¯„ä»·

**æ€»ä½“æƒ…æ„Ÿå€¾å‘**: {report['summary']['overall_sentiment']}

### æƒ…æ„Ÿåˆ†å¸ƒ
- æ­£é¢åé¦ˆ: {report['summary']['sentiment_distribution']['positive']}
- è´Ÿé¢åé¦ˆ: {report['summary']['sentiment_distribution']['negative']}
- ä¸­æ€§åé¦ˆ: {report['summary']['sentiment_distribution']['neutral']}

### å…³é”®æ´å¯Ÿ
"""
    for insight in report['summary']['key_insights']:
        md += f"- {insight}\n"
    
    md += "\n---\n\n## ğŸ’Š ç–—æ•ˆåé¦ˆ\n\n"
    efficacy = report['detailed_feedback']['efficacy']
    md += f"å…±{efficacy['count']}æ¡åé¦ˆæåˆ°ç–—æ•ˆ\n\n"
    
    for i, feedback in enumerate(efficacy['feedbacks'][:10], 1):
        sentiment_emoji = "ğŸ‘" if feedback['sentiment'] == "positive" else "ğŸ‘" if feedback['sentiment'] == "negative" else "ğŸ˜"
        md += f"{i}. {sentiment_emoji} {feedback['description']}\n"
        if feedback.get('source_url'):
            md += f"   - æ¥æº: {feedback['source_url']}\n"
    
    md += "\n---\n\n## âš ï¸ å‰¯ä½œç”¨åé¦ˆ\n\n"
    side_effects = report['detailed_feedback']['side_effects']
    md += f"å…±{side_effects['count']}æ¡åé¦ˆæåˆ°å‰¯ä½œç”¨\n\n"
    
    for i, feedback in enumerate(side_effects['feedbacks'][:10], 1):
        md += f"{i}. {feedback['description']}\n"
        if feedback.get('source_url'):
            md += f"   - æ¥æº: {feedback['source_url']}\n"
    
    md += "\n---\n\n## ğŸ’¬ ä½¿ç”¨ä½“éªŒ\n\n"
    experience = report['detailed_feedback']['experience']
    md += f"å…±{experience['count']}æ¡åé¦ˆæåˆ°ä½¿ç”¨ä½“éªŒ\n\n"
    
    for i, feedback in enumerate(experience['feedbacks'][:10], 1):
        sentiment_emoji = "ğŸ‘" if feedback['sentiment'] == "positive" else "ğŸ‘" if feedback['sentiment'] == "negative" else "ğŸ˜"
        md += f"{i}. {sentiment_emoji} {feedback['description']}\n"
        if feedback.get('source_url'):
            md += f"   - æ¥æº: {feedback['source_url']}\n"
    
    md += f"\n---\n\n## ğŸ“Œ å…è´£å£°æ˜\n\n{report['disclaimer']}\n"
    
    return md


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•è¯ç‰©åé¦ˆæ”¶é›†å·¥å…·...")
    
    # æµ‹è¯•è¯ç‰©åç§°éªŒè¯
    validation_result = validate_drug_name("é˜¿å¸åŒ¹æ—")
    print("âœ… è¯ç‰©åç§°éªŒè¯:", json.loads(validation_result)["standardized_name"])
    
    # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
    stats_result = get_cache_statistics()
    print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡:", json.loads(stats_result)["total_cached_drugs"])
    
    print("âœ… å·¥å…·æµ‹è¯•å®Œæˆï¼")
