#!/usr/bin/env python3
"""
Excelæ•°æ®å¤„ç†å·¥å…·æ¨¡å—

æä¾›Excelæ–‡ä»¶è¯»å–ã€æ•°æ®åˆ†æå’Œç¼“å­˜ç®¡ç†åŠŸèƒ½ã€‚
æ‰€æœ‰å·¥å…·å‡½æ•°éµå¾ªStrandsæ¡†æ¶è§„èŒƒï¼Œä½¿ç”¨@toolè£…é¥°å™¨ã€‚
"""

import json
import os
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import pandas as pd
import numpy as np
from openpyxl import load_workbook

from strands import tool


@tool
def excel_data_reader(
    file_path: str,
    sheet_name: Optional[str] = None,
    header_row: int = 0,
    skip_rows: Optional[List[int]] = None
) -> str:
    """
    Excelæ–‡ä»¶è¯»å–å·¥å…·
    
    åŠŸèƒ½ï¼šè¯»å–.xlsxå’Œ.xlsæ ¼å¼çš„Excelæ–‡ä»¶ï¼Œæ”¯æŒå¤šå·¥ä½œè¡¨è¯†åˆ«ã€ç©ºå€¼å¤„ç†ã€å¼‚å¸¸å€¼è®°å½•
    
    Args:
        file_path (str): Excelæ–‡ä»¶è·¯å¾„
        sheet_name (Optional[str]): å·¥ä½œè¡¨åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        header_row (int): è¡¨å¤´è¡Œå·ï¼Œé»˜è®¤ä¸º0ï¼ˆç¬¬ä¸€è¡Œï¼‰
        skip_rows (Optional[List[int]]): è¦è·³è¿‡çš„è¡Œå·åˆ—è¡¨
        
    Returns:
        str: JSONæ ¼å¼çš„è¯»å–ç»“æœï¼ŒåŒ…å«DataFrameæ•°æ®ã€å…ƒæ•°æ®å’Œå¼‚å¸¸å€¼ä¿¡æ¯
    """
    try:
        # éªŒè¯æ–‡ä»¶è·¯å¾„
        if not os.path.exists(file_path):
            return json.dumps({
                "status": "error",
                "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        # éªŒè¯æ–‡ä»¶æ ¼å¼
        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext not in ['.xlsx', '.xls']:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œä»…æ”¯æŒ.xlsxå’Œ.xls",
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
        workbook = load_workbook(file_path, read_only=True, data_only=True)
        all_sheets = workbook.sheetnames
        workbook.close()
        
        # ç¡®å®šè¦è¯»å–çš„å·¥ä½œè¡¨
        target_sheet = sheet_name if sheet_name else all_sheets[0]
        
        if sheet_name and sheet_name not in all_sheets:
            return json.dumps({
                "status": "error",
                "message": f"å·¥ä½œè¡¨'{sheet_name}'ä¸å­˜åœ¨",
                "available_sheets": all_sheets,
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        # è¯»å–Excelæ•°æ®
        df = pd.read_excel(
            file_path,
            sheet_name=target_sheet,
            header=header_row,
            skiprows=skip_rows
        )
        
        # å¤„ç†ç©ºå€¼
        null_counts = df.isnull().sum().to_dict()
        null_percentages = (df.isnull().sum() / len(df) * 100).to_dict()
        
        # è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
        outliers = {}
        for col in df.select_dtypes(include=[np.number]).columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()
            if outlier_indices:
                outliers[col] = {
                    "count": len(outlier_indices),
                    "indices": outlier_indices[:10],  # æœ€å¤šè¿”å›å‰10ä¸ª
                    "lower_bound": float(lower_bound),
                    "upper_bound": float(upper_bound)
                }
        
        # æ•°æ®ç±»å‹ä¿¡æ¯
        dtypes_info = {col: str(dtype) for col, dtype in df.dtypes.items()}
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        numeric_stats = {}
        for col in df.select_dtypes(include=[np.number]).columns:
            numeric_stats[col] = {
                "count": int(df[col].count()),
                "mean": float(df[col].mean()) if not pd.isna(df[col].mean()) else None,
                "std": float(df[col].std()) if not pd.isna(df[col].std()) else None,
                "min": float(df[col].min()) if not pd.isna(df[col].min()) else None,
                "25%": float(df[col].quantile(0.25)) if not pd.isna(df[col].quantile(0.25)) else None,
                "50%": float(df[col].quantile(0.50)) if not pd.isna(df[col].quantile(0.50)) else None,
                "75%": float(df[col].quantile(0.75)) if not pd.isna(df[col].quantile(0.75)) else None,
                "max": float(df[col].max()) if not pd.isna(df[col].max()) else None
            }
        
        # åˆ†ç±»åˆ—ä¿¡æ¯
        categorical_info = {}
        for col in df.select_dtypes(include=['object']).columns:
            value_counts = df[col].value_counts().head(10).to_dict()
            categorical_info[col] = {
                "unique_count": int(df[col].nunique()),
                "top_values": {str(k): int(v) for k, v in value_counts.items()},
                "null_count": int(df[col].isnull().sum())
            }
        
        # è½¬æ¢DataFrameä¸ºJSONæ ¼å¼
        data_json = df.to_dict(orient='records')
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "status": "success",
            "file_info": {
                "file_path": file_path,
                "file_name": os.path.basename(file_path),
                "file_size_bytes": os.path.getsize(file_path),
                "sheet_name": target_sheet,
                "all_sheets": all_sheets,
                "read_time": datetime.now().isoformat()
            },
            "data_shape": {
                "rows": len(df),
                "columns": len(df.columns),
                "total_cells": len(df) * len(df.columns)
            },
            "columns": {
                "names": df.columns.tolist(),
                "data_types": dtypes_info
            },
            "data": data_json,
            "metadata": {
                "null_counts": {k: int(v) for k, v in null_counts.items()},
                "null_percentages": {k: float(v) for k, v in null_percentages.items()},
                "numeric_statistics": numeric_stats,
                "categorical_info": categorical_info,
                "outliers": outliers
            },
            "data_quality": {
                "completeness_score": float(100 - (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100)),
                "total_nulls": int(df.isnull().sum().sum()),
                "total_outliers": sum(info["count"] for info in outliers.values())
            }
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "timestamp": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)


@tool
def data_analyzer(
    data_json: str,
    analysis_dimensions: Optional[List[str]] = None,
    include_correlation: bool = True,
    include_trends: bool = True
) -> str:
    """
    æ•°æ®åˆ†æå·¥å…·
    
    åŠŸèƒ½ï¼šè¿›è¡Œæ·±åº¦æ•°æ®åˆ†æï¼ŒåŒ…æ‹¬ç»Ÿè®¡åˆ†æã€è¶‹åŠ¿åˆ†æã€ç›¸å…³æ€§åˆ†æå’Œå¼‚å¸¸æ£€æµ‹
    
    Args:
        data_json (str): JSONæ ¼å¼çš„DataFrameæ•°æ®ï¼ˆé€šå¸¸æ¥è‡ªexcel_data_readerï¼‰
        analysis_dimensions (Optional[List[str]]): è¦åˆ†æçš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™åˆ†ææ‰€æœ‰æ•°å€¼åˆ—
        include_correlation (bool): æ˜¯å¦åŒ…å«ç›¸å…³æ€§åˆ†æï¼Œé»˜è®¤True
        include_trends (bool): æ˜¯å¦åŒ…å«è¶‹åŠ¿åˆ†æï¼Œé»˜è®¤True
        
    Returns:
        str: JSONæ ¼å¼çš„åˆ†æç»“æœ
    """
    try:
        # è§£æè¾“å…¥æ•°æ®
        input_data = json.loads(data_json)
        
        # æå–DataFrameæ•°æ®
        if "data" in input_data:
            df_data = input_data["data"]
        else:
            df_data = input_data
        
        df = pd.DataFrame(df_data)
        
        if df.empty:
            return json.dumps({
                "status": "error",
                "message": "æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ",
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        # ç¡®å®šåˆ†æç»´åº¦
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        if analysis_dimensions:
            numeric_columns = [col for col in analysis_dimensions if col in numeric_columns]
        
        if not numeric_columns:
            return json.dumps({
                "status": "error",
                "message": "æ²¡æœ‰å¯åˆ†æçš„æ•°å€¼åˆ—",
                "available_columns": df.columns.tolist(),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        # åŸºç¡€ç»Ÿè®¡åˆ†æ
        statistical_analysis = {}
        for col in numeric_columns:
            col_data = df[col].dropna()
            if len(col_data) == 0:
                continue
                
            statistical_analysis[col] = {
                "count": int(len(col_data)),
                "mean": float(col_data.mean()),
                "median": float(col_data.median()),
                "std": float(col_data.std()),
                "variance": float(col_data.var()),
                "min": float(col_data.min()),
                "max": float(col_data.max()),
                "range": float(col_data.max() - col_data.min()),
                "q1": float(col_data.quantile(0.25)),
                "q3": float(col_data.quantile(0.75)),
                "iqr": float(col_data.quantile(0.75) - col_data.quantile(0.25)),
                "skewness": float(col_data.skew()),
                "kurtosis": float(col_data.kurtosis()),
                "coefficient_of_variation": float(col_data.std() / col_data.mean() * 100) if col_data.mean() != 0 else None
            }
        
        # ç›¸å…³æ€§åˆ†æ
        correlation_matrix = {}
        if include_correlation and len(numeric_columns) > 1:
            corr = df[numeric_columns].corr()
            correlation_matrix = {
                "matrix": corr.to_dict(),
                "high_correlations": []
            }
            
            # è¯†åˆ«é«˜ç›¸å…³æ€§å¯¹
            for i in range(len(corr.columns)):
                for j in range(i+1, len(corr.columns)):
                    corr_value = corr.iloc[i, j]
                    if abs(corr_value) > 0.7:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                        correlation_matrix["high_correlations"].append({
                            "variable1": corr.columns[i],
                            "variable2": corr.columns[j],
                            "correlation": float(corr_value),
                            "strength": "strong" if abs(corr_value) > 0.9 else "moderate"
                        })
        
        # è¶‹åŠ¿åˆ†æï¼ˆæ—¶é—´åºåˆ—è¯†åˆ«ï¼‰
        trend_analysis = {}
        if include_trends:
            # å°è¯•è¯†åˆ«æ—¶é—´åˆ—
            time_columns = []
            for col in df.columns:
                try:
                    # ä½¿ç”¨ errors='coerce' é¿å…è­¦å‘Šï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å› NaT
                    parsed_dates = pd.to_datetime(df[col], errors='coerce')
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå¤šçš„æœ‰æ•ˆæ—¥æœŸï¼ˆè‡³å°‘50%æ˜¯æœ‰æ•ˆæ—¥æœŸï¼‰
                    if parsed_dates.notna().sum() / len(parsed_dates) > 0.5:
                        time_columns.append(col)
                except:
                    continue
            
            if time_columns:
                time_col = time_columns[0]
                df_sorted = df.sort_values(by=time_col)
                
                for col in numeric_columns:
                    if col != time_col:
                        values = df_sorted[col].dropna()
                        if len(values) > 3:
                            # è®¡ç®—è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰
                            x = np.arange(len(values))
                            y = values.values
                            slope = np.polyfit(x, y, 1)[0]
                            
                            trend_analysis[col] = {
                                "trend_direction": "increasing" if slope > 0 else "decreasing",
                                "slope": float(slope),
                                "start_value": float(values.iloc[0]),
                                "end_value": float(values.iloc[-1]),
                                "change_percentage": float((values.iloc[-1] - values.iloc[0]) / values.iloc[0] * 100) if values.iloc[0] != 0 else None,
                                "time_column": time_col
                            }
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        anomaly_detection = {}
        for col in numeric_columns:
            col_data = df[col].dropna()
            if len(col_data) < 4:
                continue
            
            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
            if len(outliers) > 0:
                anomaly_detection[col] = {
                    "count": len(outliers),
                    "percentage": float(len(outliers) / len(col_data) * 100),
                    "outlier_indices": outliers.index.tolist()[:20],  # æœ€å¤šè¿”å›å‰20ä¸ª
                    "outlier_values": [float(v) for v in outliers[col].tolist()[:20]],
                    "bounds": {
                        "lower": float(lower_bound),
                        "upper": float(upper_bound)
                    }
                }
        
        # åˆ†ç»„ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨åˆ†ç±»åˆ—ï¼‰
        groupby_analysis = {}
        categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
        if categorical_columns and numeric_columns:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªåˆ†ç±»åˆ—å’Œç¬¬ä¸€ä¸ªæ•°å€¼åˆ—è¿›è¡Œåˆ†ç»„ç»Ÿè®¡
            cat_col = categorical_columns[0]
            num_col = numeric_columns[0]
            
            grouped = df.groupby(cat_col)[num_col].agg(['count', 'mean', 'std', 'min', 'max'])
            groupby_analysis = {
                "groupby_column": cat_col,
                "value_column": num_col,
                "groups": grouped.to_dict(orient='index')
            }
        
        # æ„å»ºåˆ†æç»“æœ
        result = {
            "status": "success",
            "analysis_time": datetime.now().isoformat(),
            "data_shape": {
                "rows": len(df),
                "columns": len(df.columns)
            },
            "analyzed_columns": numeric_columns,
            "statistical_analysis": statistical_analysis,
            "correlation_analysis": correlation_matrix if include_correlation else {},
            "trend_analysis": trend_analysis if include_trends else {},
            "anomaly_detection": anomaly_detection,
            "groupby_analysis": groupby_analysis,
            "data_insights": {
                "total_numeric_columns": len(numeric_columns),
                "total_categorical_columns": len(categorical_columns),
                "columns_with_outliers": len(anomaly_detection),
                "high_correlation_pairs": len(correlation_matrix.get("high_correlations", [])) if include_correlation else 0
            }
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "timestamp": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)


@tool
def cache_manager(
    operation: str,
    session_id: str,
    file_info: Optional[Dict[str, Any]] = None,
    cache_base_dir: str = ".cache/excel_report_generator"
) -> str:
    """
    ç¼“å­˜ç®¡ç†å·¥å…·
    
    åŠŸèƒ½ï¼šç®¡ç†ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Šç¼“å­˜ï¼Œæ”¯æŒåˆ›å»ºä¼šè¯ç›®å½•ã€æ–‡ä»¶å¤ç”¨æ£€æµ‹ã€è¿‡æœŸæ¸…ç†
    
    Args:
        operation (str): æ“ä½œç±»å‹ (create_session/check_cache/save_cache/list_cache/clean_cache)
        session_id (str): ä¼šè¯ID
        file_info (Optional[Dict[str, Any]]): æ–‡ä»¶ä¿¡æ¯å­—å…¸ï¼ˆç”¨äºsave_cacheå’Œcheck_cacheï¼‰
        cache_base_dir (str): ç¼“å­˜åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸º.cache/excel_report_generator
        
    Returns:
        str: JSONæ ¼å¼çš„æ“ä½œç»“æœ
    """
    try:
        cache_dir = Path(cache_base_dir)
        session_dir = cache_dir / session_id
        
        if operation == "create_session":
            # åˆ›å»ºä¼šè¯ç›®å½•
            session_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºä¼šè¯å…ƒæ•°æ®
            metadata = {
                "session_id": session_id,
                "created_at": datetime.now().isoformat(),
                "status": "active"
            }
            
            metadata_file = session_dir / "session_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            return json.dumps({
                "status": "success",
                "operation": "create_session",
                "session_id": session_id,
                "session_path": str(session_dir),
                "metadata_file": str(metadata_file),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        elif operation == "check_cache":
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            if not file_info or "file_hash" not in file_info:
                return json.dumps({
                    "status": "error",
                    "message": "file_infoå¿…é¡»åŒ…å«file_hashå­—æ®µ",
                    "timestamp": datetime.now().isoformat()
                }, ensure_ascii=False, indent=2)
            
            file_hash = file_info["file_hash"]
            cached_files = list(session_dir.glob(f"*{file_hash}*")) if session_dir.exists() else []
            
            return json.dumps({
                "status": "success",
                "operation": "check_cache",
                "cache_exists": len(cached_files) > 0,
                "cached_files": [str(f) for f in cached_files],
                "file_count": len(cached_files),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        elif operation == "save_cache":
            # ä¿å­˜ç¼“å­˜ä¿¡æ¯
            if not file_info:
                return json.dumps({
                    "status": "error",
                    "message": "file_infoä¸èƒ½ä¸ºç©º",
                    "timestamp": datetime.now().isoformat()
                }, ensure_ascii=False, indent=2)
            
            session_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç¼“å­˜è®°å½•
            cache_record_file = session_dir / "cache_records.json"
            cache_records = []
            
            if cache_record_file.exists():
                with open(cache_record_file, 'r', encoding='utf-8') as f:
                    cache_records = json.load(f)
            
            cache_records.append({
                **file_info,
                "cached_at": datetime.now().isoformat()
            })
            
            with open(cache_record_file, 'w', encoding='utf-8') as f:
                json.dump(cache_records, f, ensure_ascii=False, indent=2)
            
            return json.dumps({
                "status": "success",
                "operation": "save_cache",
                "cache_record_file": str(cache_record_file),
                "total_cached_files": len(cache_records),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        elif operation == "list_cache":
            # åˆ—å‡ºç¼“å­˜æ–‡ä»¶
            if not session_dir.exists():
                return json.dumps({
                    "status": "success",
                    "operation": "list_cache",
                    "cached_files": [],
                    "total_files": 0,
                    "timestamp": datetime.now().isoformat()
                }, ensure_ascii=False, indent=2)
            
            cache_record_file = session_dir / "cache_records.json"
            cache_records = []
            
            if cache_record_file.exists():
                with open(cache_record_file, 'r', encoding='utf-8') as f:
                    cache_records = json.load(f)
            
            return json.dumps({
                "status": "success",
                "operation": "list_cache",
                "session_id": session_id,
                "cached_files": cache_records,
                "total_files": len(cache_records),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        elif operation == "clean_cache":
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            if not session_dir.exists():
                return json.dumps({
                    "status": "success",
                    "operation": "clean_cache",
                    "message": "ä¼šè¯ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†",
                    "timestamp": datetime.now().isoformat()
                }, ensure_ascii=False, indent=2)
            
            # åˆ é™¤ä¼šè¯ç›®å½•åŠå…¶æ‰€æœ‰å†…å®¹
            import shutil
            shutil.rmtree(session_dir)
            
            return json.dumps({
                "status": "success",
                "operation": "clean_cache",
                "session_id": session_id,
                "cleaned_path": str(session_dir),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œ: {operation}",
                "supported_operations": ["create_session", "check_cache", "save_cache", "list_cache", "clean_cache"],
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return json.dumps({
            "status": "error",
            "message": f"ç¼“å­˜ç®¡ç†å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
            "timestamp": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•Excelæ•°æ®å¤„ç†å·¥å…·...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    print("âœ… Excelæ•°æ®å¤„ç†å·¥å…·æ¨¡å—åŠ è½½æˆåŠŸï¼")
